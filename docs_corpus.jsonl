{"url":"https:\/\/docs.atlan.com\/","title":"Atlan | Atlan Documentation","text":"Discover, trust, and govern your data & AI ecosystem Everything you need to get started with Atlan. Ã° Quick-start guide Step-by-step onboarding Ã° Â§ Secure agent Enterprise-grade deployment options Ã° Playbooks automation Rule-based metadata updates at scale Ã° Â¡Ã¯Â¸ Govern & manage Create data contracts & policies Ã° Integrate Automation, collaboration & other integrations Ã¯Â¸ Introductory walkthrough Play with APIs in minutes Ã° Â» Client SDKs Java, Python & more Ã° Â¦ Packages Developer-built utilities and integrations Get started with Atlan by building the right strategy and setting a strong foundation. A comprehensive look at Atlan's security philosophy, core values, and rigorous security procedures Help and support Find answers or contact our team for personalized assistance"}
{"url":"https:\/\/developer.atlan.com\/","title":"Atlan - Developers","text":"Are you keen to activate your metadata? To automate tedious manual work, or reduce the complexity of some of your largest data challenges? Welcome to our ðŸ¡ for the developer community of Atlan. Here you'll find reference materials for working with Atlan programmatically. We look forward to helping you, the ðŸ§‘'s of data, develop the future ðŸ¤–'s of data!"}
{"url":"https:\/\/docs.atlan.com\/#__docusaurus_skipToContent_fallback","title":"Atlan | Atlan Documentation","text":"Discover, trust, and govern your data & AI ecosystem Everything you need to get started with Atlan. Ã° Quick-start guide Step-by-step onboarding Ã° Â§ Secure agent Enterprise-grade deployment options Ã° Playbooks automation Rule-based metadata updates at scale Ã° Â¡Ã¯Â¸ Govern & manage Create data contracts & policies Ã° Integrate Automation, collaboration & other integrations Ã¯Â¸ Introductory walkthrough Play with APIs in minutes Ã° Â» Client SDKs Java, Python & more Ã° Â¦ Packages Developer-built utilities and integrations Get started with Atlan by building the right strategy and setting a strong foundation. A comprehensive look at Atlan's security philosophy, core values, and rigorous security procedures Help and support Find answers or contact our team for personalized assistance"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/snowflake\/how-tos\/set-up-snowflake","title":"Set up Snowflake | Atlan Documentation","text":"You need your Snowflake administrator to run these commands - you may not have access yourself. Create user and role in Snowflake Create a role and user in Snowflake using the following commands: Create role Create a role in Snowflake using the following commands: Replace with the default warehouse to use when running the Snowflake crawler . Atlan requires the following privileges to: OPERATE enables Atlan to start the virtual warehouse to fetch metadata if the warehouse has stopped. USAGE enables Atlan to show or list metadata from Snowflake. This in turn enables the Snowflake crawler to run the SHOW query. Create a user Create a separate user to integrate into Atlan, using one of the following 3 options: See Snowflake's official guide for details on generating an RSA key-pair . To create a user with a key-pair, replace the value for rsa_public_key with the public key and run the following: Learn more about the SERVICE type property in Snowflake documentation . Atlan only supports encrypted private keys with a non-empty passphrase - generally recommended as more secure. An empty passphrase results in workflow failures. To generate an encrypted private key, omit the -nocrypt option. Refer to Snowflake documentation to learn more. Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. To create a user with a password, replace and run the following: Learn more about the LEGACY_SERVICE type property in Snowflake documentation . This method is currently only available if Okta is your IdP (Snowflake supports) authenticating natively through Okta : Create a user in your identity provider (IdP) and use federated authentication in Snowflake . The password for this user must be maintained solely in the IdP and multi-factor authentication (MFA) must be disabled. Grant role to user To grant the atlan_user_role to the new user: Configure OAuth (client credentials flow) with Microsoft Entra ID To configure OAuth authentication using Microsoft Entra ID (formerly Azure AD) with the client credentials flow: Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions In Snowflake, create a security integration using the following: CREATE SECURITY INTEGRATION external_oauth_azure_ad TYPE = external_oauth ENABLED = true EXTERNAL_OAUTH_TYPE = azure EXTERNAL_OAUTH_ISSUER = '\\ ' EXTERNAL_OAUTH_JWS_KEYS_URL = '\\ ' EXTERNAL_OAUTH_AUDIENCE_LIST = ( '\\ ' ) EXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM = 'sub' EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE = 'login_name' ; Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app In Snowflake, create a security integration using the following: Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: CREATE USER oauth_svc_user WITH LOGIN_NAME = '\\ ' -- Use Azure client OBJECT ID DEFAULT_ROLE = \\ DEFAULT_WAREHOUSE = \\ ; Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: Grant the configured role to this user: GRANT ROLE \\ TO USER oauth_svc_user ; Grant the configured role to this user: Choose metadata fetching method Atlan supports two methods for fetching metadata from Snowflake - account usage and information schema. You should choose one of these two methods to set up Snowflake: Grant permissions for account usage method If you want to set warehouse timeouts when using this method, set a large value initially for the workflow to succeed. Once the workflow has succeeded, adjust the value to be twice the extraction time. This method uses the views in SNOWFLAKE.ACCOUNT_USAGE (or a copied version of this schema) to fetch the metadata from Snowflake into Atlan. You can be more granular with permissions using this method, but there are limitations with this approach . To crawl assets, generate lineage, and import tags If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Snowflake stores all tag objects in the ACCOUNT_USAGE schema. If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner , you need to grant the same permissions to import tags as required for crawling Snowflake assets. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. To crawl streams To crawl streams, provide the following permissions: To crawl current streams: Replace with the Snowflake database name. Replace with the Snowflake database name. To crawl future streams: To crawl future streams: Replace with the Snowflake database name. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. Grant permissions for information schema method This method uses views in the INFORMATION_SCHEMA schema in Snowflake databases to fetch metadata. You still need to grant specific permissions to enable Atlan to crawl metadata, preview data, and query data with this method. To crawl existing assets Grant these permissions to crawl assets that already exist in Snowflake. If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Grant permissions to crawl existing assets: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) Grant permissions to crawl functions: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) For secure user-defined functions (UDFs), grant OWNERSHIP permissions to retrieve metadata: Replace the placeholders with the appropriate values: : The name of the schema that contains the user-defined function (UDF). : The name of the secure UDF that requires ownership permissions. : The role that gets assigned ownership of the secure UDF. The statements given on this page apply to all schemas, tables, and views in a database in Snowflake. If you want to limit access to only certain objects, you can instead specify the exact objects individually as well. To crawl future assets To crawl assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. To grant permissions at a database level: Replace with the database you want to crawl in Atlan. (Repeat the statements for every database you want to integrate into Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) To mine query history for lineage To also mine Snowflake's query history (for lineage), add these permissions. You can use either option: To mine query history direct from Snowflake's internal tables: To mine query history from a cloned or copied set of tables, where you can also remove any sensitive data: Replace with the name of the cloned database, and with the name of the cloned schema containing account usage details. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to be able to preview and query in Atlan. (Repeat the statements for every database and schema you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. (Optional) To import Snowflake tags Snowflake stores all tag objects in the ACCOUNT_USAGE schema. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To import tags from Snowflake , grant these permissions: To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. (Optional) To push updated tags to Snowflake To push tags updated for assets in Atlan to Snowflake , grant these permissions: You can learn more about tag privileges from Snowflake documentation . (Optional) To crawl dynamic tables Atlan currently supports fetching metadata for dynamic tables using the MONITOR privilege. Refer to Snowflake documentation to learn more. To crawl existing dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: To crawl future dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) (Optional) To crawl Iceberg tables Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method. To crawl Iceberg tables from Snowflake, grant the following permissions: To crawl existing Iceberg tables in Snowflake: To crawl future Iceberg tables in Snowflake: To crawl Iceberg catalog metadata for Iceberg tables in Snowflake: You must first grant permissions to crawl existing Iceberg tables for this permission to work on catalogs. You must also grant permissions to all the catalogs you want to crawl in Atlan individually. (Optional) To crawl Snowflake stages Atlan supports crawling metadata for Snowflake stages using the USAGE and READ privileges. For more information, see the Snowflake documentation for INFORMATION_SCHEMA.STAGES. To crawl stages from Snowflake: Grant USAGE and READ privileges on all existing stages at the database level: Replace with the name of your Snowflake database Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Replace with the role you've granted Atlan to use for crawling. Grant USAGE and READ privileges on all future stages at the database level: Grant USAGE and READ privileges on all future stages at the database level: Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Allowlist the Atlan IP If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support ticket from within Atlan, or submit a request . (If you aren't using the IP allowlist in your Snowflake instance, you can skip this step.) Create user and role in Snowflake Choose metadata fetching method Grant permissions for account usage method Grant permissions for information schema method Allowlist the Atlan IP"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-databricks","title":"Set up Databricks | Atlan Documentation","text":"Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: SQL warehouse (formerly SQL endpoint) To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after . minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Create a service principal You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Once you've copied the client ID and secret, click Done . Azure service principal authentication You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Tenant ID (directory ID) Create a service principal To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : usage and popularity metrics Enable system.access schema You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage system.query.history (to mine query history for usage and popularity metrics) You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, ) USE SCHEMA and SELECT on the schema (for example, . ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. (Optional) Grant view permissions to access Databricks entities via APIs Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3 6 for each catalog you want to crawl in Atlan. SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/business-intelligence\/microsoft-power-bi\/how-tos\/set-up-microsoft-power-bi","title":"Set up Microsoft Power BI | Atlan Documentation","text":"Depending on the authentication method you choose, you may need a combination of your Cloud Application Administrator or Application Administrator for Microsoft Entra ID, Microsoft 365 administrator for Microsoft 365, and Fabric Administrator ( formerly known as Power BI Administrator ) for Microsoft Power BI to complete these tasks -> you may not have access yourself. This guide outlines how to set up Microsoft Power BI so it can connect with Atlan for metadata extraction and lineage tracking. Before you begin Register application in Microsoft Entra ID You need your Cloud Application Administrator or Application Administrator to complete these steps > you may not have access yourself. This is required if the creation of registered applications isn't enabled for the entire organization. To register a new application in Microsoft Entra ID: Log in to the Azure portal . Click App registrations from the left menu. Click + New registration . Enter a name for your client application and click Register . Application (client) ID Directory (tenant) ID Click Certificates & secrets from the left menu. Under Client secrets , click + New client secret . Enter a description, select an expiry time, and click Add . Copy and securely store the client secret Value . Create security group in Microsoft Entra ID You need your Cloud Application Administrator or Application Administrator to complete these steps - you may not have access yourself. To create a security group for your application: Log in to the Azure portal . Click Groups under the Manage section. Click New group . Set the Group type to Security . Enter a Group name and optional description. Click No members selected . Click Select and then Create . By the end of these steps, you have registered an application with Microsoft Entra ID and created a Security Group with the appropriate member. Configure authentication options Atlan supports two authentication methods for fetching metadata from Microsoft Power BI: Service principal authentication (recommended) When using Service Principal authentication, you must decide how the connector shall access metadata to catalog assets and build lineage. There are two supported options: This option grants permissions that let the service principal to access only admin-level Power BI APIs. In this mode, Atlan extracts metadata exclusively using administrative endpoints. This option is recommended for stricter access control environments. You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks - you may not have access yourself. To configure admin API access: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Admin API settings : Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group under Specific security groups Click Apply Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group under Specific security groups Click Apply Add your security group under Specific security groups Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Add your security group This option grants permissions that let the service principal to access both admin and non-admin Power BI APIs. This enables Atlan to extract richer metadata and build detailed lineage across Power BI assets. You need to be at least a member of the Microsoft Power BI workspace to which you want to add the security group to complete these steps - you may not have access yourself. Make sure that you add the security group from the homepage and not the admin portal. To assign a Microsoft Power BI workspace role to the security group: Open the Microsoft Power BI homepage . Open Workspaces and select the workspace you want to access from Atlan. Click Access . In the panel: Enter the name of your security group where it says Enter email addresses Choose one of the following roles: Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Click Add . Enter the name of your security group where it says Enter email addresses Choose one of the following roles: Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Click Add . You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks - you may not have access yourself. To enable both admin and non-admin API access: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Developer settings : Expand Service principals can use Fabric APIs and set to Enabled Add your security group under Specific security groups Click Apply Expand Service principals can use Fabric APIs and set to Enabled Add your security group under Specific security groups Click Apply Add your security group under Specific security groups Under Admin API settings : Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Add your security group After making these changes, you typically need to wait 15-30 minutes for the settings to take effect across Microsoft's services. Delegated user authentication Atlan doesn't recommend using delegated user authentication as it's also not recommended by Microsoft. You need your Microsoft 365 administrator to complete these steps - you may not have access yourself. To assign the delegated user to the Fabric Administrator role: Open the Microsoft 365 admin portal . Click Users and then Active users from the left menu. Select the delegated user. Under Roles , click Manage roles . Expand Show all by category . Under Collaboration , select Fabric Administrator . Click Save changes . You need your Cloud Application Administrator or Application Administrator to complete these steps, you may not have access yourself. The following permissions are only required for delegated user authentication. If using service principal authentication, you don't need to configure any delegated permissions for a service principal it's recommended that you avoid adding these permissions. These are never used and can cause errors that may be hard to troubleshoot. To add permissions for the registered application : In your app registration, click API permissions under the Manage section. Click Add a permission . Click Delegated permissions and select: Capacity.Read.All Dataflow.Read.All Dataset.Read.All Report.Read.All Tenant.Read.All Workspace.Read.All Click Grant Admin consent (If you only see the Add permissions button, you aren't an administrator). You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks, you may not have access yourself. To enable the Microsoft Power BI admin API: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Admin API settings : Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply . Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply . Add your security group Click Apply . Before you begin Configure authentication options"}
{"url":"https:\/\/docs.atlan.com\/platform\/references\/atlan-architecture","title":"Atlan architecture | Atlan Documentation","text":"Atlan is a cloud-first solution. Single-tenant SaaS is the recommended deployment model. Atlan currently supports hosting tenants on the following cloud platforms: Amazon Web Services (AWS) Microsoft Azure Google Cloud Platform (GCP) The components of Atlan are isolated, across both compute and data. For more details, see How are resources isolated? Platform components Kong is an API gateway. It handles rate limiting and token verification on all incoming API requests. Apache Keycloak is an identity and access management component. It manages everything to do with users, login, SSO and so on. Heracles is Atlan's API service. It houses the business logic used by the frontend and APIs to interact with other platform components. PostgreSQL is a SQL database. Many services on the platform use it for storage. HashiCorp Vault is a secret manager. It stores sensitive credentials provided by the user. Apache Ranger is the policy engine. It provides fine-grained access control over data in the metastore. Argo Workflows is a workflow orchestrator for k8s. It runs and manages long-running jobs in a container and k8s-native fashion. Admission Controller is a k8s admission controller. It performs certain actions when Argo Workflows are updated such as workflow alerts. Apache Zookeeper manages consensus and coordination for the metastore services. Apache Cassandra is an object-oriented database used to store the metastore's data. Apache Kafka is an event stream. It enables event-driven use cases across the platform. Heka is Atlan's SQL component. It parses, rewrites and optimizes SQL queries and is powered by Apache Calcite . Redis is a cache layer used by Heracles. Platform management components Velero performs cluster backups. Fluent Bit is a logging and metrics processor. It parses and pushes logs from pods to various destinations. Central components Zenduty is used for incident response. Alerts are sent when something goes wrong in one of the clusters. Argo CD is used for continuous deployment. Changes in git repositories lead to upgrades in the clusters. Github Actions update the Docker container images as part of the development process. Sendgrid is used to send emails. The frontend is a Vue.js web application that's hosted on S3 and delivered via Amazon CloudFront content delivery network (CDN) service. Alertmanager sends alerts generated by metrics stored in Prometheus. Grafana provides observability dashboards. VictoriaMetrics is a fast, cost-effective, and scalable monitoring solution and time series database. It processes high volumes of data and enables long-term storing. Atlan marketplace (not pictured) The marketplace offers packages (workflows) that perform long-running tasks on the Atlan platform. The ecosystem enables the creation of metadata and lineage connectors. See security.atlan.com for the latest policies and standards, reports and certifications, architecture, diagrams and more. Platform management components Atlan marketplace (not pictured)"}
{"url":"https:\/\/docs.atlan.com\/product\/integrations\/automation\/browser-extension\/how-tos\/use-the-atlan-browser-extension","title":"Use the Atlan browser extension | Atlan Documentation","text":"The Atlan browser extension provides metadata context directly in your supported data tools . You can use the extension in the following Chromium-based browsers: Google Chrome and Microsoft Edge. Install the extension To install Atlan's browser extension: You can either: Find the extension in the Chrome Web Store: https:\/\/chrome.google.com\/webstore\/detail\/atlan\/fipjfjlalpnbejlmmpfnmlkadjgaaheg From the upper right of any screen in Atlan, navigate to your name and then click Profile . Click the four dots icon in the resulting dialog to get to integrations. Under Apps , for Browser extension , click Install . Find the extension in the Chrome Web Store: https:\/\/chrome.google.com\/webstore\/detail\/atlan\/fipjfjlalpnbejlmmpfnmlkadjgaaheg From the upper right of any screen in Atlan, navigate to your name and then click Profile . Click the four dots icon in the resulting dialog to get to integrations. Under Apps , for Browser extension , click Install . Click the four dots icon in the resulting dialog to get to integrations. Under Apps , for Browser extension , click Install . To install the Atlan browser extension: For Google Chrome, in the upper right of your screen, click Add to Chrome . When prompted for confirmation, click the Add extension button. For Microsoft Edge, follow the steps in Add an extension to Microsoft Edge from the Chrome Web Store . For Google Chrome, in the upper right of your screen, click Add to Chrome . When prompted for confirmation, click the Add extension button. For Microsoft Edge, follow the steps in Add an extension to Microsoft Edge from the Chrome Web Store . Currently, you can't install the browser extension on mobile devices or tablets. You can also install Atlan's browser extension at the workspace level . To set this up, you need to be an administrator or have access to the admin console of your organization's Google account. If your organization uses managed browsers, you can configure the extension for managed browsers . Configure the extension Once installed, configure the Atlan browser extension to get started. Optionally, Atlan admins can preconfigure custom domains for data sources , if any. Configure the extension as a user To configure the browser extension, once installed: If you are logged into your Atlan instance, skip to the next step. If you haven't logged into Atlan, log in to your Atlan instance when prompted. In the Options page, to enter the URL of your Atlan instance: If your organization uses an Atlan domain (for example, _mycompany_.atlan.com ), the Atlan instance URL appears preselected. Click Get started . (Optional) Switch to a different Atlan domain, if required. If your organization uses a custom domain (for example, _atlan_.mycompany.com ), enter the URL of your Atlan instance and then click Get started . If your organization uses an Atlan domain (for example, _mycompany_.atlan.com ), the Atlan instance URL appears preselected. Click Get started . (Optional) Switch to a different Atlan domain, if required. If your organization uses a custom domain (for example, _atlan_.mycompany.com ), enter the URL of your Atlan instance and then click Get started . After a successful login, the message Updated successfully appears. (Optional) If your data tools are hosted on custom domains, rather than the standard SaaS domain of each tool: Click the Configure custom domain link at the bottom. In the dropdown on the left, select your data tool. In the text box on the right, enter the custom domain you use for that tool. Repeat these steps for each tool hosted on a custom domain. Click the Save button when finished. If your Atlan admin has preconfigured custom domains for data sources , you won't be able to update or remove these selections. Click + Add to configure custom domains for additional data sources as required. Click the Configure custom domain link at the bottom. In the dropdown on the left, select your data tool. In the text box on the right, enter the custom domain you use for that tool. Repeat these steps for each tool hosted on a custom domain. Click the Save button when finished. In the dropdown on the left, select your data tool. In the text box on the right, enter the custom domain you use for that tool. Repeat these steps for each tool hosted on a custom domain. Click the Save button when finished. If your Atlan admin has preconfigured custom domains for data sources , you won't be able to update or remove these selections. Click + Add to configure custom domains for additional data sources as required. You can now close the Options tab. The extension is now ready to use! Ã° (Optional) Configure custom domains as an admin You need to be an admin user in Atlan to configure custom domains for data sources from the admin center. To configure custom domains, from within Atlan: From the left menu of any screen, click Admin . Under Workspace , click Integrations . Under Apps , expand the Browser extension tile. In the Browser extension tile, for Set up your custom data source. , if your data tools are hosted on custom domains rather than the standard SaaS domain of each tool, click the Configure link to configure them for users in your organization. For Connector , select a supported tool for the browser extension. In the adjacent field, enter the URL of the custom domain for your data source. (Optional) Click + Add to add more. Click Save to save your configuration. info Ã° Âª Did you know? For any supported tools that you have configured, your users won't be able to update or remove these selections. They can, however, add additional custom domains for data sources. For Connector , select a supported tool for the browser extension. In the adjacent field, enter the URL of the custom domain for your data source. (Optional) Click + Add to add more. Click Save to save your configuration. Ã° Âª Did you know? For any supported tools that you have configured, your users won't be able to update or remove these selections. They can, however, add additional custom domains for data sources. (Optional) For Download Atlan extension or share with your team , you can either install the Atlan browser extension for your own use or share the link with your users. Anyone with access to Atlan any admin, member, or guest user and a supported tool can use the browser extension. First, log into Atlan. When using Atlan's browser extension in a supported tool , the extension only reads the URL of your browser tab no other data is accessed. If using Atlan's browser extension on any website , it only reads the favicon, page title, and URL of your browser tab. Learn more about Atlan browser extension security . Access and enrich context in-flow To access context for an asset, from within a supported tool: Log into the supported tool. Open any supported asset. In the lower-right corner of the page, click the small Atlan icon. danger The icon to activate Atlan is not the extension icon that appears at the top of your Chrome browser. This small Atlan icon in the lower right corner of the page is the only way to access the metadata for the asset you are viewing in another tool. The icon to activate Atlan is not the extension icon that appears at the top of your Chrome browser. This small Atlan icon in the lower right corner of the page is the only way to access the metadata for the asset you are viewing in another tool. In the sidebar that appears: Click the tabs and links to view all context about the asset. Make changes to any of the metadata you'd like. Click the tabs and links to view all context about the asset. Make changes to any of the metadata you'd like. Now you can understand and enrich assets without leaving your data tools themselves! Ã° The Atlan sidebar automatically reloads as you browse your assets in a supported tool to show details about the asset you're currently viewing. Your permissions in Atlan control what metadata you can see and change in the extension. The extension opens a new browser tab on Atlan's discovery page, with the results for that text! Ã° Add a resource You can link any web page as a resource to your assets in Atlan using the browser extension. To add a web page as a resource to an asset: In the top right of the web page you're viewing, click the Atlan Chrome extension . In the resource clipper menu, under Link this page to an asset , select the asset to which you'd like to add the web page as a resource. Click Save to confirm your selection. (Optional) Once the resource has been linked successfully, click the Open in Atlan button to view the linked asset directly in Atlan. You can now add resources to your assets in Atlan from any website! Ã° The Tableau extension offers native embeddings directly in your dashboards. See Enable embedded metadata in Tableau for more information. Supported tools Currently, the Atlan browser extension supports assets in the following tools: Amazon QuickSight : analyses, dashboards, and datasets Databricks : databases, schemas, views, and tables dbt Cloud : models and sources in the model editor and dbt docs Google BigQuery : datasets, schemas, views, and tables IBM Cognos Analytics : folders, dashboards, packages, explorations, reports, files, data sources, and modules Looker : dashboards, explores, and folders Microsoft Power BI : dashboards, reports, dataflows, and datasets Mode : collections, reports, queries, and charts Qlik Sense Cloud : apps, datasets, sheets, and spaces Redash : queries, dashboards, and visualizations Salesforce : objects Sigma : datasets, pages, and data elements Snowflake (via Snowsight schema explorer): databases, schemas, tables, views, dynamic tables, streams, and pipes Tableau : dashboards, data sources, workbooks, and metrics. Additionally, you can choose to switch the Tableau extension to offer native embeddings directly in your dashboards. See Enable embedded metadata in Tableau for more information. ThoughtSpot : liveboards, answers, visualizations, and tables MicroStrategy : dossiers, reports, documents Install the extension Configure the extension"}
{"url":"https:\/\/docs.atlan.com\/get-started\/how-tos\/quick-start-for-admins","title":"Administrators | Atlan Documentation","text":"User management User management is a critical part of data governance. Atlan's user management capabilities should be a mainstay of how you organize and control access for people in your organization. Add and manage users from the admin center It's super simple to invite and remove users from Atlan from the Admin center . You can also manage existing users by adding them to groups, changing their roles, or set up SSO , SCIM , and SMTP configurations. Manage access control from the governance center The Governance center is where you can build access control mechanisms to manage user access . Personas allow you to group users into teams, such as Financial Analysts or Cloud Engineers , and set policies based on the access those personas should have. Purposes are where you can build policies based on the actions or access that a user might need. For example, you can use Atlan's policy-based access controls to manage access to PII and other sensitive data. This is a best practice for data governance. Once you set these policies, Atlan will enforce them throughout your users' experience. This means that users who don't have access to a particular type of data will not be able to see it. Governance workflows help you set up robust controls on data access management, metadata enrichment, new entity creation, and more, with out-of-the-box workflow templates and automated execution. Asset profile The asset profile in Atlan gives you a quick and clear understanding of what a data asset contains. You can think of the asset profile as the TL;DR about your data. The glossary provides key intel on your data assets so you can quickly understand important attributes of your data, such as: Owners of your data, so you know who to ask for clarification. Certificate status, to easily understand if metadata enrichment is still in progress or the asset is ready to be used. Linked assets that are relevant to the term, so you can explore other helpful material. Here are a few of the things that make Atlan's discovery awesome: Intelligent keyword recognition sees through your typos to show exactly what you wanted, no matter what you actually typed. Sort by popularity to quickly discover what assets your teammates are using every day."}
{"url":"https:\/\/docs.atlan.com\/secure-agent","title":"Secure Agent | Atlan Documentation","text":"The Atlan Secure Agent is a lightweight, Kubernetes-based application that enables secure metadata extraction. It connects internal systems with Atlan SaaS while keeping sensitive data protected and doesn t require inbound connectivity. Running within an organization s controlled environment, the Secure Agent ensures compliance with security policies and automates metadata processing. Figure 1: The Secure Agent runs in the customer environment and acts as a gateway. Key capabilities The Secure Agent is designed for secure, scalable, and efficient metadata extraction. Security-first architecture Runs entirely within the organization's infrastructure, preventing secrets from leaving its boundary. Uses outbound, encrypted communication to interact with Atlan SaaS. Supports logging and monitoring and integrates with external monitoring systems for auditing and compliance. Scalable metadata extraction A single deployment of the Agent can connect to multiple source systems. Supports multiple concurrent metadata extraction jobs. Uses Kubernetes-based workloads for efficient resource management. Flexible deployment Deploys on cloud-based Kubernetes environments (such as Amazon EKS, Azure AKS, and Google GKE) or on-premises clusters. Scales dynamically based on workload demands. Automated operations Continuously monitors system health and sends heartbeats to Atlan. Captures and uploads execution logs for troubleshooting and auditing. Provides performance insights through metrics and alerts. How it works The Secure Agent follows a job-based execution model where metadata extraction tasks are scheduled and executed within the organization's environment. The workflow typically involves: Atlan triggers a metadata extraction job. The Secure Agent retrieves job details and extracts metadata using source-specific connectors. Extracted metadata is shared with Atlan either through cloud storage or direct ingestion. Atlan workflows process the extracted metadata and publish the assets. Logs and execution status are sent to Atlan for monitoring and auditing. See also Deployment architecture : Learn more about how the Secure Agent integrates with your environment and supports secure metadata extraction. How it works"}
{"url":"https:\/\/docs.atlan.com\/product\/capabilities\/playbooks","title":"Playbooks | Atlan Documentation","text":"Get started How to set up playbooks Playbook management How to manage playbooks : Monitor and maintain your playbook workflows. How to automate data profiling : Set up automated data quality checks. Troubleshooting playbooks : Solutions for common playbook issues."}
{"url":"https:\/\/docs.atlan.com\/product\/capabilities\/governance\/contracts","title":"Contracts | Atlan Documentation","text":"Get started Follow these steps to implement contracts in Atlan: Create data contracts Add contract impact analysis in GitHub : Detailed instructions on adding contracts for impact analysis in GitHub."}
{"url":"https:\/\/docs.atlan.com\/product\/integrations","title":"Integrations | Atlan Documentation","text":"Atlan integrates with a wide range of tools to help you automate workflows, connect with your favorite apps, and manage identity and access. These integrations connect your data catalog with the tools your teams already use, creating a seamless data experience across your tech stack. Key concepts Integration categories : Atlan offers integrations across five categories: project management, communication, collaboration, automation, and identity management. Connection methods : Most integrations use secure authentication methods like OAuth, API keys, or service accounts. Bi-directional sync : Updates flow between Atlan and integrated tools, ensuring data consistency across platforms. Custom webhooks : Extend Atlan's capabilities by building custom integrations using the provided APIs and webhooks. Core offerings Connect with platforms like AWS Lambda to automate data workflows and streamline routine tasks. Integrate with tools like Slack and Microsoft Teams to enhance team collaboration and knowledge sharing. Connect with SMTP for real-time alerts. Integrate with identity providers like Okta and Azure AD for seamless authentication and user management. Connect with tools like Jira and Service Now to link data assets to projects and track data-related tasks. Get started Select an integration Choose from Atlan's available integrations based on your team's tools and workflows. Follow the integration-specific setup guide to establish a secure connection with your tool. Test and activate Verify the integration is working correctly with a test action, then activate for your organization. Need a custom integration? Atlan provides APIs and webhooks that let you build custom integrations with any tool in your tech stack."}
{"url":"https:\/\/developer.atlan.com\/getting-started\/","title":"Introductory walkthrough - Developer","text":"An introductory walkthrough You might also like our Atlan Platform Essentials certification . Not sure where to start? Allow us to introduce Atlan development through example. 1 We strongly recommend using one of our SDKs to simplify the development process. As a first step, set one up: The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include logback as a simple binding mechanism to send any logging information out to your console (standard out). Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on PyPI . You can use pip to install it as follows: Provide two values to create an Atlan client: Provide your Atlan tenant URL to the base_url parameter. (You can also do this through environment variables .) Provide your API token to the api_key parameter. (You can also do this through environment variables .) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include slf4j-simple as a simple binding mechanism to send any logging information out to your console (standard out), along with the kotlin-logging-jvm microutil. Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on GitHub , ready to be included in your project: Provide two values to set up connectivity to Atlan: Provide your Atlan tenant URL to the assets.Context() method. If you prefer using the value from an environment variable, you can use assets.NewContext() without any parameters. Provide your API token as the second parameter to the assets.Context() method. (Or again, have it picked up automatically by the assets.NewContext() method.) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. Don't forget to give permissions If you want to be able to access existing metadata with an API token, don't forget that you need to assign one or more personas to the API token that grant it access to metadata. Now that you have an SDK installed and configured, you are ready to code! Before we jump straight to code, though, let's first introduce some key concepts in Atlan: What is an asset? In Atlan, we refer to all objects that provide context to your data as assets . Each type of asset in Atlan has a set of: Properties , such as: Certificates Announcements Properties , such as: Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table In an object-oriented programming sense, think of an asset as an instance of a class. The structure of an asset (the class itself, in this analogy) is defined by something called a type definition , but that's for another day. So as you can see: There are many different kinds of assets: tables, columns, schemas, databases, business intelligence dashboards, reports, and so on. Assets inter-relate with each other: a table has a parent schema and child columns, a schema has a parent database and child tables, and so on. Different kinds of assets have some common properties (like certificates) and other properties that are unique to that kind of asset (like a columnCount that only exists on tables, not on schemas or databases). When you know the asset When you already know which asset you want to retrieve, you can read it from Atlan using one of its identifiers . We'll discuss these in more detail as part of updates, but for now you can think of them as: is a primary key for an asset: completely unique, but meaningless by itself is a business key for an asset: unique for a given kind of asset, and interpretable You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the asset.get_by_guid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the asset.get_by_qualified_name() method on the Atlan client, providing the type of asset you expect to retrieve and its qualified_name . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the assets.GetByGuid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the assets.GetByQualifiedName() method on the Atlan client, providing the type of asset you expect to retrieve and its qualifiedName . (Each asset type is its own unique class in the SDK.) Note that the response is strongly typed: If you are retrieving a table, you will get a table back (as long as it exists). You do not need to figure out what properties or relationships exist on a table - the Table class defines them for for you already. In any modern IDE, this means you have type-ahead support for retrieving the properties and relationships from the table variable. You can also refer to the types reference in this portal for full details of every kind of asset. Retrieval by identifier can be more costly than you might expect Even though you are retrieving an asset by an identifier, this can be more costly than you might expect. Retrieving an asset in this way will: Retrieve all its properties and their values Retrieve all its relationships Imagine the asset you are retrieving has 100's or 1000's of these. If you only care about its certificate and any owners, you will be retrieving far more information than you need. When you need to find it first For example, imagine you want to find all tables named MY_TABLE : You can then run the request using Execute() . For example, if you want to know the certificate of the asset you only need to tack that onto the query: Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many include_on_results calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can include as many attributes in IncludeOnResults as you want to specify the properties and relationships you want to retrieve for matching assets. Also gives the best performance If all you want to do is check or report on metadata, you should have a starting point from the information above. Or, now that you've found an asset of interest, maybe you want to update the asset with additional metadata ? Once again, before we jump to code, let's first understand some key concepts about how Atlan handles updates: Importance of identifiers Most operations on assets are upserts , that is, they could either create (insert) or update a given asset. How do you know which is going to happen? To answer this question, you need to understand how Atlan uniquely identifies each asset. Recall earlier we discussed asset's different identifiers in Atlan . Every asset in Atlan has at least the following two unique identifiers. These are both mandatory for every asset, so no asset can exist without these: Atlan uses globally-unique identifiers (GUIDs) to uniquely identify each asset, globally . They look something like this: As the name implies, GUIDs are: Globally unique (across all systems). Generated in a way that makes it nearly impossible for anything else to ever generate that same ID. 2 Note that this means the GUID itself is not : Meaningful or capable of being interpreted in any way Atlan uses qualifiedName s to uniquely identify assets based on their characteristics. They look something like this: Qualified names are not : Globally unique (across all systems). Instead, they are: Consistently constructed in a meaningful way, making it possible for them to be reconstructed. Note that this means the qualifiedName is: Meaningful and capable of being interpreted How these impact updates Since they are truly unique, operations that include a GUID will only update an asset, not create one. Conversely, operations that take a qualifiedName can: Create an asset, if no exactly-matching qualifiedName is found in Atlan. Update an asset, if an exact-match for the qualifiedName is found in Atlan. These operations also require a typeName , so that if creation does occur the correct type of asset is created. Unintended consequences of this behavior Be careful when using operations with only the qualifiedName . You may end up creating assets when you were only expecting them to be updated or to fail if they did not already exist. This is particularly true when you do not give the exact, case-sensitive qualifiedName of an asset. a\/b\/c\/d is not the same as a\/B\/c\/d when it comes to qualifiedName s. Perhaps this leaves you wondering: why have a qualifiedName at all? The qualifiedName 's purpose is to identify what is a unique asset. Many different tools might all have information about that asset. Having a common \"identity\" means that many different systems can each independently construct its identifier the same way. If a crawler gets table details from Snowflake it can upsert based on those identity characteristics in Atlan. The crawler will not create duplicate tables every time it runs. This gives idempotency. Looker knows the same identity characteristics for the Snowflake tables and columns. So if you get details from Looker about the tables it uses for reporting, you can link them together in lineage. (Looker can construct the same identifier for the table as Snowflake itself.) These characteristics are not possible using GUIDs alone. Limit to changes only Now that you understand the nuances of identifiers, let's look at how you can update metadata in Atlan. In general, you only need to send changes to Atlan. You do not need to send an entire asset each time you want to make changes to it. For example, imagine you want to mark a table as certified but do not want to change anything else (its name, description, owner details, and so on): You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualified_name . Using the updater() class method on any asset type, you pass in (typically) the qualified_name and name of the asset. You can then add onto the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to client.asset.save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the Updater() method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns an object into which you can then place any updates. You can place into the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to .Save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. Atlan will handle idempotency By sending only the changes you want to apply, Atlan can make idempotent updates. Atlan will only attempt to update the asset with the changes you send. Atlan leaves any existing metadata on the asset as-is. If the asset already has the metadata values you are sending, Atlan does nothing. It will not even update audit details like the last update timestamp, and is thus idempotent. What if you want to make changes to many assets, as efficiently as possible? Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Where to go from here Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Real code samples our customers use to solve particular use cases. Review live samples Events Delve deep into the details of the events Atlan triggers. Learn more about events Delve deep into the details of the events Atlan triggers. Learn more about events Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†©"}
{"url":"https:\/\/developer.atlan.com\/sdks\/","title":"Integration options - Developer","text":"Throughout the portal you can focus on your preferred integration approach (and switch between them as you like): CLI Use the Atlan CLI to manage data contracts for assets in Atlan. Get started with CLI Use the Atlan CLI to manage data contracts for assets in Atlan. Get started with CLI dbt Use dbt's meta field to enrich metadata resources straight from dbt into Atlan. Get started with dbt Use dbt's meta field to enrich metadata resources straight from dbt into Atlan. Get started with dbt Java Pull our Java SDK from Maven Central, just like any other dependency. Get started with Java Pull our Java SDK from Maven Central, just like any other dependency. Get started with Java Python Pull our Python SDK from PyPI, just like any other dependency. Get started with Python Pull our Python SDK from PyPI, just like any other dependency. Get started with Python Kotlin Pull our Java SDK from Maven Central, just like any other dependency. Get started with Kotlin Pull our Java SDK from Maven Central, just like any other dependency. Get started with Kotlin Scala Pull our Java SDK from Maven Central, just like any other dependency. Get started with Scala Pull our Java SDK from Maven Central, just like any other dependency. Get started with Scala Clojure Pull our Java SDK from Maven Central, just like any other dependency. Get started with Clojure Pull our Java SDK from Maven Central, just like any other dependency. Get started with Clojure Go Pull our Go SDK from GitHub, just like any other dependency. Get started with Go Pull our Go SDK from GitHub, just like any other dependency. Get started with Go Events Tap into events Atlan produces to take immediate action, as metadata changes. Get started with events Tap into events Atlan produces to take immediate action, as metadata changes. Get started with events Raw REST API You can call directly into our REST API, though we would recommend the SDKs. Get started with raw REST APIs Raw REST API You can call directly into our REST API, though we would recommend the SDKs. Get started with raw REST APIs"}
{"url":"https:\/\/docs.atlan.com\/support\/references\/customer-support","title":"Customer support | Atlan Documentation","text":"One of Atlan's core values is to help you and your team do your life's best work. Ã° That's why Atlan wants to make it as easy as possible for you to keep driving your work forward with data. Atlan's customer support is a combination of several teams in Atlan: Product support personnel Cloud support personnel DevOps\/engineering support personnel Vast repository of self-service resources Service-level commitment Atlan's Technical Support team provides support globally with high response commitment levels. This includes 24\/7 SRE support for critical (P0) issues. Customers get a service-level commitment, including the following: 99.5% uptime for Atlan Dedicated support center, available from within the Atlan product Commitments for aggressive response times for business critical issues Designated Customer Success Manager to assist with escalations Ways to contact support Ã¯Â¸ Email support at a dedicated customer support email account ( [email protected] ) Ã° Â¨ Ã° Â» In-product support widget to log tickets and a help desk portal to log and track tickets. You can sign up to track support tickets on the help desk portal. You must use your organizational email address as the username and create a password. Ã° Submit a support request via the online form. To track your support tickets: Navigate to https:\/\/atlan.zendesk.com and log into the help desk portal with your credentials or via SSO. From the top right, click your avatar, and then from the dropdown, click My activities . On the My activities page, you can do the following: My requests and Requests I'm CC'd on - view and edit the support tickets you either created or were copied on, respectively. Organization requests - to access all other support tickets for your organization, please reach out to your customer success manager. Atlan provides you with read access to all the support tickets for your organization. To be able to comment on or close them, you must be CC'd on all tickets. My requests and Requests I'm CC'd on - view and edit the support tickets you either created or were copied on, respectively. Organization requests - to access all other support tickets for your organization, please reach out to your customer success manager. Atlan provides you with read access to all the support tickets for your organization. To be able to comment on or close them, you must be CC'd on all tickets. Hours of operation 24x7 availability for all requests and issues Severity levels The Atlan Technical Support team determines the severity of an issue. The customer's position is considered, and these guidelines are followed to determine priority. Below are the response time SLAs: Escalation procedure If the business impact of a support request changes or a ticket isn't being handled according to your expectations, you may escalate the ticket. Please first speak with the Technical Support representative assigned to the ticket to confirm that the business impact and urgency are understood. You may further escalate by contacting: 1st level of escalation : Technical Support Engineer 2nd level of escalation : Director, Support 3rd level of escalation : Head of Customer Experience Ways to contact support Hours of operation"}
{"url":"https:\/\/developer.atlan.com\/concepts\/","title":"Developing with Atlan - Developer","text":"Overall site map If you are new to Atlan, or to developing with Atlan, start with one of the following two options. These will set you up to develop with Atlan, step-by-step. Atlan University Self-paced, video-based walkthrough of the essentials of Atlan as a platform. Atlan Platform Essentials certification Self-paced, video-based walkthrough of the essentials of Atlan as a platform. Atlan Platform Essentials certification Introductory walkthrough Documentation-based walkthrough, including step-by-step examples. Start the walkthrough Documentation-based walkthrough, including step-by-step examples. Start the walkthrough Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Real code samples our customers use to solve particular use cases. Review live samples Events Delve deep into the details of the events Atlan triggers. Learn more about events Delve deep into the details of the events Atlan triggers. Learn more about events"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/snowflake\/how-tos\/set-up-snowflake#__docusaurus_skipToContent_fallback","title":"Set up Snowflake | Atlan Documentation","text":"You need your Snowflake administrator to run these commands - you may not have access yourself. Create user and role in Snowflake Create a role and user in Snowflake using the following commands: Create role Create a role in Snowflake using the following commands: Replace with the default warehouse to use when running the Snowflake crawler . Atlan requires the following privileges to: OPERATE enables Atlan to start the virtual warehouse to fetch metadata if the warehouse has stopped. USAGE enables Atlan to show or list metadata from Snowflake. This in turn enables the Snowflake crawler to run the SHOW query. Create a user Create a separate user to integrate into Atlan, using one of the following 3 options: See Snowflake's official guide for details on generating an RSA key-pair . To create a user with a key-pair, replace the value for rsa_public_key with the public key and run the following: Learn more about the SERVICE type property in Snowflake documentation . Atlan only supports encrypted private keys with a non-empty passphrase - generally recommended as more secure. An empty passphrase results in workflow failures. To generate an encrypted private key, omit the -nocrypt option. Refer to Snowflake documentation to learn more. Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. To create a user with a password, replace and run the following: Learn more about the LEGACY_SERVICE type property in Snowflake documentation . This method is currently only available if Okta is your IdP (Snowflake supports) authenticating natively through Okta : Create a user in your identity provider (IdP) and use federated authentication in Snowflake . The password for this user must be maintained solely in the IdP and multi-factor authentication (MFA) must be disabled. Grant role to user To grant the atlan_user_role to the new user: Configure OAuth (client credentials flow) with Microsoft Entra ID To configure OAuth authentication using Microsoft Entra ID (formerly Azure AD) with the client credentials flow: Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions In Snowflake, create a security integration using the following: CREATE SECURITY INTEGRATION external_oauth_azure_ad TYPE = external_oauth ENABLED = true EXTERNAL_OAUTH_TYPE = azure EXTERNAL_OAUTH_ISSUER = '\\ ' EXTERNAL_OAUTH_JWS_KEYS_URL = '\\ ' EXTERNAL_OAUTH_AUDIENCE_LIST = ( '\\ ' ) EXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM = 'sub' EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE = 'login_name' ; Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app In Snowflake, create a security integration using the following: Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: CREATE USER oauth_svc_user WITH LOGIN_NAME = '\\ ' -- Use Azure client OBJECT ID DEFAULT_ROLE = \\ DEFAULT_WAREHOUSE = \\ ; Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: Grant the configured role to this user: GRANT ROLE \\ TO USER oauth_svc_user ; Grant the configured role to this user: Choose metadata fetching method Atlan supports two methods for fetching metadata from Snowflake - account usage and information schema. You should choose one of these two methods to set up Snowflake: Grant permissions for account usage method If you want to set warehouse timeouts when using this method, set a large value initially for the workflow to succeed. Once the workflow has succeeded, adjust the value to be twice the extraction time. This method uses the views in SNOWFLAKE.ACCOUNT_USAGE (or a copied version of this schema) to fetch the metadata from Snowflake into Atlan. You can be more granular with permissions using this method, but there are limitations with this approach . To crawl assets, generate lineage, and import tags If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Snowflake stores all tag objects in the ACCOUNT_USAGE schema. If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner , you need to grant the same permissions to import tags as required for crawling Snowflake assets. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. To crawl streams To crawl streams, provide the following permissions: To crawl current streams: Replace with the Snowflake database name. Replace with the Snowflake database name. To crawl future streams: To crawl future streams: Replace with the Snowflake database name. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. Grant permissions for information schema method This method uses views in the INFORMATION_SCHEMA schema in Snowflake databases to fetch metadata. You still need to grant specific permissions to enable Atlan to crawl metadata, preview data, and query data with this method. To crawl existing assets Grant these permissions to crawl assets that already exist in Snowflake. If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Grant permissions to crawl existing assets: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) Grant permissions to crawl functions: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) For secure user-defined functions (UDFs), grant OWNERSHIP permissions to retrieve metadata: Replace the placeholders with the appropriate values: : The name of the schema that contains the user-defined function (UDF). : The name of the secure UDF that requires ownership permissions. : The role that gets assigned ownership of the secure UDF. The statements given on this page apply to all schemas, tables, and views in a database in Snowflake. If you want to limit access to only certain objects, you can instead specify the exact objects individually as well. To crawl future assets To crawl assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. To grant permissions at a database level: Replace with the database you want to crawl in Atlan. (Repeat the statements for every database you want to integrate into Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) To mine query history for lineage To also mine Snowflake's query history (for lineage), add these permissions. You can use either option: To mine query history direct from Snowflake's internal tables: To mine query history from a cloned or copied set of tables, where you can also remove any sensitive data: Replace with the name of the cloned database, and with the name of the cloned schema containing account usage details. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to be able to preview and query in Atlan. (Repeat the statements for every database and schema you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. (Optional) To import Snowflake tags Snowflake stores all tag objects in the ACCOUNT_USAGE schema. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To import tags from Snowflake , grant these permissions: To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. (Optional) To push updated tags to Snowflake To push tags updated for assets in Atlan to Snowflake , grant these permissions: You can learn more about tag privileges from Snowflake documentation . (Optional) To crawl dynamic tables Atlan currently supports fetching metadata for dynamic tables using the MONITOR privilege. Refer to Snowflake documentation to learn more. To crawl existing dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: To crawl future dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) (Optional) To crawl Iceberg tables Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method. To crawl Iceberg tables from Snowflake, grant the following permissions: To crawl existing Iceberg tables in Snowflake: To crawl future Iceberg tables in Snowflake: To crawl Iceberg catalog metadata for Iceberg tables in Snowflake: You must first grant permissions to crawl existing Iceberg tables for this permission to work on catalogs. You must also grant permissions to all the catalogs you want to crawl in Atlan individually. (Optional) To crawl Snowflake stages Atlan supports crawling metadata for Snowflake stages using the USAGE and READ privileges. For more information, see the Snowflake documentation for INFORMATION_SCHEMA.STAGES. To crawl stages from Snowflake: Grant USAGE and READ privileges on all existing stages at the database level: Replace with the name of your Snowflake database Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Replace with the role you've granted Atlan to use for crawling. Grant USAGE and READ privileges on all future stages at the database level: Grant USAGE and READ privileges on all future stages at the database level: Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Allowlist the Atlan IP If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support ticket from within Atlan, or submit a request . (If you aren't using the IP allowlist in your Snowflake instance, you can skip this step.) Create user and role in Snowflake Choose metadata fetching method Grant permissions for account usage method Grant permissions for information schema method Allowlist the Atlan IP"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/snowflake\/how-tos\/set-up-snowflake#create-user-and-role-in-snowflake","title":"Set up Snowflake | Atlan Documentation","text":"You need your Snowflake administrator to run these commands - you may not have access yourself. Create user and role in Snowflake Create a role and user in Snowflake using the following commands: Create role Create a role in Snowflake using the following commands: Replace with the default warehouse to use when running the Snowflake crawler . Atlan requires the following privileges to: OPERATE enables Atlan to start the virtual warehouse to fetch metadata if the warehouse has stopped. USAGE enables Atlan to show or list metadata from Snowflake. This in turn enables the Snowflake crawler to run the SHOW query. Create a user Create a separate user to integrate into Atlan, using one of the following 3 options: See Snowflake's official guide for details on generating an RSA key-pair . To create a user with a key-pair, replace the value for rsa_public_key with the public key and run the following: Learn more about the SERVICE type property in Snowflake documentation . Atlan only supports encrypted private keys with a non-empty passphrase - generally recommended as more secure. An empty passphrase results in workflow failures. To generate an encrypted private key, omit the -nocrypt option. Refer to Snowflake documentation to learn more. Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. To create a user with a password, replace and run the following: Learn more about the LEGACY_SERVICE type property in Snowflake documentation . This method is currently only available if Okta is your IdP (Snowflake supports) authenticating natively through Okta : Create a user in your identity provider (IdP) and use federated authentication in Snowflake . The password for this user must be maintained solely in the IdP and multi-factor authentication (MFA) must be disabled. Grant role to user To grant the atlan_user_role to the new user: Configure OAuth (client credentials flow) with Microsoft Entra ID To configure OAuth authentication using Microsoft Entra ID (formerly Azure AD) with the client credentials flow: Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions In Snowflake, create a security integration using the following: CREATE SECURITY INTEGRATION external_oauth_azure_ad TYPE = external_oauth ENABLED = true EXTERNAL_OAUTH_TYPE = azure EXTERNAL_OAUTH_ISSUER = '\\ ' EXTERNAL_OAUTH_JWS_KEYS_URL = '\\ ' EXTERNAL_OAUTH_AUDIENCE_LIST = ( '\\ ' ) EXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM = 'sub' EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE = 'login_name' ; Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app In Snowflake, create a security integration using the following: Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: CREATE USER oauth_svc_user WITH LOGIN_NAME = '\\ ' -- Use Azure client OBJECT ID DEFAULT_ROLE = \\ DEFAULT_WAREHOUSE = \\ ; Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: Grant the configured role to this user: GRANT ROLE \\ TO USER oauth_svc_user ; Grant the configured role to this user: Choose metadata fetching method Atlan supports two methods for fetching metadata from Snowflake - account usage and information schema. You should choose one of these two methods to set up Snowflake: Grant permissions for account usage method If you want to set warehouse timeouts when using this method, set a large value initially for the workflow to succeed. Once the workflow has succeeded, adjust the value to be twice the extraction time. This method uses the views in SNOWFLAKE.ACCOUNT_USAGE (or a copied version of this schema) to fetch the metadata from Snowflake into Atlan. You can be more granular with permissions using this method, but there are limitations with this approach . To crawl assets, generate lineage, and import tags If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Snowflake stores all tag objects in the ACCOUNT_USAGE schema. If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner , you need to grant the same permissions to import tags as required for crawling Snowflake assets. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. To crawl streams To crawl streams, provide the following permissions: To crawl current streams: Replace with the Snowflake database name. Replace with the Snowflake database name. To crawl future streams: To crawl future streams: Replace with the Snowflake database name. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. Grant permissions for information schema method This method uses views in the INFORMATION_SCHEMA schema in Snowflake databases to fetch metadata. You still need to grant specific permissions to enable Atlan to crawl metadata, preview data, and query data with this method. To crawl existing assets Grant these permissions to crawl assets that already exist in Snowflake. If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Grant permissions to crawl existing assets: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) Grant permissions to crawl functions: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) For secure user-defined functions (UDFs), grant OWNERSHIP permissions to retrieve metadata: Replace the placeholders with the appropriate values: : The name of the schema that contains the user-defined function (UDF). : The name of the secure UDF that requires ownership permissions. : The role that gets assigned ownership of the secure UDF. The statements given on this page apply to all schemas, tables, and views in a database in Snowflake. If you want to limit access to only certain objects, you can instead specify the exact objects individually as well. To crawl future assets To crawl assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. To grant permissions at a database level: Replace with the database you want to crawl in Atlan. (Repeat the statements for every database you want to integrate into Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) To mine query history for lineage To also mine Snowflake's query history (for lineage), add these permissions. You can use either option: To mine query history direct from Snowflake's internal tables: To mine query history from a cloned or copied set of tables, where you can also remove any sensitive data: Replace with the name of the cloned database, and with the name of the cloned schema containing account usage details. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to be able to preview and query in Atlan. (Repeat the statements for every database and schema you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. (Optional) To import Snowflake tags Snowflake stores all tag objects in the ACCOUNT_USAGE schema. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To import tags from Snowflake , grant these permissions: To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. (Optional) To push updated tags to Snowflake To push tags updated for assets in Atlan to Snowflake , grant these permissions: You can learn more about tag privileges from Snowflake documentation . (Optional) To crawl dynamic tables Atlan currently supports fetching metadata for dynamic tables using the MONITOR privilege. Refer to Snowflake documentation to learn more. To crawl existing dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: To crawl future dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) (Optional) To crawl Iceberg tables Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method. To crawl Iceberg tables from Snowflake, grant the following permissions: To crawl existing Iceberg tables in Snowflake: To crawl future Iceberg tables in Snowflake: To crawl Iceberg catalog metadata for Iceberg tables in Snowflake: You must first grant permissions to crawl existing Iceberg tables for this permission to work on catalogs. You must also grant permissions to all the catalogs you want to crawl in Atlan individually. (Optional) To crawl Snowflake stages Atlan supports crawling metadata for Snowflake stages using the USAGE and READ privileges. For more information, see the Snowflake documentation for INFORMATION_SCHEMA.STAGES. To crawl stages from Snowflake: Grant USAGE and READ privileges on all existing stages at the database level: Replace with the name of your Snowflake database Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Replace with the role you've granted Atlan to use for crawling. Grant USAGE and READ privileges on all future stages at the database level: Grant USAGE and READ privileges on all future stages at the database level: Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Allowlist the Atlan IP If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support ticket from within Atlan, or submit a request . (If you aren't using the IP allowlist in your Snowflake instance, you can skip this step.) Create user and role in Snowflake Choose metadata fetching method Grant permissions for account usage method Grant permissions for information schema method Allowlist the Atlan IP"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/snowflake\/how-tos\/set-up-snowflake#create-role","title":"Set up Snowflake | Atlan Documentation","text":"You need your Snowflake administrator to run these commands - you may not have access yourself. Create user and role in Snowflake Create a role and user in Snowflake using the following commands: Create role Create a role in Snowflake using the following commands: Replace with the default warehouse to use when running the Snowflake crawler . Atlan requires the following privileges to: OPERATE enables Atlan to start the virtual warehouse to fetch metadata if the warehouse has stopped. USAGE enables Atlan to show or list metadata from Snowflake. This in turn enables the Snowflake crawler to run the SHOW query. Create a user Create a separate user to integrate into Atlan, using one of the following 3 options: See Snowflake's official guide for details on generating an RSA key-pair . To create a user with a key-pair, replace the value for rsa_public_key with the public key and run the following: Learn more about the SERVICE type property in Snowflake documentation . Atlan only supports encrypted private keys with a non-empty passphrase - generally recommended as more secure. An empty passphrase results in workflow failures. To generate an encrypted private key, omit the -nocrypt option. Refer to Snowflake documentation to learn more. Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. To create a user with a password, replace and run the following: Learn more about the LEGACY_SERVICE type property in Snowflake documentation . This method is currently only available if Okta is your IdP (Snowflake supports) authenticating natively through Okta : Create a user in your identity provider (IdP) and use federated authentication in Snowflake . The password for this user must be maintained solely in the IdP and multi-factor authentication (MFA) must be disabled. Grant role to user To grant the atlan_user_role to the new user: Configure OAuth (client credentials flow) with Microsoft Entra ID To configure OAuth authentication using Microsoft Entra ID (formerly Azure AD) with the client credentials flow: Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions In Snowflake, create a security integration using the following: CREATE SECURITY INTEGRATION external_oauth_azure_ad TYPE = external_oauth ENABLED = true EXTERNAL_OAUTH_TYPE = azure EXTERNAL_OAUTH_ISSUER = '\\ ' EXTERNAL_OAUTH_JWS_KEYS_URL = '\\ ' EXTERNAL_OAUTH_AUDIENCE_LIST = ( '\\ ' ) EXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM = 'sub' EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE = 'login_name' ; Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app In Snowflake, create a security integration using the following: Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: CREATE USER oauth_svc_user WITH LOGIN_NAME = '\\ ' -- Use Azure client OBJECT ID DEFAULT_ROLE = \\ DEFAULT_WAREHOUSE = \\ ; Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: Grant the configured role to this user: GRANT ROLE \\ TO USER oauth_svc_user ; Grant the configured role to this user: Choose metadata fetching method Atlan supports two methods for fetching metadata from Snowflake - account usage and information schema. You should choose one of these two methods to set up Snowflake: Grant permissions for account usage method If you want to set warehouse timeouts when using this method, set a large value initially for the workflow to succeed. Once the workflow has succeeded, adjust the value to be twice the extraction time. This method uses the views in SNOWFLAKE.ACCOUNT_USAGE (or a copied version of this schema) to fetch the metadata from Snowflake into Atlan. You can be more granular with permissions using this method, but there are limitations with this approach . To crawl assets, generate lineage, and import tags If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Snowflake stores all tag objects in the ACCOUNT_USAGE schema. If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner , you need to grant the same permissions to import tags as required for crawling Snowflake assets. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. To crawl streams To crawl streams, provide the following permissions: To crawl current streams: Replace with the Snowflake database name. Replace with the Snowflake database name. To crawl future streams: To crawl future streams: Replace with the Snowflake database name. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. Grant permissions for information schema method This method uses views in the INFORMATION_SCHEMA schema in Snowflake databases to fetch metadata. You still need to grant specific permissions to enable Atlan to crawl metadata, preview data, and query data with this method. To crawl existing assets Grant these permissions to crawl assets that already exist in Snowflake. If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Grant permissions to crawl existing assets: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) Grant permissions to crawl functions: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) For secure user-defined functions (UDFs), grant OWNERSHIP permissions to retrieve metadata: Replace the placeholders with the appropriate values: : The name of the schema that contains the user-defined function (UDF). : The name of the secure UDF that requires ownership permissions. : The role that gets assigned ownership of the secure UDF. The statements given on this page apply to all schemas, tables, and views in a database in Snowflake. If you want to limit access to only certain objects, you can instead specify the exact objects individually as well. To crawl future assets To crawl assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. To grant permissions at a database level: Replace with the database you want to crawl in Atlan. (Repeat the statements for every database you want to integrate into Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) To mine query history for lineage To also mine Snowflake's query history (for lineage), add these permissions. You can use either option: To mine query history direct from Snowflake's internal tables: To mine query history from a cloned or copied set of tables, where you can also remove any sensitive data: Replace with the name of the cloned database, and with the name of the cloned schema containing account usage details. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to be able to preview and query in Atlan. (Repeat the statements for every database and schema you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. (Optional) To import Snowflake tags Snowflake stores all tag objects in the ACCOUNT_USAGE schema. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To import tags from Snowflake , grant these permissions: To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. (Optional) To push updated tags to Snowflake To push tags updated for assets in Atlan to Snowflake , grant these permissions: You can learn more about tag privileges from Snowflake documentation . (Optional) To crawl dynamic tables Atlan currently supports fetching metadata for dynamic tables using the MONITOR privilege. Refer to Snowflake documentation to learn more. To crawl existing dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: To crawl future dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) (Optional) To crawl Iceberg tables Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method. To crawl Iceberg tables from Snowflake, grant the following permissions: To crawl existing Iceberg tables in Snowflake: To crawl future Iceberg tables in Snowflake: To crawl Iceberg catalog metadata for Iceberg tables in Snowflake: You must first grant permissions to crawl existing Iceberg tables for this permission to work on catalogs. You must also grant permissions to all the catalogs you want to crawl in Atlan individually. (Optional) To crawl Snowflake stages Atlan supports crawling metadata for Snowflake stages using the USAGE and READ privileges. For more information, see the Snowflake documentation for INFORMATION_SCHEMA.STAGES. To crawl stages from Snowflake: Grant USAGE and READ privileges on all existing stages at the database level: Replace with the name of your Snowflake database Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Replace with the role you've granted Atlan to use for crawling. Grant USAGE and READ privileges on all future stages at the database level: Grant USAGE and READ privileges on all future stages at the database level: Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Allowlist the Atlan IP If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support ticket from within Atlan, or submit a request . (If you aren't using the IP allowlist in your Snowflake instance, you can skip this step.) Create user and role in Snowflake Choose metadata fetching method Grant permissions for account usage method Grant permissions for information schema method Allowlist the Atlan IP"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/snowflake\/how-tos\/crawl-snowflake","title":"Crawl Snowflake | Atlan Documentation","text":"Once you have configured the Snowflake user permissions , you can establish a connection between Atlan and Snowflake. (If you are also using AWS PrivateLink or Azure Private Link for Snowflake, you will need to set that up first, too.) To crawl metadata from Snowflake, review the order of operations and then complete the following steps. Select the source To select Snowflake as your source: In the top right of any screen, navigate to New and then click New Workflow . From the list of packages, select Snowflake Assets and click on Setup Workflow . Provide credentials Choose your extraction method: In Direct extraction, Atlan connects to your database and crawls metadata directly. In Offline extraction, you will need to first extract metadata yourself and make it available in S3 . This is currently only supported when using the information schema extraction method to fetch metadata with basic authentication . In Agent extraction, Atlan's secure agent executes metadata extraction within the organization's environment. Direct extraction method To enter your Snowflake credentials: For Account Identifiers (Host) , enter the hostname, AWS PrivateLink endpoint , or Azure Private Link endpoint for your Snowflake instance. For Authentication , choose the method you configured when setting up the Snowflake user : For Basic authentication, enter the Username and Password you configured in either Snowflake or the identity provider. info Ã° Âª Did you know? Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. For Keypair authentication, enter the Username , Encrypted Private Key , and Private Key Password you configured. Atlan only supports encrypted private keys with a non-empty passphrase - generally recommended as more secure. An empty passphrase will result in workflow failures. To generate an encrypted private key, refer to Snowflake documentation . For Okta SSO authentication, enter the Username , Password , and Authenticator you configured. The Authenticator will be the Okta URL endpoint of your Okta account , typically in the form of https:\/\/ .okta.com . For Basic authentication, enter the Username and Password you configured in either Snowflake or the identity provider. info Ã° Âª Did you know? Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. For Basic authentication, enter the Username and Password you configured in either Snowflake or the identity provider. Ã° Âª Did you know? Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. For Keypair authentication, enter the Username , Encrypted Private Key , and Private Key Password you configured. Atlan only supports encrypted private keys with a non-empty passphrase - generally recommended as more secure. An empty passphrase will result in workflow failures. To generate an encrypted private key, refer to Snowflake documentation . For Keypair authentication, enter the Username , Encrypted Private Key , and Private Key Password you configured. Atlan only supports encrypted private keys with a non-empty passphrase - generally recommended as more secure. An empty passphrase will result in workflow failures. To generate an encrypted private key, refer to Snowflake documentation . For Okta SSO authentication, enter the Username , Password , and Authenticator you configured. The Authenticator will be the Okta URL endpoint of your Okta account , typically in the form of https:\/\/ .okta.com . For Okta SSO authentication, enter the Username , Password , and Authenticator you configured. The Authenticator will be the Okta URL endpoint of your Okta account , typically in the form of https:\/\/ .okta.com . For Role , select the Snowflake role through which the crawler should run. For Warehouse , select the Snowflake warehouse in which the crawler should run. Click Test Authentication to confirm connectivity to Snowflake using these details. Once successful, at the bottom of the screen, click Next . Offline extraction method Atlan supports the offline extraction method for fetching metadata from Snowflake. This method uses Atlan's metadata-extractor tool to fetch metadata. You will need to first extract the metadata yourself and then make it available in S3 . To enter your S3 details: For Bucket name , enter the name of your S3 bucket. If you are reusing Atlan's S3 bucket, you can leave this blank. For Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include databases.json , columns- .json , and so on. For Bucket region , enter the name of the S3 region. When complete, at the bottom of the screen, click Next . Configure the connection To complete the Snowflake connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection - not even admins. (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . If you do not specify any user or group, nobody will be able to manage the connection - not even admins. (Optional) To prevent users from querying any Snowflake data, change Allow SQL Query to No . (Optional) To prevent users from querying any Snowflake data, change Allow SQL Query to No . (Optional) To prevent users from previewing any Snowflake data, change Allow Data Preview to No . (Optional) To prevent users from previewing any Snowflake data, change Allow Data Preview to No . At the bottom of the screen, click Next to proceed. At the bottom of the screen, click Next to proceed. Agent extraction method Atlan supports using a Secure Agent for fetching metadata from Snowflake. To use a Secure Agent, follow these steps: Select the Agent tab. Configure the Snowflake data source by adding the secret keys for your secret store. For details on the required fields, refer to the Direct extraction section. Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. Click Next after completing the configuration. Configure the crawler When modifying an existing Snowflake connection, switching to a different extraction method will delete and recreate all assets in the existing connection. If you'd like to change the extraction method, contact Atlan support for assistance. Before running the Snowflake crawler, you can further configure it. You must select the Extraction method you configured when you set up Snowflake : For Information Schema method , keep the default selection. Change to Account Usage method and specify the following: Database Name of the copied Snowflake database Schema Name of the copied ACCOUNT_USAGE schema Incremental extraction Public preview - Toggle incremental extraction for faster and more efficient metadata extraction. Database Name of the copied Snowflake database Schema Name of the copied ACCOUNT_USAGE schema Incremental extraction Public preview - Toggle incremental extraction for faster and more efficient metadata extraction. You can override the defaults for any of the remaining options: For Asset selection , select a filtering option: To select the assets you want to include in crawling, click Include by hierarchy and filter for assets down to the database or schema level. (This will default to all assets, if none are specified.) To have the crawler include Databases , Schemas , or Tables & Views based on a naming convention, click Include by regex and specify a regular expression - for example, specifying ATLAN_EXAMPLE_DB.* for Databases will include all the matching databases and their child assets. To select the assets you want to exclude from crawling, click Exclude by hierarchy and filter for assets down to the database or schema level. (This will default to no assets, if none are specified.) To have the crawler ignore Databases , Schemas , or Tables & Views based on a naming convention, click Exclude by regex and specify a regular expression - for example, specifying ATLAN_EXAMPLE_TABLES.* for Tables & Views will exclude all the matching tables and views. Click + to add more filters. If you add multiple filters, assets will be crawled based on matching all the filtering conditions you have set. For Asset selection , select a filtering option: To select the assets you want to include in crawling, click Include by hierarchy and filter for assets down to the database or schema level. (This will default to all assets, if none are specified.) To have the crawler include Databases , Schemas , or Tables & Views based on a naming convention, click Include by regex and specify a regular expression - for example, specifying ATLAN_EXAMPLE_DB.* for Databases will include all the matching databases and their child assets. To select the assets you want to exclude from crawling, click Exclude by hierarchy and filter for assets down to the database or schema level. (This will default to no assets, if none are specified.) To have the crawler ignore Databases , Schemas , or Tables & Views based on a naming convention, click Exclude by regex and specify a regular expression - for example, specifying ATLAN_EXAMPLE_TABLES.* for Tables & Views will exclude all the matching tables and views. Click + to add more filters. If you add multiple filters, assets will be crawled based on matching all the filtering conditions you have set. To exclude lineage for views in Snowflake, change View Definition Lineage to No . To exclude lineage for views in Snowflake, change View Definition Lineage to No . To import tags from Snowflake to Atlan , change Import Tags to Yes . Note the following: If using the Account Usage extraction method, grant the same permissions as required for crawling Snowflake assets to import tags and push updated tags to Snowflake. If using the Information Schema extraction method, note that Snowflake stores all tag objects in the ACCOUNT_USAGE schema. You will need to grant permissions on the account usage schema instead to import tags from Snowflake. danger Object tagging in Snowflake currently requires Enterprise Edition or higher . If your organization does not have Enterprise Edition or higher and you try to import Snowflake tags to Atlan, the Snowflake connection will fail with an error - unable to retrieve tags. To import tags from Snowflake to Atlan , change Import Tags to Yes . Note the following: If using the Account Usage extraction method, grant the same permissions as required for crawling Snowflake assets to import tags and push updated tags to Snowflake. If using the Information Schema extraction method, note that Snowflake stores all tag objects in the ACCOUNT_USAGE schema. You will need to grant permissions on the account usage schema instead to import tags from Snowflake. Object tagging in Snowflake currently requires Enterprise Edition or higher . If your organization does not have Enterprise Edition or higher and you try to import Snowflake tags to Atlan, the Snowflake connection will fail with an error - unable to retrieve tags. For Control Config , keep Default for the default configuration or click Custom to further configure the crawler: If you have received a custom crawler configuration from Atlan support, for Custom Config , enter the value provided. You can also: Enter {\"ignore-all-case\": true} to enable crawling assets with case-sensitive identifiers. For Enable Source Level Filtering , click True to enable schema-level filtering at source or keep False to disable it. For Use JDBC Internal Methods , click True to enable JDBC internal methods for data extraction or click False to disable it. For Exclude tables with empty data , change to Yes to exclude any tables and corresponding columns without any data. For Exclude views , change to Yes to exclude all views from crawling. For Control Config , keep Default for the default configuration or click Custom to further configure the crawler: If you have received a custom crawler configuration from Atlan support, for Custom Config , enter the value provided. You can also: Enter {\"ignore-all-case\": true} to enable crawling assets with case-sensitive identifiers. Enter {\"ignore-all-case\": true} to enable crawling assets with case-sensitive identifiers. For Enable Source Level Filtering , click True to enable schema-level filtering at source or keep False to disable it. For Use JDBC Internal Methods , click True to enable JDBC internal methods for data extraction or click False to disable it. For Exclude tables with empty data , change to Yes to exclude any tables and corresponding columns without any data. For Exclude views , change to Yes to exclude all views from crawling. If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler To run the Snowflake crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets in Atlan's asset page! Ã° Note that the Atlan crawler will currently skip any unsupported data types to ensure a successful workflow run. Select the source Configure the connection Configure the crawler Run the crawler"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/snowflake\/how-tos\/set-up-snowflake#create-a-user","title":"Set up Snowflake | Atlan Documentation","text":"You need your Snowflake administrator to run these commands - you may not have access yourself. Create user and role in Snowflake Create a role and user in Snowflake using the following commands: Create role Create a role in Snowflake using the following commands: Replace with the default warehouse to use when running the Snowflake crawler . Atlan requires the following privileges to: OPERATE enables Atlan to start the virtual warehouse to fetch metadata if the warehouse has stopped. USAGE enables Atlan to show or list metadata from Snowflake. This in turn enables the Snowflake crawler to run the SHOW query. Create a user Create a separate user to integrate into Atlan, using one of the following 3 options: See Snowflake's official guide for details on generating an RSA key-pair . To create a user with a key-pair, replace the value for rsa_public_key with the public key and run the following: Learn more about the SERVICE type property in Snowflake documentation . Atlan only supports encrypted private keys with a non-empty passphrase - generally recommended as more secure. An empty passphrase results in workflow failures. To generate an encrypted private key, omit the -nocrypt option. Refer to Snowflake documentation to learn more. Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. To create a user with a password, replace and run the following: Learn more about the LEGACY_SERVICE type property in Snowflake documentation . This method is currently only available if Okta is your IdP (Snowflake supports) authenticating natively through Okta : Create a user in your identity provider (IdP) and use federated authentication in Snowflake . The password for this user must be maintained solely in the IdP and multi-factor authentication (MFA) must be disabled. Grant role to user To grant the atlan_user_role to the new user: Configure OAuth (client credentials flow) with Microsoft Entra ID To configure OAuth authentication using Microsoft Entra ID (formerly Azure AD) with the client credentials flow: Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions In Snowflake, create a security integration using the following: CREATE SECURITY INTEGRATION external_oauth_azure_ad TYPE = external_oauth ENABLED = true EXTERNAL_OAUTH_TYPE = azure EXTERNAL_OAUTH_ISSUER = '\\ ' EXTERNAL_OAUTH_JWS_KEYS_URL = '\\ ' EXTERNAL_OAUTH_AUDIENCE_LIST = ( '\\ ' ) EXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM = 'sub' EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE = 'login_name' ; Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app In Snowflake, create a security integration using the following: Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: CREATE USER oauth_svc_user WITH LOGIN_NAME = '\\ ' -- Use Azure client OBJECT ID DEFAULT_ROLE = \\ DEFAULT_WAREHOUSE = \\ ; Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: Grant the configured role to this user: GRANT ROLE \\ TO USER oauth_svc_user ; Grant the configured role to this user: Choose metadata fetching method Atlan supports two methods for fetching metadata from Snowflake - account usage and information schema. You should choose one of these two methods to set up Snowflake: Grant permissions for account usage method If you want to set warehouse timeouts when using this method, set a large value initially for the workflow to succeed. Once the workflow has succeeded, adjust the value to be twice the extraction time. This method uses the views in SNOWFLAKE.ACCOUNT_USAGE (or a copied version of this schema) to fetch the metadata from Snowflake into Atlan. You can be more granular with permissions using this method, but there are limitations with this approach . To crawl assets, generate lineage, and import tags If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Snowflake stores all tag objects in the ACCOUNT_USAGE schema. If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner , you need to grant the same permissions to import tags as required for crawling Snowflake assets. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. To crawl streams To crawl streams, provide the following permissions: To crawl current streams: Replace with the Snowflake database name. Replace with the Snowflake database name. To crawl future streams: To crawl future streams: Replace with the Snowflake database name. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. Grant permissions for information schema method This method uses views in the INFORMATION_SCHEMA schema in Snowflake databases to fetch metadata. You still need to grant specific permissions to enable Atlan to crawl metadata, preview data, and query data with this method. To crawl existing assets Grant these permissions to crawl assets that already exist in Snowflake. If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Grant permissions to crawl existing assets: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) Grant permissions to crawl functions: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) For secure user-defined functions (UDFs), grant OWNERSHIP permissions to retrieve metadata: Replace the placeholders with the appropriate values: : The name of the schema that contains the user-defined function (UDF). : The name of the secure UDF that requires ownership permissions. : The role that gets assigned ownership of the secure UDF. The statements given on this page apply to all schemas, tables, and views in a database in Snowflake. If you want to limit access to only certain objects, you can instead specify the exact objects individually as well. To crawl future assets To crawl assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. To grant permissions at a database level: Replace with the database you want to crawl in Atlan. (Repeat the statements for every database you want to integrate into Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) To mine query history for lineage To also mine Snowflake's query history (for lineage), add these permissions. You can use either option: To mine query history direct from Snowflake's internal tables: To mine query history from a cloned or copied set of tables, where you can also remove any sensitive data: Replace with the name of the cloned database, and with the name of the cloned schema containing account usage details. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to be able to preview and query in Atlan. (Repeat the statements for every database and schema you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. (Optional) To import Snowflake tags Snowflake stores all tag objects in the ACCOUNT_USAGE schema. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To import tags from Snowflake , grant these permissions: To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. (Optional) To push updated tags to Snowflake To push tags updated for assets in Atlan to Snowflake , grant these permissions: You can learn more about tag privileges from Snowflake documentation . (Optional) To crawl dynamic tables Atlan currently supports fetching metadata for dynamic tables using the MONITOR privilege. Refer to Snowflake documentation to learn more. To crawl existing dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: To crawl future dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) (Optional) To crawl Iceberg tables Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method. To crawl Iceberg tables from Snowflake, grant the following permissions: To crawl existing Iceberg tables in Snowflake: To crawl future Iceberg tables in Snowflake: To crawl Iceberg catalog metadata for Iceberg tables in Snowflake: You must first grant permissions to crawl existing Iceberg tables for this permission to work on catalogs. You must also grant permissions to all the catalogs you want to crawl in Atlan individually. (Optional) To crawl Snowflake stages Atlan supports crawling metadata for Snowflake stages using the USAGE and READ privileges. For more information, see the Snowflake documentation for INFORMATION_SCHEMA.STAGES. To crawl stages from Snowflake: Grant USAGE and READ privileges on all existing stages at the database level: Replace with the name of your Snowflake database Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Replace with the role you've granted Atlan to use for crawling. Grant USAGE and READ privileges on all future stages at the database level: Grant USAGE and READ privileges on all future stages at the database level: Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Allowlist the Atlan IP If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support ticket from within Atlan, or submit a request . (If you aren't using the IP allowlist in your Snowflake instance, you can skip this step.) Create user and role in Snowflake Choose metadata fetching method Grant permissions for account usage method Grant permissions for information schema method Allowlist the Atlan IP"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/snowflake\/how-tos\/set-up-snowflake#with-a-public-key-in-snowflake","title":"Set up Snowflake | Atlan Documentation","text":"You need your Snowflake administrator to run these commands - you may not have access yourself. Create user and role in Snowflake Create a role and user in Snowflake using the following commands: Create role Create a role in Snowflake using the following commands: Replace with the default warehouse to use when running the Snowflake crawler . Atlan requires the following privileges to: OPERATE enables Atlan to start the virtual warehouse to fetch metadata if the warehouse has stopped. USAGE enables Atlan to show or list metadata from Snowflake. This in turn enables the Snowflake crawler to run the SHOW query. Create a user Create a separate user to integrate into Atlan, using one of the following 3 options: See Snowflake's official guide for details on generating an RSA key-pair . To create a user with a key-pair, replace the value for rsa_public_key with the public key and run the following: Learn more about the SERVICE type property in Snowflake documentation . Atlan only supports encrypted private keys with a non-empty passphrase - generally recommended as more secure. An empty passphrase results in workflow failures. To generate an encrypted private key, omit the -nocrypt option. Refer to Snowflake documentation to learn more. Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. To create a user with a password, replace and run the following: Learn more about the LEGACY_SERVICE type property in Snowflake documentation . This method is currently only available if Okta is your IdP (Snowflake supports) authenticating natively through Okta : Create a user in your identity provider (IdP) and use federated authentication in Snowflake . The password for this user must be maintained solely in the IdP and multi-factor authentication (MFA) must be disabled. Grant role to user To grant the atlan_user_role to the new user: Configure OAuth (client credentials flow) with Microsoft Entra ID To configure OAuth authentication using Microsoft Entra ID (formerly Azure AD) with the client credentials flow: Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions In Snowflake, create a security integration using the following: CREATE SECURITY INTEGRATION external_oauth_azure_ad TYPE = external_oauth ENABLED = true EXTERNAL_OAUTH_TYPE = azure EXTERNAL_OAUTH_ISSUER = '\\ ' EXTERNAL_OAUTH_JWS_KEYS_URL = '\\ ' EXTERNAL_OAUTH_AUDIENCE_LIST = ( '\\ ' ) EXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM = 'sub' EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE = 'login_name' ; Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app In Snowflake, create a security integration using the following: Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: CREATE USER oauth_svc_user WITH LOGIN_NAME = '\\ ' -- Use Azure client OBJECT ID DEFAULT_ROLE = \\ DEFAULT_WAREHOUSE = \\ ; Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: Grant the configured role to this user: GRANT ROLE \\ TO USER oauth_svc_user ; Grant the configured role to this user: Choose metadata fetching method Atlan supports two methods for fetching metadata from Snowflake - account usage and information schema. You should choose one of these two methods to set up Snowflake: Grant permissions for account usage method If you want to set warehouse timeouts when using this method, set a large value initially for the workflow to succeed. Once the workflow has succeeded, adjust the value to be twice the extraction time. This method uses the views in SNOWFLAKE.ACCOUNT_USAGE (or a copied version of this schema) to fetch the metadata from Snowflake into Atlan. You can be more granular with permissions using this method, but there are limitations with this approach . To crawl assets, generate lineage, and import tags If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Snowflake stores all tag objects in the ACCOUNT_USAGE schema. If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner , you need to grant the same permissions to import tags as required for crawling Snowflake assets. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. To crawl streams To crawl streams, provide the following permissions: To crawl current streams: Replace with the Snowflake database name. Replace with the Snowflake database name. To crawl future streams: To crawl future streams: Replace with the Snowflake database name. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. Grant permissions for information schema method This method uses views in the INFORMATION_SCHEMA schema in Snowflake databases to fetch metadata. You still need to grant specific permissions to enable Atlan to crawl metadata, preview data, and query data with this method. To crawl existing assets Grant these permissions to crawl assets that already exist in Snowflake. If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Grant permissions to crawl existing assets: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) Grant permissions to crawl functions: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) For secure user-defined functions (UDFs), grant OWNERSHIP permissions to retrieve metadata: Replace the placeholders with the appropriate values: : The name of the schema that contains the user-defined function (UDF). : The name of the secure UDF that requires ownership permissions. : The role that gets assigned ownership of the secure UDF. The statements given on this page apply to all schemas, tables, and views in a database in Snowflake. If you want to limit access to only certain objects, you can instead specify the exact objects individually as well. To crawl future assets To crawl assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. To grant permissions at a database level: Replace with the database you want to crawl in Atlan. (Repeat the statements for every database you want to integrate into Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) To mine query history for lineage To also mine Snowflake's query history (for lineage), add these permissions. You can use either option: To mine query history direct from Snowflake's internal tables: To mine query history from a cloned or copied set of tables, where you can also remove any sensitive data: Replace with the name of the cloned database, and with the name of the cloned schema containing account usage details. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to be able to preview and query in Atlan. (Repeat the statements for every database and schema you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. (Optional) To import Snowflake tags Snowflake stores all tag objects in the ACCOUNT_USAGE schema. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To import tags from Snowflake , grant these permissions: To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. (Optional) To push updated tags to Snowflake To push tags updated for assets in Atlan to Snowflake , grant these permissions: You can learn more about tag privileges from Snowflake documentation . (Optional) To crawl dynamic tables Atlan currently supports fetching metadata for dynamic tables using the MONITOR privilege. Refer to Snowflake documentation to learn more. To crawl existing dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: To crawl future dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) (Optional) To crawl Iceberg tables Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method. To crawl Iceberg tables from Snowflake, grant the following permissions: To crawl existing Iceberg tables in Snowflake: To crawl future Iceberg tables in Snowflake: To crawl Iceberg catalog metadata for Iceberg tables in Snowflake: You must first grant permissions to crawl existing Iceberg tables for this permission to work on catalogs. You must also grant permissions to all the catalogs you want to crawl in Atlan individually. (Optional) To crawl Snowflake stages Atlan supports crawling metadata for Snowflake stages using the USAGE and READ privileges. For more information, see the Snowflake documentation for INFORMATION_SCHEMA.STAGES. To crawl stages from Snowflake: Grant USAGE and READ privileges on all existing stages at the database level: Replace with the name of your Snowflake database Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Replace with the role you've granted Atlan to use for crawling. Grant USAGE and READ privileges on all future stages at the database level: Grant USAGE and READ privileges on all future stages at the database level: Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Allowlist the Atlan IP If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support ticket from within Atlan, or submit a request . (If you aren't using the IP allowlist in your Snowflake instance, you can skip this step.) Create user and role in Snowflake Choose metadata fetching method Grant permissions for account usage method Grant permissions for information schema method Allowlist the Atlan IP"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/snowflake\/how-tos\/set-up-snowflake#with-a-password-in-snowflake","title":"Set up Snowflake | Atlan Documentation","text":"You need your Snowflake administrator to run these commands - you may not have access yourself. Create user and role in Snowflake Create a role and user in Snowflake using the following commands: Create role Create a role in Snowflake using the following commands: Replace with the default warehouse to use when running the Snowflake crawler . Atlan requires the following privileges to: OPERATE enables Atlan to start the virtual warehouse to fetch metadata if the warehouse has stopped. USAGE enables Atlan to show or list metadata from Snowflake. This in turn enables the Snowflake crawler to run the SHOW query. Create a user Create a separate user to integrate into Atlan, using one of the following 3 options: See Snowflake's official guide for details on generating an RSA key-pair . To create a user with a key-pair, replace the value for rsa_public_key with the public key and run the following: Learn more about the SERVICE type property in Snowflake documentation . Atlan only supports encrypted private keys with a non-empty passphrase - generally recommended as more secure. An empty passphrase results in workflow failures. To generate an encrypted private key, omit the -nocrypt option. Refer to Snowflake documentation to learn more. Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. To create a user with a password, replace and run the following: Learn more about the LEGACY_SERVICE type property in Snowflake documentation . This method is currently only available if Okta is your IdP (Snowflake supports) authenticating natively through Okta : Create a user in your identity provider (IdP) and use federated authentication in Snowflake . The password for this user must be maintained solely in the IdP and multi-factor authentication (MFA) must be disabled. Grant role to user To grant the atlan_user_role to the new user: Configure OAuth (client credentials flow) with Microsoft Entra ID To configure OAuth authentication using Microsoft Entra ID (formerly Azure AD) with the client credentials flow: Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions In Snowflake, create a security integration using the following: CREATE SECURITY INTEGRATION external_oauth_azure_ad TYPE = external_oauth ENABLED = true EXTERNAL_OAUTH_TYPE = azure EXTERNAL_OAUTH_ISSUER = '\\ ' EXTERNAL_OAUTH_JWS_KEYS_URL = '\\ ' EXTERNAL_OAUTH_AUDIENCE_LIST = ( '\\ ' ) EXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM = 'sub' EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE = 'login_name' ; Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app In Snowflake, create a security integration using the following: Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: CREATE USER oauth_svc_user WITH LOGIN_NAME = '\\ ' -- Use Azure client OBJECT ID DEFAULT_ROLE = \\ DEFAULT_WAREHOUSE = \\ ; Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: Grant the configured role to this user: GRANT ROLE \\ TO USER oauth_svc_user ; Grant the configured role to this user: Choose metadata fetching method Atlan supports two methods for fetching metadata from Snowflake - account usage and information schema. You should choose one of these two methods to set up Snowflake: Grant permissions for account usage method If you want to set warehouse timeouts when using this method, set a large value initially for the workflow to succeed. Once the workflow has succeeded, adjust the value to be twice the extraction time. This method uses the views in SNOWFLAKE.ACCOUNT_USAGE (or a copied version of this schema) to fetch the metadata from Snowflake into Atlan. You can be more granular with permissions using this method, but there are limitations with this approach . To crawl assets, generate lineage, and import tags If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Snowflake stores all tag objects in the ACCOUNT_USAGE schema. If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner , you need to grant the same permissions to import tags as required for crawling Snowflake assets. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. To crawl streams To crawl streams, provide the following permissions: To crawl current streams: Replace with the Snowflake database name. Replace with the Snowflake database name. To crawl future streams: To crawl future streams: Replace with the Snowflake database name. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. Grant permissions for information schema method This method uses views in the INFORMATION_SCHEMA schema in Snowflake databases to fetch metadata. You still need to grant specific permissions to enable Atlan to crawl metadata, preview data, and query data with this method. To crawl existing assets Grant these permissions to crawl assets that already exist in Snowflake. If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Grant permissions to crawl existing assets: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) Grant permissions to crawl functions: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) For secure user-defined functions (UDFs), grant OWNERSHIP permissions to retrieve metadata: Replace the placeholders with the appropriate values: : The name of the schema that contains the user-defined function (UDF). : The name of the secure UDF that requires ownership permissions. : The role that gets assigned ownership of the secure UDF. The statements given on this page apply to all schemas, tables, and views in a database in Snowflake. If you want to limit access to only certain objects, you can instead specify the exact objects individually as well. To crawl future assets To crawl assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. To grant permissions at a database level: Replace with the database you want to crawl in Atlan. (Repeat the statements for every database you want to integrate into Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) To mine query history for lineage To also mine Snowflake's query history (for lineage), add these permissions. You can use either option: To mine query history direct from Snowflake's internal tables: To mine query history from a cloned or copied set of tables, where you can also remove any sensitive data: Replace with the name of the cloned database, and with the name of the cloned schema containing account usage details. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to be able to preview and query in Atlan. (Repeat the statements for every database and schema you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. (Optional) To import Snowflake tags Snowflake stores all tag objects in the ACCOUNT_USAGE schema. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To import tags from Snowflake , grant these permissions: To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. (Optional) To push updated tags to Snowflake To push tags updated for assets in Atlan to Snowflake , grant these permissions: You can learn more about tag privileges from Snowflake documentation . (Optional) To crawl dynamic tables Atlan currently supports fetching metadata for dynamic tables using the MONITOR privilege. Refer to Snowflake documentation to learn more. To crawl existing dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: To crawl future dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) (Optional) To crawl Iceberg tables Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method. To crawl Iceberg tables from Snowflake, grant the following permissions: To crawl existing Iceberg tables in Snowflake: To crawl future Iceberg tables in Snowflake: To crawl Iceberg catalog metadata for Iceberg tables in Snowflake: You must first grant permissions to crawl existing Iceberg tables for this permission to work on catalogs. You must also grant permissions to all the catalogs you want to crawl in Atlan individually. (Optional) To crawl Snowflake stages Atlan supports crawling metadata for Snowflake stages using the USAGE and READ privileges. For more information, see the Snowflake documentation for INFORMATION_SCHEMA.STAGES. To crawl stages from Snowflake: Grant USAGE and READ privileges on all existing stages at the database level: Replace with the name of your Snowflake database Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Replace with the role you've granted Atlan to use for crawling. Grant USAGE and READ privileges on all future stages at the database level: Grant USAGE and READ privileges on all future stages at the database level: Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Allowlist the Atlan IP If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support ticket from within Atlan, or submit a request . (If you aren't using the IP allowlist in your Snowflake instance, you can skip this step.) Create user and role in Snowflake Choose metadata fetching method Grant permissions for account usage method Grant permissions for information schema method Allowlist the Atlan IP"}
{"url":"https:\/\/docs.atlan.com\/product\/connections\/how-tos\/manage-connectivity","title":"Manage connectivity | Atlan Documentation","text":"Once you've scheduled or run a workflow you can modify its configuration at any time. The configuration that can be modified may vary by workflow but the general steps remain consistent. Modify connectivity To modify the configuration of an existing workflow, complete the following steps. On the left of any screen, navigate to Workflow . Under Monitor select an existing workflow tile. (You may need to expand the run history or filter first.) From the Workflow Run History table, click on the previous run of the workflow you want to modify. In the upper left of the screen, change to the Config tab. Modify the parts of the workflow configuration you require: Under Credential , use the Edit Credentials button to change the credentials for the source. danger If you're updating the connection credentials, you may also need to update the metadata filters before running the updated workflow. Atlan currently does not detect changes to your connection settings and update the metadata filters automatically. Under Connection settings , use the Edit button to change the connection details: Modify whether or not querying or data previews are allowed for the source. Modify the query row limit to enable exporting large query results via email . Modify the query timeout limit - expandable up to 60 minutes. Under Connection Admins , click the pencil icon to add or remove connection admins. danger If you do not specify any user or group, nobody will be able to manage the connection - not even admins. Under Metadata , use the selectors to modify which metadata to include and exclude. To check for any permissions or other configuration issues before running the workflow, click Preflight checks . Under Credential , use the Edit Credentials button to change the credentials for the source. danger If you're updating the connection credentials, you may also need to update the metadata filters before running the updated workflow. Atlan currently does not detect changes to your connection settings and update the metadata filters automatically. Under Credential , use the Edit Credentials button to change the credentials for the source. If you're updating the connection credentials, you may also need to update the metadata filters before running the updated workflow. Atlan currently does not detect changes to your connection settings and update the metadata filters automatically. Under Connection settings , use the Edit button to change the connection details: Modify whether or not querying or data previews are allowed for the source. Modify the query row limit to enable exporting large query results via email . Modify the query timeout limit - expandable up to 60 minutes. Under Connection settings , use the Edit button to change the connection details: Modify whether or not querying or data previews are allowed for the source. Modify the query row limit to enable exporting large query results via email . Modify the query timeout limit - expandable up to 60 minutes. Under Connection Admins , click the pencil icon to add or remove connection admins. danger If you do not specify any user or group, nobody will be able to manage the connection - not even admins. Under Connection Admins , click the pencil icon to add or remove connection admins. If you do not specify any user or group, nobody will be able to manage the connection - not even admins. Under Metadata , use the selectors to modify which metadata to include and exclude. Under Metadata , use the selectors to modify which metadata to include and exclude. To check for any permissions or other configuration issues before running the workflow, click Preflight checks . To check for any permissions or other configuration issues before running the workflow, click Preflight checks . Once you've made your updates, click the Update button to save the changes. You can optionally run the workflow with the new configuration immediately. You will need to confirm your changes by clicking the Yes button. Note that some workflow changes may take a few minutes to come into effect. You can optionally run the workflow with the new configuration immediately. You will need to confirm your changes by clicking the Yes button. Note that some workflow changes may take a few minutes to come into effect. That's it - next time you run the workflow, or it runs on its schedule, it will use your changes! Ã° If you modify the Metadata portion, any previously crawled metadata that is now excluded will be archived on the next workflow run."}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/snowflake\/how-tos\/set-up-snowflake#managed-through-your-identity-provider-idp-","title":"Set up Snowflake | Atlan Documentation","text":"You need your Snowflake administrator to run these commands - you may not have access yourself. Create user and role in Snowflake Create a role and user in Snowflake using the following commands: Create role Create a role in Snowflake using the following commands: Replace with the default warehouse to use when running the Snowflake crawler . Atlan requires the following privileges to: OPERATE enables Atlan to start the virtual warehouse to fetch metadata if the warehouse has stopped. USAGE enables Atlan to show or list metadata from Snowflake. This in turn enables the Snowflake crawler to run the SHOW query. Create a user Create a separate user to integrate into Atlan, using one of the following 3 options: See Snowflake's official guide for details on generating an RSA key-pair . To create a user with a key-pair, replace the value for rsa_public_key with the public key and run the following: Learn more about the SERVICE type property in Snowflake documentation . Atlan only supports encrypted private keys with a non-empty passphrase - generally recommended as more secure. An empty passphrase results in workflow failures. To generate an encrypted private key, omit the -nocrypt option. Refer to Snowflake documentation to learn more. Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. To create a user with a password, replace and run the following: Learn more about the LEGACY_SERVICE type property in Snowflake documentation . This method is currently only available if Okta is your IdP (Snowflake supports) authenticating natively through Okta : Create a user in your identity provider (IdP) and use federated authentication in Snowflake . The password for this user must be maintained solely in the IdP and multi-factor authentication (MFA) must be disabled. Grant role to user To grant the atlan_user_role to the new user: Configure OAuth (client credentials flow) with Microsoft Entra ID To configure OAuth authentication using Microsoft Entra ID (formerly Azure AD) with the client credentials flow: Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions In Snowflake, create a security integration using the following: CREATE SECURITY INTEGRATION external_oauth_azure_ad TYPE = external_oauth ENABLED = true EXTERNAL_OAUTH_TYPE = azure EXTERNAL_OAUTH_ISSUER = '\\ ' EXTERNAL_OAUTH_JWS_KEYS_URL = '\\ ' EXTERNAL_OAUTH_AUDIENCE_LIST = ( '\\ ' ) EXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM = 'sub' EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE = 'login_name' ; Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app In Snowflake, create a security integration using the following: Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: CREATE USER oauth_svc_user WITH LOGIN_NAME = '\\ ' -- Use Azure client OBJECT ID DEFAULT_ROLE = \\ DEFAULT_WAREHOUSE = \\ ; Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: Grant the configured role to this user: GRANT ROLE \\ TO USER oauth_svc_user ; Grant the configured role to this user: Choose metadata fetching method Atlan supports two methods for fetching metadata from Snowflake - account usage and information schema. You should choose one of these two methods to set up Snowflake: Grant permissions for account usage method If you want to set warehouse timeouts when using this method, set a large value initially for the workflow to succeed. Once the workflow has succeeded, adjust the value to be twice the extraction time. This method uses the views in SNOWFLAKE.ACCOUNT_USAGE (or a copied version of this schema) to fetch the metadata from Snowflake into Atlan. You can be more granular with permissions using this method, but there are limitations with this approach . To crawl assets, generate lineage, and import tags If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Snowflake stores all tag objects in the ACCOUNT_USAGE schema. If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner , you need to grant the same permissions to import tags as required for crawling Snowflake assets. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. To crawl streams To crawl streams, provide the following permissions: To crawl current streams: Replace with the Snowflake database name. Replace with the Snowflake database name. To crawl future streams: To crawl future streams: Replace with the Snowflake database name. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. Grant permissions for information schema method This method uses views in the INFORMATION_SCHEMA schema in Snowflake databases to fetch metadata. You still need to grant specific permissions to enable Atlan to crawl metadata, preview data, and query data with this method. To crawl existing assets Grant these permissions to crawl assets that already exist in Snowflake. If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Grant permissions to crawl existing assets: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) Grant permissions to crawl functions: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) For secure user-defined functions (UDFs), grant OWNERSHIP permissions to retrieve metadata: Replace the placeholders with the appropriate values: : The name of the schema that contains the user-defined function (UDF). : The name of the secure UDF that requires ownership permissions. : The role that gets assigned ownership of the secure UDF. The statements given on this page apply to all schemas, tables, and views in a database in Snowflake. If you want to limit access to only certain objects, you can instead specify the exact objects individually as well. To crawl future assets To crawl assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. To grant permissions at a database level: Replace with the database you want to crawl in Atlan. (Repeat the statements for every database you want to integrate into Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) To mine query history for lineage To also mine Snowflake's query history (for lineage), add these permissions. You can use either option: To mine query history direct from Snowflake's internal tables: To mine query history from a cloned or copied set of tables, where you can also remove any sensitive data: Replace with the name of the cloned database, and with the name of the cloned schema containing account usage details. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to be able to preview and query in Atlan. (Repeat the statements for every database and schema you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. (Optional) To import Snowflake tags Snowflake stores all tag objects in the ACCOUNT_USAGE schema. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To import tags from Snowflake , grant these permissions: To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. (Optional) To push updated tags to Snowflake To push tags updated for assets in Atlan to Snowflake , grant these permissions: You can learn more about tag privileges from Snowflake documentation . (Optional) To crawl dynamic tables Atlan currently supports fetching metadata for dynamic tables using the MONITOR privilege. Refer to Snowflake documentation to learn more. To crawl existing dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: To crawl future dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) (Optional) To crawl Iceberg tables Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method. To crawl Iceberg tables from Snowflake, grant the following permissions: To crawl existing Iceberg tables in Snowflake: To crawl future Iceberg tables in Snowflake: To crawl Iceberg catalog metadata for Iceberg tables in Snowflake: You must first grant permissions to crawl existing Iceberg tables for this permission to work on catalogs. You must also grant permissions to all the catalogs you want to crawl in Atlan individually. (Optional) To crawl Snowflake stages Atlan supports crawling metadata for Snowflake stages using the USAGE and READ privileges. For more information, see the Snowflake documentation for INFORMATION_SCHEMA.STAGES. To crawl stages from Snowflake: Grant USAGE and READ privileges on all existing stages at the database level: Replace with the name of your Snowflake database Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Replace with the role you've granted Atlan to use for crawling. Grant USAGE and READ privileges on all future stages at the database level: Grant USAGE and READ privileges on all future stages at the database level: Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Allowlist the Atlan IP If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support ticket from within Atlan, or submit a request . (If you aren't using the IP allowlist in your Snowflake instance, you can skip this step.) Create user and role in Snowflake Choose metadata fetching method Grant permissions for account usage method Grant permissions for information schema method Allowlist the Atlan IP"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/snowflake\/how-tos\/set-up-snowflake#grant-role-to-user","title":"Set up Snowflake | Atlan Documentation","text":"You need your Snowflake administrator to run these commands - you may not have access yourself. Create user and role in Snowflake Create a role and user in Snowflake using the following commands: Create role Create a role in Snowflake using the following commands: Replace with the default warehouse to use when running the Snowflake crawler . Atlan requires the following privileges to: OPERATE enables Atlan to start the virtual warehouse to fetch metadata if the warehouse has stopped. USAGE enables Atlan to show or list metadata from Snowflake. This in turn enables the Snowflake crawler to run the SHOW query. Create a user Create a separate user to integrate into Atlan, using one of the following 3 options: See Snowflake's official guide for details on generating an RSA key-pair . To create a user with a key-pair, replace the value for rsa_public_key with the public key and run the following: Learn more about the SERVICE type property in Snowflake documentation . Atlan only supports encrypted private keys with a non-empty passphrase - generally recommended as more secure. An empty passphrase results in workflow failures. To generate an encrypted private key, omit the -nocrypt option. Refer to Snowflake documentation to learn more. Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. To create a user with a password, replace and run the following: Learn more about the LEGACY_SERVICE type property in Snowflake documentation . This method is currently only available if Okta is your IdP (Snowflake supports) authenticating natively through Okta : Create a user in your identity provider (IdP) and use federated authentication in Snowflake . The password for this user must be maintained solely in the IdP and multi-factor authentication (MFA) must be disabled. Grant role to user To grant the atlan_user_role to the new user: Configure OAuth (client credentials flow) with Microsoft Entra ID To configure OAuth authentication using Microsoft Entra ID (formerly Azure AD) with the client credentials flow: Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions In Snowflake, create a security integration using the following: CREATE SECURITY INTEGRATION external_oauth_azure_ad TYPE = external_oauth ENABLED = true EXTERNAL_OAUTH_TYPE = azure EXTERNAL_OAUTH_ISSUER = '\\ ' EXTERNAL_OAUTH_JWS_KEYS_URL = '\\ ' EXTERNAL_OAUTH_AUDIENCE_LIST = ( '\\ ' ) EXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM = 'sub' EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE = 'login_name' ; Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app In Snowflake, create a security integration using the following: Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: CREATE USER oauth_svc_user WITH LOGIN_NAME = '\\ ' -- Use Azure client OBJECT ID DEFAULT_ROLE = \\ DEFAULT_WAREHOUSE = \\ ; Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: Grant the configured role to this user: GRANT ROLE \\ TO USER oauth_svc_user ; Grant the configured role to this user: Choose metadata fetching method Atlan supports two methods for fetching metadata from Snowflake - account usage and information schema. You should choose one of these two methods to set up Snowflake: Grant permissions for account usage method If you want to set warehouse timeouts when using this method, set a large value initially for the workflow to succeed. Once the workflow has succeeded, adjust the value to be twice the extraction time. This method uses the views in SNOWFLAKE.ACCOUNT_USAGE (or a copied version of this schema) to fetch the metadata from Snowflake into Atlan. You can be more granular with permissions using this method, but there are limitations with this approach . To crawl assets, generate lineage, and import tags If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Snowflake stores all tag objects in the ACCOUNT_USAGE schema. If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner , you need to grant the same permissions to import tags as required for crawling Snowflake assets. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. To crawl streams To crawl streams, provide the following permissions: To crawl current streams: Replace with the Snowflake database name. Replace with the Snowflake database name. To crawl future streams: To crawl future streams: Replace with the Snowflake database name. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. Grant permissions for information schema method This method uses views in the INFORMATION_SCHEMA schema in Snowflake databases to fetch metadata. You still need to grant specific permissions to enable Atlan to crawl metadata, preview data, and query data with this method. To crawl existing assets Grant these permissions to crawl assets that already exist in Snowflake. If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Grant permissions to crawl existing assets: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) Grant permissions to crawl functions: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) For secure user-defined functions (UDFs), grant OWNERSHIP permissions to retrieve metadata: Replace the placeholders with the appropriate values: : The name of the schema that contains the user-defined function (UDF). : The name of the secure UDF that requires ownership permissions. : The role that gets assigned ownership of the secure UDF. The statements given on this page apply to all schemas, tables, and views in a database in Snowflake. If you want to limit access to only certain objects, you can instead specify the exact objects individually as well. To crawl future assets To crawl assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. To grant permissions at a database level: Replace with the database you want to crawl in Atlan. (Repeat the statements for every database you want to integrate into Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) To mine query history for lineage To also mine Snowflake's query history (for lineage), add these permissions. You can use either option: To mine query history direct from Snowflake's internal tables: To mine query history from a cloned or copied set of tables, where you can also remove any sensitive data: Replace with the name of the cloned database, and with the name of the cloned schema containing account usage details. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to be able to preview and query in Atlan. (Repeat the statements for every database and schema you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. (Optional) To import Snowflake tags Snowflake stores all tag objects in the ACCOUNT_USAGE schema. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To import tags from Snowflake , grant these permissions: To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. (Optional) To push updated tags to Snowflake To push tags updated for assets in Atlan to Snowflake , grant these permissions: You can learn more about tag privileges from Snowflake documentation . (Optional) To crawl dynamic tables Atlan currently supports fetching metadata for dynamic tables using the MONITOR privilege. Refer to Snowflake documentation to learn more. To crawl existing dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: To crawl future dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) (Optional) To crawl Iceberg tables Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method. To crawl Iceberg tables from Snowflake, grant the following permissions: To crawl existing Iceberg tables in Snowflake: To crawl future Iceberg tables in Snowflake: To crawl Iceberg catalog metadata for Iceberg tables in Snowflake: You must first grant permissions to crawl existing Iceberg tables for this permission to work on catalogs. You must also grant permissions to all the catalogs you want to crawl in Atlan individually. (Optional) To crawl Snowflake stages Atlan supports crawling metadata for Snowflake stages using the USAGE and READ privileges. For more information, see the Snowflake documentation for INFORMATION_SCHEMA.STAGES. To crawl stages from Snowflake: Grant USAGE and READ privileges on all existing stages at the database level: Replace with the name of your Snowflake database Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Replace with the role you've granted Atlan to use for crawling. Grant USAGE and READ privileges on all future stages at the database level: Grant USAGE and READ privileges on all future stages at the database level: Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Allowlist the Atlan IP If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support ticket from within Atlan, or submit a request . (If you aren't using the IP allowlist in your Snowflake instance, you can skip this step.) Create user and role in Snowflake Choose metadata fetching method Grant permissions for account usage method Grant permissions for information schema method Allowlist the Atlan IP"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/snowflake\/how-tos\/set-up-snowflake#configure-oauth-client-credentials-flow-with-microsoft-entra-id","title":"Set up Snowflake | Atlan Documentation","text":"You need your Snowflake administrator to run these commands - you may not have access yourself. Create user and role in Snowflake Create a role and user in Snowflake using the following commands: Create role Create a role in Snowflake using the following commands: Replace with the default warehouse to use when running the Snowflake crawler . Atlan requires the following privileges to: OPERATE enables Atlan to start the virtual warehouse to fetch metadata if the warehouse has stopped. USAGE enables Atlan to show or list metadata from Snowflake. This in turn enables the Snowflake crawler to run the SHOW query. Create a user Create a separate user to integrate into Atlan, using one of the following 3 options: See Snowflake's official guide for details on generating an RSA key-pair . To create a user with a key-pair, replace the value for rsa_public_key with the public key and run the following: Learn more about the SERVICE type property in Snowflake documentation . Atlan only supports encrypted private keys with a non-empty passphrase - generally recommended as more secure. An empty passphrase results in workflow failures. To generate an encrypted private key, omit the -nocrypt option. Refer to Snowflake documentation to learn more. Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. To create a user with a password, replace and run the following: Learn more about the LEGACY_SERVICE type property in Snowflake documentation . This method is currently only available if Okta is your IdP (Snowflake supports) authenticating natively through Okta : Create a user in your identity provider (IdP) and use federated authentication in Snowflake . The password for this user must be maintained solely in the IdP and multi-factor authentication (MFA) must be disabled. Grant role to user To grant the atlan_user_role to the new user: Configure OAuth (client credentials flow) with Microsoft Entra ID To configure OAuth authentication using Microsoft Entra ID (formerly Azure AD) with the client credentials flow: Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions In Snowflake, create a security integration using the following: CREATE SECURITY INTEGRATION external_oauth_azure_ad TYPE = external_oauth ENABLED = true EXTERNAL_OAUTH_TYPE = azure EXTERNAL_OAUTH_ISSUER = '\\ ' EXTERNAL_OAUTH_JWS_KEYS_URL = '\\ ' EXTERNAL_OAUTH_AUDIENCE_LIST = ( '\\ ' ) EXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM = 'sub' EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE = 'login_name' ; Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app In Snowflake, create a security integration using the following: Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: CREATE USER oauth_svc_user WITH LOGIN_NAME = '\\ ' -- Use Azure client OBJECT ID DEFAULT_ROLE = \\ DEFAULT_WAREHOUSE = \\ ; Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: Grant the configured role to this user: GRANT ROLE \\ TO USER oauth_svc_user ; Grant the configured role to this user: Choose metadata fetching method Atlan supports two methods for fetching metadata from Snowflake - account usage and information schema. You should choose one of these two methods to set up Snowflake: Grant permissions for account usage method If you want to set warehouse timeouts when using this method, set a large value initially for the workflow to succeed. Once the workflow has succeeded, adjust the value to be twice the extraction time. This method uses the views in SNOWFLAKE.ACCOUNT_USAGE (or a copied version of this schema) to fetch the metadata from Snowflake into Atlan. You can be more granular with permissions using this method, but there are limitations with this approach . To crawl assets, generate lineage, and import tags If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Snowflake stores all tag objects in the ACCOUNT_USAGE schema. If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner , you need to grant the same permissions to import tags as required for crawling Snowflake assets. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. To crawl streams To crawl streams, provide the following permissions: To crawl current streams: Replace with the Snowflake database name. Replace with the Snowflake database name. To crawl future streams: To crawl future streams: Replace with the Snowflake database name. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. Grant permissions for information schema method This method uses views in the INFORMATION_SCHEMA schema in Snowflake databases to fetch metadata. You still need to grant specific permissions to enable Atlan to crawl metadata, preview data, and query data with this method. To crawl existing assets Grant these permissions to crawl assets that already exist in Snowflake. If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Grant permissions to crawl existing assets: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) Grant permissions to crawl functions: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) For secure user-defined functions (UDFs), grant OWNERSHIP permissions to retrieve metadata: Replace the placeholders with the appropriate values: : The name of the schema that contains the user-defined function (UDF). : The name of the secure UDF that requires ownership permissions. : The role that gets assigned ownership of the secure UDF. The statements given on this page apply to all schemas, tables, and views in a database in Snowflake. If you want to limit access to only certain objects, you can instead specify the exact objects individually as well. To crawl future assets To crawl assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. To grant permissions at a database level: Replace with the database you want to crawl in Atlan. (Repeat the statements for every database you want to integrate into Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) To mine query history for lineage To also mine Snowflake's query history (for lineage), add these permissions. You can use either option: To mine query history direct from Snowflake's internal tables: To mine query history from a cloned or copied set of tables, where you can also remove any sensitive data: Replace with the name of the cloned database, and with the name of the cloned schema containing account usage details. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to be able to preview and query in Atlan. (Repeat the statements for every database and schema you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. (Optional) To import Snowflake tags Snowflake stores all tag objects in the ACCOUNT_USAGE schema. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To import tags from Snowflake , grant these permissions: To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. (Optional) To push updated tags to Snowflake To push tags updated for assets in Atlan to Snowflake , grant these permissions: You can learn more about tag privileges from Snowflake documentation . (Optional) To crawl dynamic tables Atlan currently supports fetching metadata for dynamic tables using the MONITOR privilege. Refer to Snowflake documentation to learn more. To crawl existing dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: To crawl future dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) (Optional) To crawl Iceberg tables Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method. To crawl Iceberg tables from Snowflake, grant the following permissions: To crawl existing Iceberg tables in Snowflake: To crawl future Iceberg tables in Snowflake: To crawl Iceberg catalog metadata for Iceberg tables in Snowflake: You must first grant permissions to crawl existing Iceberg tables for this permission to work on catalogs. You must also grant permissions to all the catalogs you want to crawl in Atlan individually. (Optional) To crawl Snowflake stages Atlan supports crawling metadata for Snowflake stages using the USAGE and READ privileges. For more information, see the Snowflake documentation for INFORMATION_SCHEMA.STAGES. To crawl stages from Snowflake: Grant USAGE and READ privileges on all existing stages at the database level: Replace with the name of your Snowflake database Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Replace with the role you've granted Atlan to use for crawling. Grant USAGE and READ privileges on all future stages at the database level: Grant USAGE and READ privileges on all future stages at the database level: Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Allowlist the Atlan IP If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support ticket from within Atlan, or submit a request . (If you aren't using the IP allowlist in your Snowflake instance, you can skip this step.) Create user and role in Snowflake Choose metadata fetching method Grant permissions for account usage method Grant permissions for information schema method Allowlist the Atlan IP"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/snowflake\/how-tos\/set-up-snowflake#choose-metadata-fetching-method","title":"Set up Snowflake | Atlan Documentation","text":"You need your Snowflake administrator to run these commands - you may not have access yourself. Create user and role in Snowflake Create a role and user in Snowflake using the following commands: Create role Create a role in Snowflake using the following commands: Replace with the default warehouse to use when running the Snowflake crawler . Atlan requires the following privileges to: OPERATE enables Atlan to start the virtual warehouse to fetch metadata if the warehouse has stopped. USAGE enables Atlan to show or list metadata from Snowflake. This in turn enables the Snowflake crawler to run the SHOW query. Create a user Create a separate user to integrate into Atlan, using one of the following 3 options: See Snowflake's official guide for details on generating an RSA key-pair . To create a user with a key-pair, replace the value for rsa_public_key with the public key and run the following: Learn more about the SERVICE type property in Snowflake documentation . Atlan only supports encrypted private keys with a non-empty passphrase - generally recommended as more secure. An empty passphrase results in workflow failures. To generate an encrypted private key, omit the -nocrypt option. Refer to Snowflake documentation to learn more. Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. To create a user with a password, replace and run the following: Learn more about the LEGACY_SERVICE type property in Snowflake documentation . This method is currently only available if Okta is your IdP (Snowflake supports) authenticating natively through Okta : Create a user in your identity provider (IdP) and use federated authentication in Snowflake . The password for this user must be maintained solely in the IdP and multi-factor authentication (MFA) must be disabled. Grant role to user To grant the atlan_user_role to the new user: Configure OAuth (client credentials flow) with Microsoft Entra ID To configure OAuth authentication using Microsoft Entra ID (formerly Azure AD) with the client credentials flow: Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions In Snowflake, create a security integration using the following: CREATE SECURITY INTEGRATION external_oauth_azure_ad TYPE = external_oauth ENABLED = true EXTERNAL_OAUTH_TYPE = azure EXTERNAL_OAUTH_ISSUER = '\\ ' EXTERNAL_OAUTH_JWS_KEYS_URL = '\\ ' EXTERNAL_OAUTH_AUDIENCE_LIST = ( '\\ ' ) EXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM = 'sub' EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE = 'login_name' ; Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app In Snowflake, create a security integration using the following: Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: CREATE USER oauth_svc_user WITH LOGIN_NAME = '\\ ' -- Use Azure client OBJECT ID DEFAULT_ROLE = \\ DEFAULT_WAREHOUSE = \\ ; Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: Grant the configured role to this user: GRANT ROLE \\ TO USER oauth_svc_user ; Grant the configured role to this user: Choose metadata fetching method Atlan supports two methods for fetching metadata from Snowflake - account usage and information schema. You should choose one of these two methods to set up Snowflake: Grant permissions for account usage method If you want to set warehouse timeouts when using this method, set a large value initially for the workflow to succeed. Once the workflow has succeeded, adjust the value to be twice the extraction time. This method uses the views in SNOWFLAKE.ACCOUNT_USAGE (or a copied version of this schema) to fetch the metadata from Snowflake into Atlan. You can be more granular with permissions using this method, but there are limitations with this approach . To crawl assets, generate lineage, and import tags If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Snowflake stores all tag objects in the ACCOUNT_USAGE schema. If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner , you need to grant the same permissions to import tags as required for crawling Snowflake assets. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. To crawl streams To crawl streams, provide the following permissions: To crawl current streams: Replace with the Snowflake database name. Replace with the Snowflake database name. To crawl future streams: To crawl future streams: Replace with the Snowflake database name. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. Grant permissions for information schema method This method uses views in the INFORMATION_SCHEMA schema in Snowflake databases to fetch metadata. You still need to grant specific permissions to enable Atlan to crawl metadata, preview data, and query data with this method. To crawl existing assets Grant these permissions to crawl assets that already exist in Snowflake. If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Grant permissions to crawl existing assets: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) Grant permissions to crawl functions: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) For secure user-defined functions (UDFs), grant OWNERSHIP permissions to retrieve metadata: Replace the placeholders with the appropriate values: : The name of the schema that contains the user-defined function (UDF). : The name of the secure UDF that requires ownership permissions. : The role that gets assigned ownership of the secure UDF. The statements given on this page apply to all schemas, tables, and views in a database in Snowflake. If you want to limit access to only certain objects, you can instead specify the exact objects individually as well. To crawl future assets To crawl assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. To grant permissions at a database level: Replace with the database you want to crawl in Atlan. (Repeat the statements for every database you want to integrate into Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) To mine query history for lineage To also mine Snowflake's query history (for lineage), add these permissions. You can use either option: To mine query history direct from Snowflake's internal tables: To mine query history from a cloned or copied set of tables, where you can also remove any sensitive data: Replace with the name of the cloned database, and with the name of the cloned schema containing account usage details. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to be able to preview and query in Atlan. (Repeat the statements for every database and schema you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. (Optional) To import Snowflake tags Snowflake stores all tag objects in the ACCOUNT_USAGE schema. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To import tags from Snowflake , grant these permissions: To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. (Optional) To push updated tags to Snowflake To push tags updated for assets in Atlan to Snowflake , grant these permissions: You can learn more about tag privileges from Snowflake documentation . (Optional) To crawl dynamic tables Atlan currently supports fetching metadata for dynamic tables using the MONITOR privilege. Refer to Snowflake documentation to learn more. To crawl existing dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: To crawl future dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) (Optional) To crawl Iceberg tables Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method. To crawl Iceberg tables from Snowflake, grant the following permissions: To crawl existing Iceberg tables in Snowflake: To crawl future Iceberg tables in Snowflake: To crawl Iceberg catalog metadata for Iceberg tables in Snowflake: You must first grant permissions to crawl existing Iceberg tables for this permission to work on catalogs. You must also grant permissions to all the catalogs you want to crawl in Atlan individually. (Optional) To crawl Snowflake stages Atlan supports crawling metadata for Snowflake stages using the USAGE and READ privileges. For more information, see the Snowflake documentation for INFORMATION_SCHEMA.STAGES. To crawl stages from Snowflake: Grant USAGE and READ privileges on all existing stages at the database level: Replace with the name of your Snowflake database Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Replace with the role you've granted Atlan to use for crawling. Grant USAGE and READ privileges on all future stages at the database level: Grant USAGE and READ privileges on all future stages at the database level: Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Allowlist the Atlan IP If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support ticket from within Atlan, or submit a request . (If you aren't using the IP allowlist in your Snowflake instance, you can skip this step.) Create user and role in Snowflake Choose metadata fetching method Grant permissions for account usage method Grant permissions for information schema method Allowlist the Atlan IP"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/snowflake\/how-tos\/set-up-snowflake#grant-permissions-for-account-usage-method","title":"Set up Snowflake | Atlan Documentation","text":"You need your Snowflake administrator to run these commands - you may not have access yourself. Create user and role in Snowflake Create a role and user in Snowflake using the following commands: Create role Create a role in Snowflake using the following commands: Replace with the default warehouse to use when running the Snowflake crawler . Atlan requires the following privileges to: OPERATE enables Atlan to start the virtual warehouse to fetch metadata if the warehouse has stopped. USAGE enables Atlan to show or list metadata from Snowflake. This in turn enables the Snowflake crawler to run the SHOW query. Create a user Create a separate user to integrate into Atlan, using one of the following 3 options: See Snowflake's official guide for details on generating an RSA key-pair . To create a user with a key-pair, replace the value for rsa_public_key with the public key and run the following: Learn more about the SERVICE type property in Snowflake documentation . Atlan only supports encrypted private keys with a non-empty passphrase - generally recommended as more secure. An empty passphrase results in workflow failures. To generate an encrypted private key, omit the -nocrypt option. Refer to Snowflake documentation to learn more. Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. To create a user with a password, replace and run the following: Learn more about the LEGACY_SERVICE type property in Snowflake documentation . This method is currently only available if Okta is your IdP (Snowflake supports) authenticating natively through Okta : Create a user in your identity provider (IdP) and use federated authentication in Snowflake . The password for this user must be maintained solely in the IdP and multi-factor authentication (MFA) must be disabled. Grant role to user To grant the atlan_user_role to the new user: Configure OAuth (client credentials flow) with Microsoft Entra ID To configure OAuth authentication using Microsoft Entra ID (formerly Azure AD) with the client credentials flow: Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions In Snowflake, create a security integration using the following: CREATE SECURITY INTEGRATION external_oauth_azure_ad TYPE = external_oauth ENABLED = true EXTERNAL_OAUTH_TYPE = azure EXTERNAL_OAUTH_ISSUER = '\\ ' EXTERNAL_OAUTH_JWS_KEYS_URL = '\\ ' EXTERNAL_OAUTH_AUDIENCE_LIST = ( '\\ ' ) EXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM = 'sub' EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE = 'login_name' ; Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app In Snowflake, create a security integration using the following: Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: CREATE USER oauth_svc_user WITH LOGIN_NAME = '\\ ' -- Use Azure client OBJECT ID DEFAULT_ROLE = \\ DEFAULT_WAREHOUSE = \\ ; Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: Grant the configured role to this user: GRANT ROLE \\ TO USER oauth_svc_user ; Grant the configured role to this user: Choose metadata fetching method Atlan supports two methods for fetching metadata from Snowflake - account usage and information schema. You should choose one of these two methods to set up Snowflake: Grant permissions for account usage method If you want to set warehouse timeouts when using this method, set a large value initially for the workflow to succeed. Once the workflow has succeeded, adjust the value to be twice the extraction time. This method uses the views in SNOWFLAKE.ACCOUNT_USAGE (or a copied version of this schema) to fetch the metadata from Snowflake into Atlan. You can be more granular with permissions using this method, but there are limitations with this approach . To crawl assets, generate lineage, and import tags If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Snowflake stores all tag objects in the ACCOUNT_USAGE schema. If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner , you need to grant the same permissions to import tags as required for crawling Snowflake assets. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. To crawl streams To crawl streams, provide the following permissions: To crawl current streams: Replace with the Snowflake database name. Replace with the Snowflake database name. To crawl future streams: To crawl future streams: Replace with the Snowflake database name. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. Grant permissions for information schema method This method uses views in the INFORMATION_SCHEMA schema in Snowflake databases to fetch metadata. You still need to grant specific permissions to enable Atlan to crawl metadata, preview data, and query data with this method. To crawl existing assets Grant these permissions to crawl assets that already exist in Snowflake. If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Grant permissions to crawl existing assets: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) Grant permissions to crawl functions: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) For secure user-defined functions (UDFs), grant OWNERSHIP permissions to retrieve metadata: Replace the placeholders with the appropriate values: : The name of the schema that contains the user-defined function (UDF). : The name of the secure UDF that requires ownership permissions. : The role that gets assigned ownership of the secure UDF. The statements given on this page apply to all schemas, tables, and views in a database in Snowflake. If you want to limit access to only certain objects, you can instead specify the exact objects individually as well. To crawl future assets To crawl assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. To grant permissions at a database level: Replace with the database you want to crawl in Atlan. (Repeat the statements for every database you want to integrate into Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) To mine query history for lineage To also mine Snowflake's query history (for lineage), add these permissions. You can use either option: To mine query history direct from Snowflake's internal tables: To mine query history from a cloned or copied set of tables, where you can also remove any sensitive data: Replace with the name of the cloned database, and with the name of the cloned schema containing account usage details. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to be able to preview and query in Atlan. (Repeat the statements for every database and schema you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. (Optional) To import Snowflake tags Snowflake stores all tag objects in the ACCOUNT_USAGE schema. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To import tags from Snowflake , grant these permissions: To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. (Optional) To push updated tags to Snowflake To push tags updated for assets in Atlan to Snowflake , grant these permissions: You can learn more about tag privileges from Snowflake documentation . (Optional) To crawl dynamic tables Atlan currently supports fetching metadata for dynamic tables using the MONITOR privilege. Refer to Snowflake documentation to learn more. To crawl existing dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: To crawl future dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) (Optional) To crawl Iceberg tables Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method. To crawl Iceberg tables from Snowflake, grant the following permissions: To crawl existing Iceberg tables in Snowflake: To crawl future Iceberg tables in Snowflake: To crawl Iceberg catalog metadata for Iceberg tables in Snowflake: You must first grant permissions to crawl existing Iceberg tables for this permission to work on catalogs. You must also grant permissions to all the catalogs you want to crawl in Atlan individually. (Optional) To crawl Snowflake stages Atlan supports crawling metadata for Snowflake stages using the USAGE and READ privileges. For more information, see the Snowflake documentation for INFORMATION_SCHEMA.STAGES. To crawl stages from Snowflake: Grant USAGE and READ privileges on all existing stages at the database level: Replace with the name of your Snowflake database Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Replace with the role you've granted Atlan to use for crawling. Grant USAGE and READ privileges on all future stages at the database level: Grant USAGE and READ privileges on all future stages at the database level: Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Allowlist the Atlan IP If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support ticket from within Atlan, or submit a request . (If you aren't using the IP allowlist in your Snowflake instance, you can skip this step.) Create user and role in Snowflake Choose metadata fetching method Grant permissions for account usage method Grant permissions for information schema method Allowlist the Atlan IP"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/snowflake\/troubleshooting\/troubleshooting-snowflake-connectivity","title":"Troubleshooting Snowflake connectivity | Atlan Documentation","text":"Missing warehouse grants The user doesn t have USAGE and OPERATE grants on a warehouse. Grant warehouse access to the role : GRANT OPERATE , USAGE ON WAREHOUSE \" \" TO ROLE atlan_user_role ; Grant warehouse access to the role : Then, ensure that you grant the role to the new user : GRANT ROLE atlan_user_role TO USER atlan_user ; Then, ensure that you grant the role to the new user : Missing authorized access to SNOWFLAKE.ACCOUNT_USAGE schema The user doesn t have authorized access to the SNOWFLAKE.ACCOUNT_USAGE database Reach out to your account admin to grant imported privileges on the Snowflake database to the role: USE ROLE ACCOUNTADMIN ; GRANT IMPORTED PRIVILEGES ON DATABASE SNOWFLAKE TO ROLE atlan_user_role ; Reach out to your account admin to grant imported privileges on the Snowflake database to the role: If using a copied database , you'll need to grant the following permissions: GRANT USAGE ON DATABASE \" \" TO ROLE atlan_user_role ; GRANT USAGE ON SCHEMA \" \" IN DATABASE \" \" TO ROLE atlan_user_role ; GRANT REFERENCES ON ALL VIEWS IN DATABASE \" \" TO ROLE atlan_user_role ; If using a copied database , you'll need to grant the following permissions: Missing usage grants on databases and\/or schemas The user doesn't have usage grants to the databases ` $missingDatabases ` and schemas ` $missingSchemas Grant missing permissions listed here for information schema extraction method. Atlan IP not allowlisted Atlan's current location or network isn't recognized by Snowflake's security settings. This can happen if Atlan's IP address isn't on the list of allowed addresses in Snowflake's network policies. If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist . Contact Atlan support to obtain Atlan's IP addresses. The username or the password provided to connect to the Snowflake account is incorrect. Sign into the Snowflake account for the specified host and verify that the username and password are correct. You can also create a new user, if required, by following the steps here . Missing or unauthorized role The role specified in your connection configuration doesn't exist in Snowflake or your user account doesn't have grant to use this role. If the role does not exist or is missing the required grants, create a role and then grant the role to the user . User account locked The user account you're using to connect to Snowflake has been locked temporarily because of multiple incorrect login attempts. Wait for the user account to unlock or create a different user account to continue. Missing or unauthorized warehouse The warehouse specified in your connection configuration doesn't exist in Snowflake or your user account doesn't have grant to use this warehouse. Ensure that the warehouse name is configured correctly. Ensure that the warehouse name is configured correctly. Update the warehouse name in the configuration if your account is using a different warehouse. Create a role and then grant the role to the user for the updated warehouse. Update the warehouse name in the configuration if your account is using a different warehouse. Create a role and then grant the role to the user for the updated warehouse. Missing access to non-system databases or schemas The configured user doesn't have usage grants to any database or schema. or The configured user doesn't have usage grants to any non-system database or schema. This pertains to the information schema method of fetching metadata. Ensure that the user has authorized access to the databases and schemas to be crawled. Grant the requisite permissions as outlined here . Check the grants on the role attached to the user defined for the crawler. Ensure the missing database or schema is present in these grants. SHOW GRANTS TO ROLE atlan_user_role ; Check the grants on the role attached to the user defined for the crawler. Ensure the missing database or schema is present in these grants. When using incremental extraction, consider running a one-time full extraction to capture any newly introduced metadata. When using incremental extraction, consider running a one-time full extraction to capture any newly introduced metadata. Make sure the role attached to the user defined for the crawler has grants for future tables and views being created in the database: GRANT USAGE ON FUTURE SCHEMAS IN DATABASE \" \" TO ROLE atlan_user_role ; GRANT REFERENCES ON FUTURE TABLES IN DATABASE \" \" TO ROLE atlan_user_role ; GRANT REFERENCES ON FUTURE VIEWS IN DATABASE \" \" TO ROLE atlan_user_role ; GRANT REFERENCES ON FUTURE EXTERNAL TABLES IN DATABASE \" \" TO ROLE atlan_user_role ; Make sure the role attached to the user defined for the crawler has grants for future tables and views being created in the database: Make sure you run the below commands as well so that new tables and views you've created in-between are also visible to the user: GRANT USAGE ON ALL SCHEMAS IN DATABASE \" \" TO role atlan_user_role ; GRANT REFERENCES ON ALL TABLES IN DATABASE \" \" TO role atlan_user_role ; GRANT REFERENCES ON ALL EXTERNAL TABLES IN DATABASE \" \" TO atlan_user_role ; GRANT REFERENCES ON ALL VIEWS IN DATABASE \" \" TO role atlan_user_role ; Make sure you run the below commands as well so that new tables and views you've created in-between are also visible to the user: The query miner only mines query history for up to the previous two weeks. The miner will not mine any queries that ran before that time window. If the queries that created your assets ran before that time window, lineage for those assets will not be present. To mine more than the previous two weeks of query history, either use S3-based query mining or contact Atlan support . Note that Snowflake itself only retains query history for so long as well, though. Once Snowflake itself no longer contains the query history we will be unable to mine it for lineage. Lineage is unsupported for parameterized queries. Snowflake currently does not resolve values for parameterized queries before logging them in query history. This limits Atlan from generating lineage in such cases. When using the account usage extraction method, there are currently some limitations. We are working with Snowflake to find workarounds for crawling the following: External table location data Procedures Primary key designation External table location data Primary key designation Furthermore, only database-level filtering is currently possible. When using the account usage method for fetching metadata, Atlan requires access to the following views in Snowflake: For the crawler: DATABASES , SCHEMATA , TABLES , VIEWS , COLUMNS , and PIPES For the miner and popularity metrics : QUERY_HISTORY , ACCESS_HISTORY , and SESSIONS This error can occur when you're connecting to Snowflake through Okta SSO and enter the URL of your Snowflake instance in a format different from the one used in Okta. Snowflake follows two URL formats: Legacy format - . .snowflakecomputing.com or . . .snowflakecomputing.com New URL format - - .snowflakecomputing.com Ensure that you're using the same Snowflake URL format in Snowflake and Okta. Refer to Snowflake documentation to learn more. If you're getting the following error messages - java.net.UnknownHostException and Name or service not known - this is a known error for users who have upgraded to the Snowflake JDBC driver version 3.13.25., have underscores in their account name, and connect to their Snowflake accounts over private link (for example, https:\/\/my_account.us-west-2.privatelink.snowflakecomputing.com ). If your Snowflake account name has an underscore - for example, my_account - the updated JDBC driver will automatically convert underscores to dashes or hyphens - . This does not affect normal URLs because Snowflake accepts URLs with both hyphens and underscores. For private link users, however, the JDBC driver will return an error if there are underscores present in the account name and the connection will fail. To troubleshoot further, refer to Snowflake documentation ."}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/snowflake\/how-tos\/set-up-snowflake#to-crawl-assets-generate-lineage-and-import-tags","title":"Set up Snowflake | Atlan Documentation","text":"You need your Snowflake administrator to run these commands - you may not have access yourself. Create user and role in Snowflake Create a role and user in Snowflake using the following commands: Create role Create a role in Snowflake using the following commands: Replace with the default warehouse to use when running the Snowflake crawler . Atlan requires the following privileges to: OPERATE enables Atlan to start the virtual warehouse to fetch metadata if the warehouse has stopped. USAGE enables Atlan to show or list metadata from Snowflake. This in turn enables the Snowflake crawler to run the SHOW query. Create a user Create a separate user to integrate into Atlan, using one of the following 3 options: See Snowflake's official guide for details on generating an RSA key-pair . To create a user with a key-pair, replace the value for rsa_public_key with the public key and run the following: Learn more about the SERVICE type property in Snowflake documentation . Atlan only supports encrypted private keys with a non-empty passphrase - generally recommended as more secure. An empty passphrase results in workflow failures. To generate an encrypted private key, omit the -nocrypt option. Refer to Snowflake documentation to learn more. Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. To create a user with a password, replace and run the following: Learn more about the LEGACY_SERVICE type property in Snowflake documentation . This method is currently only available if Okta is your IdP (Snowflake supports) authenticating natively through Okta : Create a user in your identity provider (IdP) and use federated authentication in Snowflake . The password for this user must be maintained solely in the IdP and multi-factor authentication (MFA) must be disabled. Grant role to user To grant the atlan_user_role to the new user: Configure OAuth (client credentials flow) with Microsoft Entra ID To configure OAuth authentication using Microsoft Entra ID (formerly Azure AD) with the client credentials flow: Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions In Snowflake, create a security integration using the following: CREATE SECURITY INTEGRATION external_oauth_azure_ad TYPE = external_oauth ENABLED = true EXTERNAL_OAUTH_TYPE = azure EXTERNAL_OAUTH_ISSUER = '\\ ' EXTERNAL_OAUTH_JWS_KEYS_URL = '\\ ' EXTERNAL_OAUTH_AUDIENCE_LIST = ( '\\ ' ) EXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM = 'sub' EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE = 'login_name' ; Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app In Snowflake, create a security integration using the following: Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: CREATE USER oauth_svc_user WITH LOGIN_NAME = '\\ ' -- Use Azure client OBJECT ID DEFAULT_ROLE = \\ DEFAULT_WAREHOUSE = \\ ; Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: Grant the configured role to this user: GRANT ROLE \\ TO USER oauth_svc_user ; Grant the configured role to this user: Choose metadata fetching method Atlan supports two methods for fetching metadata from Snowflake - account usage and information schema. You should choose one of these two methods to set up Snowflake: Grant permissions for account usage method If you want to set warehouse timeouts when using this method, set a large value initially for the workflow to succeed. Once the workflow has succeeded, adjust the value to be twice the extraction time. This method uses the views in SNOWFLAKE.ACCOUNT_USAGE (or a copied version of this schema) to fetch the metadata from Snowflake into Atlan. You can be more granular with permissions using this method, but there are limitations with this approach . To crawl assets, generate lineage, and import tags If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Snowflake stores all tag objects in the ACCOUNT_USAGE schema. If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner , you need to grant the same permissions to import tags as required for crawling Snowflake assets. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. To crawl streams To crawl streams, provide the following permissions: To crawl current streams: Replace with the Snowflake database name. Replace with the Snowflake database name. To crawl future streams: To crawl future streams: Replace with the Snowflake database name. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. Grant permissions for information schema method This method uses views in the INFORMATION_SCHEMA schema in Snowflake databases to fetch metadata. You still need to grant specific permissions to enable Atlan to crawl metadata, preview data, and query data with this method. To crawl existing assets Grant these permissions to crawl assets that already exist in Snowflake. If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Grant permissions to crawl existing assets: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) Grant permissions to crawl functions: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) For secure user-defined functions (UDFs), grant OWNERSHIP permissions to retrieve metadata: Replace the placeholders with the appropriate values: : The name of the schema that contains the user-defined function (UDF). : The name of the secure UDF that requires ownership permissions. : The role that gets assigned ownership of the secure UDF. The statements given on this page apply to all schemas, tables, and views in a database in Snowflake. If you want to limit access to only certain objects, you can instead specify the exact objects individually as well. To crawl future assets To crawl assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. To grant permissions at a database level: Replace with the database you want to crawl in Atlan. (Repeat the statements for every database you want to integrate into Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) To mine query history for lineage To also mine Snowflake's query history (for lineage), add these permissions. You can use either option: To mine query history direct from Snowflake's internal tables: To mine query history from a cloned or copied set of tables, where you can also remove any sensitive data: Replace with the name of the cloned database, and with the name of the cloned schema containing account usage details. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to be able to preview and query in Atlan. (Repeat the statements for every database and schema you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. (Optional) To import Snowflake tags Snowflake stores all tag objects in the ACCOUNT_USAGE schema. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To import tags from Snowflake , grant these permissions: To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. (Optional) To push updated tags to Snowflake To push tags updated for assets in Atlan to Snowflake , grant these permissions: You can learn more about tag privileges from Snowflake documentation . (Optional) To crawl dynamic tables Atlan currently supports fetching metadata for dynamic tables using the MONITOR privilege. Refer to Snowflake documentation to learn more. To crawl existing dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: To crawl future dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) (Optional) To crawl Iceberg tables Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method. To crawl Iceberg tables from Snowflake, grant the following permissions: To crawl existing Iceberg tables in Snowflake: To crawl future Iceberg tables in Snowflake: To crawl Iceberg catalog metadata for Iceberg tables in Snowflake: You must first grant permissions to crawl existing Iceberg tables for this permission to work on catalogs. You must also grant permissions to all the catalogs you want to crawl in Atlan individually. (Optional) To crawl Snowflake stages Atlan supports crawling metadata for Snowflake stages using the USAGE and READ privileges. For more information, see the Snowflake documentation for INFORMATION_SCHEMA.STAGES. To crawl stages from Snowflake: Grant USAGE and READ privileges on all existing stages at the database level: Replace with the name of your Snowflake database Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Replace with the role you've granted Atlan to use for crawling. Grant USAGE and READ privileges on all future stages at the database level: Grant USAGE and READ privileges on all future stages at the database level: Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Allowlist the Atlan IP If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support ticket from within Atlan, or submit a request . (If you aren't using the IP allowlist in your Snowflake instance, you can skip this step.) Create user and role in Snowflake Choose metadata fetching method Grant permissions for account usage method Grant permissions for information schema method Allowlist the Atlan IP"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/snowflake\/how-tos\/mine-snowflake","title":"Mine Snowflake | Atlan Documentation","text":"Once you have crawled assets from Snowflake , you can mine its query history to construct lineage. To mine lineage from Snowflake, review the order of operations and then complete the following steps. Select the miner To select the Snowflake miner: In the top right of any screen, navigate to New and then click New Workflow . From the filters along the top, click Miner . From the list of packages, select Snowflake Miner and then click Setup Workflow . Configure the miner To configure the Snowflake miner: For Connection , select the connection to mine. (To select a connection, the crawler must have already run.) For Connection , select the connection to mine. (To select a connection, the crawler must have already run.) For Miner Extraction Method , select Source , Agent , or see the separate instructions for the S3 miner . For Miner Extraction Method , select Source , Agent , or see the separate instructions for the S3 miner . For Snowflake Database : If the connection is configured with access to the snowflake database , choose Default . If the connection can only access a separate cloned database , choose Cloned Database . For Snowflake Database : If the connection is configured with access to the snowflake database , choose Default . If the connection can only access a separate cloned database , choose Cloned Database . If you are using a cloned database, enter the name of the cloned database in Database Name and the name of the cloned schema in Schema Name . If you are using a cloned database, enter the name of the cloned database in Database Name and the name of the cloned schema in Schema Name . For Start time , choose the earliest date from which to mine query history. info Ã° Âª Did you know? The miner restricts you to only querying the past two weeks of query history. If you need to query more history, for example in an initial load, consider using the S3 miner first. After the initial load, you can modify the miner's configuration to use query history extraction. For Start time , choose the earliest date from which to mine query history. Ã° Âª Did you know? The miner restricts you to only querying the past two weeks of query history. If you need to query more history, for example in an initial load, consider using the S3 miner first. After the initial load, you can modify the miner's configuration to use query history extraction. To check for any permissions or other configuration issues before running the miner, click Preflight checks . To check for any permissions or other configuration issues before running the miner, click Preflight checks . At the bottom of the screen, click Next to proceed. At the bottom of the screen, click Next to proceed. Agent extraction method Atlan supports using a Secure Agent for mining query history from Snowflake. To use a Secure Agent, follow these steps: Query redaction is a compute-intensive operation and can impact workflow performance. To use this feature effectively, assign at least 4 CPU cores to the agent. Select the Agent tab. Configure Query Redaction : By default, Query Redaction is turned off. You may choose to configure query redaction: Set Enable Query Redaction flag to true if you want to redact PII data from query history before ingesting it to Atlan. Configure PII Patterns for Query Redaction : Define patterns to identify and redact sensitive information using the following JSON format: { \"pii_patterns\" : [ { \"name\" : \"email\" , \"regex\" : \"\\\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Za-z]{2,}\\\\b\" , \"replacement\" : \"[EMAIL]\" } ] } Set Enable Query Redaction flag to true if you want to redact PII data from query history before ingesting it to Atlan. Configure PII Patterns for Query Redaction : Define patterns to identify and redact sensitive information using the following JSON format: { \"pii_patterns\" : [ { \"name\" : \"email\" , \"regex\" : \"\\\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Za-z]{2,}\\\\b\" , \"replacement\" : \"[EMAIL]\" } ] } Configure the Snowflake data source by adding the secret keys for your secret store. For details on the required fields, refer to the connection configuration used when crawling Snowflake . Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. Click Next after completing the configuration. If running the miner for the first time, Atlan recommends setting a start date around three days prior to the current date and then scheduling it daily to build up to two weeks of query history. Mining two weeks of query history on the first miner run may cause delays. For all subsequent runs, Atlan requires a minimum lag of 24 to 48 hours to capture all the relevant transformations that were part of a session. Learn more about the miner logic here . Configure the miner behavior To configure the Snowflake miner behavior: (Optional) For Calculate popularity , keep True to retrieve usage and popularity metrics for your Snowflake assets from query history. For Excluded Users , type the names of users to be excluded while calculating usage metrics for Snowflake assets. Press Enter after each name to add more names. For Excluded Users , type the names of users to be excluded while calculating usage metrics for Snowflake assets. Press Enter after each name to add more names. (Optional) For Advanced Config , keep Default for the default configuration or click Custom to configure the miner: If Atlan support has provided you with a custom control configuration, enter the configuration into the Custom Config box. You can also enter { ignore-all-case : true} to enable crawling assets with case-sensitive identifiers. For Popularity Window (days) , 90 days is the maximum limit. You can set a shorter popularity window of less than 90 days. If Atlan support has provided you with a custom control configuration, enter the configuration into the Custom Config box. You can also enter { ignore-all-case : true} to enable crawling assets with case-sensitive identifiers. For Popularity Window (days) , 90 days is the maximum limit. You can set a shorter popularity window of less than 90 days. Run the miner To run the Snowflake miner, after completing the configuration steps: To run the miner once immediately, at the bottom of the screen, click the Run button. To schedule the miner to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run button. Once the miner has completed running, you can see lineage for Snowflake assets that were created in Snowflake between the start time and when the miner ran! Ã° Select the miner Configure the miner Configure the miner behavior Run the miner"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/snowflake\/how-tos\/manage-snowflake-tags","title":"Manage Snowflake tags | Atlan Documentation","text":"Note that object tagging in Snowflake currently requires Enterprise Edition or higher . Atlan enables you to import your Snowflake tags , update your Snowflake assets with the imported tags, and push the tag updates back to Snowflake: Import tags - crawl Snowflake tags from Snowflake to Atlan Reverse sync - sync Snowflake tag updates from Atlan to Snowflake Once you've imported your Snowflake tags to Atlan: Your Snowflake assets in Atlan are automatically enriched with their Snowflake tags. Imported Snowflake tags are mapped to corresponding Atlan tags through case-insensitive name match - multiple Snowflake tags can be matched to a single tag in Atlan. You can also attach Snowflake tags , including tag values, to your Snowflake assets in Atlan - allowing you to categorize your assets at a more granular level. Atlan supports: Allowed values : attach an allowed value from a predefined list of values imported from Snowflake. Tag values: enter any value in Atlan while attaching or editing imported Snowflake tags on an asset. Allowed values : attach an allowed value from a predefined list of values imported from Snowflake. Tag values: enter any value in Atlan while attaching or editing imported Snowflake tags on an asset. You can enable reverse sync to push any tag updates for your Snowflake assets back to Snowflake - including allowed and tag values added to assets in Atlan. You can filter your assets by Snowflake tags and tag and allowed values. Enabling reverse sync only updates existing tags in Snowflake. It neither creates nor deletes any tags in Snowflake. Additional privileges are only required when using the information schema method for fetching metadata. This is because Snowflake stores all tag objects in the ACCOUNT_USAGE schema. If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner , any permissions required are already set. Account usage method Before you can import tags from Snowflake, you need to do the following: Create tags or have existing tags in Snowflake. Grant the same permissions as required for crawling Snowflake assets to import tags and push updated tags to Snowflake. Information schema method Before you can import tags from Snowflake, you need to do the following: Create tags or have existing tags in Snowflake. Grant additional permissions to import tags from Snowflake. Grant additional permissions to push updated tags to Snowflake. Import Snowflake tags to Atlan You need to be an admin user in Atlan to import Snowflake tags to Atlan. You also need to work with your Snowflake administrator to grant additional permissions to import tags from Snowflake - you may not have access yourself. You can import your Snowflake tags to Atlan through one-way tag sync. The synced Snowflake tags are matched to corresponding tags in Atlan through case-insensitive name match and your Snowflake assets are enriched with their synced tags from Snowflake. To import Snowflake tags to Atlan, you can either: Create a new Snowflake workflow and configure the crawler to import tags. Modify the crawler's configuration for an existing Snowflake workflow to change Import Tags to Yes . If you subsequently modify the workflow to disable tag import, for any tags already imported, Atlan preserves those tags. Once the crawler has completed running, tags imported from Snowflake are available to use for tagging assets! Ã° View Snowflake tags in Atlan Once you've imported your Snowflake tags, you can view and manage your Snowflake tags in Atlan. To view Snowflake tags: From the left menu of any screen, click Governance . Under the Governance heading of the _Governance cente_r, click Tags . (Optional) Under Tags , click the funnel icon to filter tags by source type. Click Snowflake to filter for tags imported from Snowflake. From the left menu under Tags , select a synced tag - synced tags display the Snowflake Ã¯Â¸ icon next to the tag name. (Optional) Click the Linked assets tab to view linked assets for your Snowflake tag. (Optional) In the top right, click the pencil icon to add a description and change the tag icon . You can't rename tags synced from Snowflake. Push tag updates to Snowflake Any admin or member user in Atlan can configure reverse sync for tag updates to Snowflake. You also need to work with your Snowflake administrator to grant additional permissions to push updates - you may not have access yourself. Reverse sync is currently only available for imported Snowflake tags in Atlan. The imported tags display a Snowflake Ã¯Â¸ icon next to the tag name. If using the account usage method , expect a data latency of up to 3 hours for reverse tag sync to be successful. You can enable reverse sync for your imported Snowflake tags in Atlan and push all tag updates for your Snowflake assets back to source. Once you have enabled reverse sync, any Snowflake assets with tags updated in Atlan are also updated in Snowflake. To enable reverse sync for imported Snowflake tags: From the left menu of any screen, click Governance . Under the Governance heading of the _Governance cente_r, click Tags . (Optional) Under Tags , click the funnel icon to filter tags by source type. Click Snowflake to filter for tags imported from Snowflake. In the left menu under Tags , select a synced Snowflake tag - synced tags display the Snowflake Ã¯Â¸ icon next to the tag name. Under Synced tags , in the upper right, turn on Enable reverse sync to synchronize tag updates from Atlan to Snowflake. In the advanced settings, you can also enable concatenation to support multiple tag values for a single column. For detailed information about multiple tag values and concatenation, see Multiple tag values and concatenation . In the corresponding confirmation dialog, click Yes, enable it to enable reverse tag sync or click Cancel . Now when you attach Snowflake tags to your Snowflake assets in Atlan, these tag updates are also pushed to Snowflake! Ã° Enabling reverse sync won't trigger any updates in Snowflake until synced tags are attached to Snowflake assets in Atlan. For any questions about managing Snowflake tags, head over here . Import Snowflake tags to Atlan View Snowflake tags in Atlan Push tag updates to Snowflake"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/snowflake\/how-tos\/set-up-snowflake#to-crawl-streams","title":"Set up Snowflake | Atlan Documentation","text":"You need your Snowflake administrator to run these commands - you may not have access yourself. Create user and role in Snowflake Create a role and user in Snowflake using the following commands: Create role Create a role in Snowflake using the following commands: Replace with the default warehouse to use when running the Snowflake crawler . Atlan requires the following privileges to: OPERATE enables Atlan to start the virtual warehouse to fetch metadata if the warehouse has stopped. USAGE enables Atlan to show or list metadata from Snowflake. This in turn enables the Snowflake crawler to run the SHOW query. Create a user Create a separate user to integrate into Atlan, using one of the following 3 options: See Snowflake's official guide for details on generating an RSA key-pair . To create a user with a key-pair, replace the value for rsa_public_key with the public key and run the following: Learn more about the SERVICE type property in Snowflake documentation . Atlan only supports encrypted private keys with a non-empty passphrase - generally recommended as more secure. An empty passphrase results in workflow failures. To generate an encrypted private key, omit the -nocrypt option. Refer to Snowflake documentation to learn more. Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. To create a user with a password, replace and run the following: Learn more about the LEGACY_SERVICE type property in Snowflake documentation . This method is currently only available if Okta is your IdP (Snowflake supports) authenticating natively through Okta : Create a user in your identity provider (IdP) and use federated authentication in Snowflake . The password for this user must be maintained solely in the IdP and multi-factor authentication (MFA) must be disabled. Grant role to user To grant the atlan_user_role to the new user: Configure OAuth (client credentials flow) with Microsoft Entra ID To configure OAuth authentication using Microsoft Entra ID (formerly Azure AD) with the client credentials flow: Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions In Snowflake, create a security integration using the following: CREATE SECURITY INTEGRATION external_oauth_azure_ad TYPE = external_oauth ENABLED = true EXTERNAL_OAUTH_TYPE = azure EXTERNAL_OAUTH_ISSUER = '\\ ' EXTERNAL_OAUTH_JWS_KEYS_URL = '\\ ' EXTERNAL_OAUTH_AUDIENCE_LIST = ( '\\ ' ) EXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM = 'sub' EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE = 'login_name' ; Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app In Snowflake, create a security integration using the following: Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: CREATE USER oauth_svc_user WITH LOGIN_NAME = '\\ ' -- Use Azure client OBJECT ID DEFAULT_ROLE = \\ DEFAULT_WAREHOUSE = \\ ; Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: Grant the configured role to this user: GRANT ROLE \\ TO USER oauth_svc_user ; Grant the configured role to this user: Choose metadata fetching method Atlan supports two methods for fetching metadata from Snowflake - account usage and information schema. You should choose one of these two methods to set up Snowflake: Grant permissions for account usage method If you want to set warehouse timeouts when using this method, set a large value initially for the workflow to succeed. Once the workflow has succeeded, adjust the value to be twice the extraction time. This method uses the views in SNOWFLAKE.ACCOUNT_USAGE (or a copied version of this schema) to fetch the metadata from Snowflake into Atlan. You can be more granular with permissions using this method, but there are limitations with this approach . To crawl assets, generate lineage, and import tags If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Snowflake stores all tag objects in the ACCOUNT_USAGE schema. If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner , you need to grant the same permissions to import tags as required for crawling Snowflake assets. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. To crawl streams To crawl streams, provide the following permissions: To crawl current streams: Replace with the Snowflake database name. Replace with the Snowflake database name. To crawl future streams: To crawl future streams: Replace with the Snowflake database name. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. Grant permissions for information schema method This method uses views in the INFORMATION_SCHEMA schema in Snowflake databases to fetch metadata. You still need to grant specific permissions to enable Atlan to crawl metadata, preview data, and query data with this method. To crawl existing assets Grant these permissions to crawl assets that already exist in Snowflake. If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Grant permissions to crawl existing assets: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) Grant permissions to crawl functions: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) For secure user-defined functions (UDFs), grant OWNERSHIP permissions to retrieve metadata: Replace the placeholders with the appropriate values: : The name of the schema that contains the user-defined function (UDF). : The name of the secure UDF that requires ownership permissions. : The role that gets assigned ownership of the secure UDF. The statements given on this page apply to all schemas, tables, and views in a database in Snowflake. If you want to limit access to only certain objects, you can instead specify the exact objects individually as well. To crawl future assets To crawl assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. To grant permissions at a database level: Replace with the database you want to crawl in Atlan. (Repeat the statements for every database you want to integrate into Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) To mine query history for lineage To also mine Snowflake's query history (for lineage), add these permissions. You can use either option: To mine query history direct from Snowflake's internal tables: To mine query history from a cloned or copied set of tables, where you can also remove any sensitive data: Replace with the name of the cloned database, and with the name of the cloned schema containing account usage details. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to be able to preview and query in Atlan. (Repeat the statements for every database and schema you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. (Optional) To import Snowflake tags Snowflake stores all tag objects in the ACCOUNT_USAGE schema. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To import tags from Snowflake , grant these permissions: To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. (Optional) To push updated tags to Snowflake To push tags updated for assets in Atlan to Snowflake , grant these permissions: You can learn more about tag privileges from Snowflake documentation . (Optional) To crawl dynamic tables Atlan currently supports fetching metadata for dynamic tables using the MONITOR privilege. Refer to Snowflake documentation to learn more. To crawl existing dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: To crawl future dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) (Optional) To crawl Iceberg tables Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method. To crawl Iceberg tables from Snowflake, grant the following permissions: To crawl existing Iceberg tables in Snowflake: To crawl future Iceberg tables in Snowflake: To crawl Iceberg catalog metadata for Iceberg tables in Snowflake: You must first grant permissions to crawl existing Iceberg tables for this permission to work on catalogs. You must also grant permissions to all the catalogs you want to crawl in Atlan individually. (Optional) To crawl Snowflake stages Atlan supports crawling metadata for Snowflake stages using the USAGE and READ privileges. For more information, see the Snowflake documentation for INFORMATION_SCHEMA.STAGES. To crawl stages from Snowflake: Grant USAGE and READ privileges on all existing stages at the database level: Replace with the name of your Snowflake database Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Replace with the role you've granted Atlan to use for crawling. Grant USAGE and READ privileges on all future stages at the database level: Grant USAGE and READ privileges on all future stages at the database level: Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Allowlist the Atlan IP If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support ticket from within Atlan, or submit a request . (If you aren't using the IP allowlist in your Snowflake instance, you can skip this step.) Create user and role in Snowflake Choose metadata fetching method Grant permissions for account usage method Grant permissions for information schema method Allowlist the Atlan IP"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/snowflake\/how-tos\/set-up-snowflake#optional-to-preview-and-query-existing-assets","title":"Set up Snowflake | Atlan Documentation","text":"You need your Snowflake administrator to run these commands - you may not have access yourself. Create user and role in Snowflake Create a role and user in Snowflake using the following commands: Create role Create a role in Snowflake using the following commands: Replace with the default warehouse to use when running the Snowflake crawler . Atlan requires the following privileges to: OPERATE enables Atlan to start the virtual warehouse to fetch metadata if the warehouse has stopped. USAGE enables Atlan to show or list metadata from Snowflake. This in turn enables the Snowflake crawler to run the SHOW query. Create a user Create a separate user to integrate into Atlan, using one of the following 3 options: See Snowflake's official guide for details on generating an RSA key-pair . To create a user with a key-pair, replace the value for rsa_public_key with the public key and run the following: Learn more about the SERVICE type property in Snowflake documentation . Atlan only supports encrypted private keys with a non-empty passphrase - generally recommended as more secure. An empty passphrase results in workflow failures. To generate an encrypted private key, omit the -nocrypt option. Refer to Snowflake documentation to learn more. Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. To create a user with a password, replace and run the following: Learn more about the LEGACY_SERVICE type property in Snowflake documentation . This method is currently only available if Okta is your IdP (Snowflake supports) authenticating natively through Okta : Create a user in your identity provider (IdP) and use federated authentication in Snowflake . The password for this user must be maintained solely in the IdP and multi-factor authentication (MFA) must be disabled. Grant role to user To grant the atlan_user_role to the new user: Configure OAuth (client credentials flow) with Microsoft Entra ID To configure OAuth authentication using Microsoft Entra ID (formerly Azure AD) with the client credentials flow: Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions In Snowflake, create a security integration using the following: CREATE SECURITY INTEGRATION external_oauth_azure_ad TYPE = external_oauth ENABLED = true EXTERNAL_OAUTH_TYPE = azure EXTERNAL_OAUTH_ISSUER = '\\ ' EXTERNAL_OAUTH_JWS_KEYS_URL = '\\ ' EXTERNAL_OAUTH_AUDIENCE_LIST = ( '\\ ' ) EXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM = 'sub' EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE = 'login_name' ; Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app In Snowflake, create a security integration using the following: Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: CREATE USER oauth_svc_user WITH LOGIN_NAME = '\\ ' -- Use Azure client OBJECT ID DEFAULT_ROLE = \\ DEFAULT_WAREHOUSE = \\ ; Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: Grant the configured role to this user: GRANT ROLE \\ TO USER oauth_svc_user ; Grant the configured role to this user: Choose metadata fetching method Atlan supports two methods for fetching metadata from Snowflake - account usage and information schema. You should choose one of these two methods to set up Snowflake: Grant permissions for account usage method If you want to set warehouse timeouts when using this method, set a large value initially for the workflow to succeed. Once the workflow has succeeded, adjust the value to be twice the extraction time. This method uses the views in SNOWFLAKE.ACCOUNT_USAGE (or a copied version of this schema) to fetch the metadata from Snowflake into Atlan. You can be more granular with permissions using this method, but there are limitations with this approach . To crawl assets, generate lineage, and import tags If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Snowflake stores all tag objects in the ACCOUNT_USAGE schema. If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner , you need to grant the same permissions to import tags as required for crawling Snowflake assets. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. To crawl streams To crawl streams, provide the following permissions: To crawl current streams: Replace with the Snowflake database name. Replace with the Snowflake database name. To crawl future streams: To crawl future streams: Replace with the Snowflake database name. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. Grant permissions for information schema method This method uses views in the INFORMATION_SCHEMA schema in Snowflake databases to fetch metadata. You still need to grant specific permissions to enable Atlan to crawl metadata, preview data, and query data with this method. To crawl existing assets Grant these permissions to crawl assets that already exist in Snowflake. If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Grant permissions to crawl existing assets: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) Grant permissions to crawl functions: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) For secure user-defined functions (UDFs), grant OWNERSHIP permissions to retrieve metadata: Replace the placeholders with the appropriate values: : The name of the schema that contains the user-defined function (UDF). : The name of the secure UDF that requires ownership permissions. : The role that gets assigned ownership of the secure UDF. The statements given on this page apply to all schemas, tables, and views in a database in Snowflake. If you want to limit access to only certain objects, you can instead specify the exact objects individually as well. To crawl future assets To crawl assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. To grant permissions at a database level: Replace with the database you want to crawl in Atlan. (Repeat the statements for every database you want to integrate into Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) To mine query history for lineage To also mine Snowflake's query history (for lineage), add these permissions. You can use either option: To mine query history direct from Snowflake's internal tables: To mine query history from a cloned or copied set of tables, where you can also remove any sensitive data: Replace with the name of the cloned database, and with the name of the cloned schema containing account usage details. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to be able to preview and query in Atlan. (Repeat the statements for every database and schema you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. (Optional) To import Snowflake tags Snowflake stores all tag objects in the ACCOUNT_USAGE schema. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To import tags from Snowflake , grant these permissions: To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. (Optional) To push updated tags to Snowflake To push tags updated for assets in Atlan to Snowflake , grant these permissions: You can learn more about tag privileges from Snowflake documentation . (Optional) To crawl dynamic tables Atlan currently supports fetching metadata for dynamic tables using the MONITOR privilege. Refer to Snowflake documentation to learn more. To crawl existing dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: To crawl future dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) (Optional) To crawl Iceberg tables Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method. To crawl Iceberg tables from Snowflake, grant the following permissions: To crawl existing Iceberg tables in Snowflake: To crawl future Iceberg tables in Snowflake: To crawl Iceberg catalog metadata for Iceberg tables in Snowflake: You must first grant permissions to crawl existing Iceberg tables for this permission to work on catalogs. You must also grant permissions to all the catalogs you want to crawl in Atlan individually. (Optional) To crawl Snowflake stages Atlan supports crawling metadata for Snowflake stages using the USAGE and READ privileges. For more information, see the Snowflake documentation for INFORMATION_SCHEMA.STAGES. To crawl stages from Snowflake: Grant USAGE and READ privileges on all existing stages at the database level: Replace with the name of your Snowflake database Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Replace with the role you've granted Atlan to use for crawling. Grant USAGE and READ privileges on all future stages at the database level: Grant USAGE and READ privileges on all future stages at the database level: Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Allowlist the Atlan IP If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support ticket from within Atlan, or submit a request . (If you aren't using the IP allowlist in your Snowflake instance, you can skip this step.) Create user and role in Snowflake Choose metadata fetching method Grant permissions for account usage method Grant permissions for information schema method Allowlist the Atlan IP"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/snowflake\/how-tos\/set-up-snowflake#optional-to-preview-and-query-future-assets","title":"Set up Snowflake | Atlan Documentation","text":"You need your Snowflake administrator to run these commands - you may not have access yourself. Create user and role in Snowflake Create a role and user in Snowflake using the following commands: Create role Create a role in Snowflake using the following commands: Replace with the default warehouse to use when running the Snowflake crawler . Atlan requires the following privileges to: OPERATE enables Atlan to start the virtual warehouse to fetch metadata if the warehouse has stopped. USAGE enables Atlan to show or list metadata from Snowflake. This in turn enables the Snowflake crawler to run the SHOW query. Create a user Create a separate user to integrate into Atlan, using one of the following 3 options: See Snowflake's official guide for details on generating an RSA key-pair . To create a user with a key-pair, replace the value for rsa_public_key with the public key and run the following: Learn more about the SERVICE type property in Snowflake documentation . Atlan only supports encrypted private keys with a non-empty passphrase - generally recommended as more secure. An empty passphrase results in workflow failures. To generate an encrypted private key, omit the -nocrypt option. Refer to Snowflake documentation to learn more. Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. To create a user with a password, replace and run the following: Learn more about the LEGACY_SERVICE type property in Snowflake documentation . This method is currently only available if Okta is your IdP (Snowflake supports) authenticating natively through Okta : Create a user in your identity provider (IdP) and use federated authentication in Snowflake . The password for this user must be maintained solely in the IdP and multi-factor authentication (MFA) must be disabled. Grant role to user To grant the atlan_user_role to the new user: Configure OAuth (client credentials flow) with Microsoft Entra ID To configure OAuth authentication using Microsoft Entra ID (formerly Azure AD) with the client credentials flow: Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions In Snowflake, create a security integration using the following: CREATE SECURITY INTEGRATION external_oauth_azure_ad TYPE = external_oauth ENABLED = true EXTERNAL_OAUTH_TYPE = azure EXTERNAL_OAUTH_ISSUER = '\\ ' EXTERNAL_OAUTH_JWS_KEYS_URL = '\\ ' EXTERNAL_OAUTH_AUDIENCE_LIST = ( '\\ ' ) EXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM = 'sub' EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE = 'login_name' ; Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app In Snowflake, create a security integration using the following: Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: CREATE USER oauth_svc_user WITH LOGIN_NAME = '\\ ' -- Use Azure client OBJECT ID DEFAULT_ROLE = \\ DEFAULT_WAREHOUSE = \\ ; Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: Grant the configured role to this user: GRANT ROLE \\ TO USER oauth_svc_user ; Grant the configured role to this user: Choose metadata fetching method Atlan supports two methods for fetching metadata from Snowflake - account usage and information schema. You should choose one of these two methods to set up Snowflake: Grant permissions for account usage method If you want to set warehouse timeouts when using this method, set a large value initially for the workflow to succeed. Once the workflow has succeeded, adjust the value to be twice the extraction time. This method uses the views in SNOWFLAKE.ACCOUNT_USAGE (or a copied version of this schema) to fetch the metadata from Snowflake into Atlan. You can be more granular with permissions using this method, but there are limitations with this approach . To crawl assets, generate lineage, and import tags If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Snowflake stores all tag objects in the ACCOUNT_USAGE schema. If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner , you need to grant the same permissions to import tags as required for crawling Snowflake assets. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. To crawl streams To crawl streams, provide the following permissions: To crawl current streams: Replace with the Snowflake database name. Replace with the Snowflake database name. To crawl future streams: To crawl future streams: Replace with the Snowflake database name. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. Grant permissions for information schema method This method uses views in the INFORMATION_SCHEMA schema in Snowflake databases to fetch metadata. You still need to grant specific permissions to enable Atlan to crawl metadata, preview data, and query data with this method. To crawl existing assets Grant these permissions to crawl assets that already exist in Snowflake. If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Grant permissions to crawl existing assets: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) Grant permissions to crawl functions: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) For secure user-defined functions (UDFs), grant OWNERSHIP permissions to retrieve metadata: Replace the placeholders with the appropriate values: : The name of the schema that contains the user-defined function (UDF). : The name of the secure UDF that requires ownership permissions. : The role that gets assigned ownership of the secure UDF. The statements given on this page apply to all schemas, tables, and views in a database in Snowflake. If you want to limit access to only certain objects, you can instead specify the exact objects individually as well. To crawl future assets To crawl assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. To grant permissions at a database level: Replace with the database you want to crawl in Atlan. (Repeat the statements for every database you want to integrate into Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) To mine query history for lineage To also mine Snowflake's query history (for lineage), add these permissions. You can use either option: To mine query history direct from Snowflake's internal tables: To mine query history from a cloned or copied set of tables, where you can also remove any sensitive data: Replace with the name of the cloned database, and with the name of the cloned schema containing account usage details. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to be able to preview and query in Atlan. (Repeat the statements for every database and schema you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. (Optional) To import Snowflake tags Snowflake stores all tag objects in the ACCOUNT_USAGE schema. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To import tags from Snowflake , grant these permissions: To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. (Optional) To push updated tags to Snowflake To push tags updated for assets in Atlan to Snowflake , grant these permissions: You can learn more about tag privileges from Snowflake documentation . (Optional) To crawl dynamic tables Atlan currently supports fetching metadata for dynamic tables using the MONITOR privilege. Refer to Snowflake documentation to learn more. To crawl existing dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: To crawl future dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) (Optional) To crawl Iceberg tables Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method. To crawl Iceberg tables from Snowflake, grant the following permissions: To crawl existing Iceberg tables in Snowflake: To crawl future Iceberg tables in Snowflake: To crawl Iceberg catalog metadata for Iceberg tables in Snowflake: You must first grant permissions to crawl existing Iceberg tables for this permission to work on catalogs. You must also grant permissions to all the catalogs you want to crawl in Atlan individually. (Optional) To crawl Snowflake stages Atlan supports crawling metadata for Snowflake stages using the USAGE and READ privileges. For more information, see the Snowflake documentation for INFORMATION_SCHEMA.STAGES. To crawl stages from Snowflake: Grant USAGE and READ privileges on all existing stages at the database level: Replace with the name of your Snowflake database Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Replace with the role you've granted Atlan to use for crawling. Grant USAGE and READ privileges on all future stages at the database level: Grant USAGE and READ privileges on all future stages at the database level: Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Allowlist the Atlan IP If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support ticket from within Atlan, or submit a request . (If you aren't using the IP allowlist in your Snowflake instance, you can skip this step.) Create user and role in Snowflake Choose metadata fetching method Grant permissions for account usage method Grant permissions for information schema method Allowlist the Atlan IP"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/snowflake\/how-tos\/set-up-snowflake#grant-permissions-for-information-schema-method","title":"Set up Snowflake | Atlan Documentation","text":"You need your Snowflake administrator to run these commands - you may not have access yourself. Create user and role in Snowflake Create a role and user in Snowflake using the following commands: Create role Create a role in Snowflake using the following commands: Replace with the default warehouse to use when running the Snowflake crawler . Atlan requires the following privileges to: OPERATE enables Atlan to start the virtual warehouse to fetch metadata if the warehouse has stopped. USAGE enables Atlan to show or list metadata from Snowflake. This in turn enables the Snowflake crawler to run the SHOW query. Create a user Create a separate user to integrate into Atlan, using one of the following 3 options: See Snowflake's official guide for details on generating an RSA key-pair . To create a user with a key-pair, replace the value for rsa_public_key with the public key and run the following: Learn more about the SERVICE type property in Snowflake documentation . Atlan only supports encrypted private keys with a non-empty passphrase - generally recommended as more secure. An empty passphrase results in workflow failures. To generate an encrypted private key, omit the -nocrypt option. Refer to Snowflake documentation to learn more. Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. To create a user with a password, replace and run the following: Learn more about the LEGACY_SERVICE type property in Snowflake documentation . This method is currently only available if Okta is your IdP (Snowflake supports) authenticating natively through Okta : Create a user in your identity provider (IdP) and use federated authentication in Snowflake . The password for this user must be maintained solely in the IdP and multi-factor authentication (MFA) must be disabled. Grant role to user To grant the atlan_user_role to the new user: Configure OAuth (client credentials flow) with Microsoft Entra ID To configure OAuth authentication using Microsoft Entra ID (formerly Azure AD) with the client credentials flow: Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions In Snowflake, create a security integration using the following: CREATE SECURITY INTEGRATION external_oauth_azure_ad TYPE = external_oauth ENABLED = true EXTERNAL_OAUTH_TYPE = azure EXTERNAL_OAUTH_ISSUER = '\\ ' EXTERNAL_OAUTH_JWS_KEYS_URL = '\\ ' EXTERNAL_OAUTH_AUDIENCE_LIST = ( '\\ ' ) EXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM = 'sub' EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE = 'login_name' ; Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app In Snowflake, create a security integration using the following: Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: CREATE USER oauth_svc_user WITH LOGIN_NAME = '\\ ' -- Use Azure client OBJECT ID DEFAULT_ROLE = \\ DEFAULT_WAREHOUSE = \\ ; Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: Grant the configured role to this user: GRANT ROLE \\ TO USER oauth_svc_user ; Grant the configured role to this user: Choose metadata fetching method Atlan supports two methods for fetching metadata from Snowflake - account usage and information schema. You should choose one of these two methods to set up Snowflake: Grant permissions for account usage method If you want to set warehouse timeouts when using this method, set a large value initially for the workflow to succeed. Once the workflow has succeeded, adjust the value to be twice the extraction time. This method uses the views in SNOWFLAKE.ACCOUNT_USAGE (or a copied version of this schema) to fetch the metadata from Snowflake into Atlan. You can be more granular with permissions using this method, but there are limitations with this approach . To crawl assets, generate lineage, and import tags If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Snowflake stores all tag objects in the ACCOUNT_USAGE schema. If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner , you need to grant the same permissions to import tags as required for crawling Snowflake assets. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. To crawl streams To crawl streams, provide the following permissions: To crawl current streams: Replace with the Snowflake database name. Replace with the Snowflake database name. To crawl future streams: To crawl future streams: Replace with the Snowflake database name. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. Grant permissions for information schema method This method uses views in the INFORMATION_SCHEMA schema in Snowflake databases to fetch metadata. You still need to grant specific permissions to enable Atlan to crawl metadata, preview data, and query data with this method. To crawl existing assets Grant these permissions to crawl assets that already exist in Snowflake. If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Grant permissions to crawl existing assets: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) Grant permissions to crawl functions: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) For secure user-defined functions (UDFs), grant OWNERSHIP permissions to retrieve metadata: Replace the placeholders with the appropriate values: : The name of the schema that contains the user-defined function (UDF). : The name of the secure UDF that requires ownership permissions. : The role that gets assigned ownership of the secure UDF. The statements given on this page apply to all schemas, tables, and views in a database in Snowflake. If you want to limit access to only certain objects, you can instead specify the exact objects individually as well. To crawl future assets To crawl assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. To grant permissions at a database level: Replace with the database you want to crawl in Atlan. (Repeat the statements for every database you want to integrate into Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) To mine query history for lineage To also mine Snowflake's query history (for lineage), add these permissions. You can use either option: To mine query history direct from Snowflake's internal tables: To mine query history from a cloned or copied set of tables, where you can also remove any sensitive data: Replace with the name of the cloned database, and with the name of the cloned schema containing account usage details. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to be able to preview and query in Atlan. (Repeat the statements for every database and schema you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. (Optional) To import Snowflake tags Snowflake stores all tag objects in the ACCOUNT_USAGE schema. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To import tags from Snowflake , grant these permissions: To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. (Optional) To push updated tags to Snowflake To push tags updated for assets in Atlan to Snowflake , grant these permissions: You can learn more about tag privileges from Snowflake documentation . (Optional) To crawl dynamic tables Atlan currently supports fetching metadata for dynamic tables using the MONITOR privilege. Refer to Snowflake documentation to learn more. To crawl existing dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: To crawl future dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) (Optional) To crawl Iceberg tables Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method. To crawl Iceberg tables from Snowflake, grant the following permissions: To crawl existing Iceberg tables in Snowflake: To crawl future Iceberg tables in Snowflake: To crawl Iceberg catalog metadata for Iceberg tables in Snowflake: You must first grant permissions to crawl existing Iceberg tables for this permission to work on catalogs. You must also grant permissions to all the catalogs you want to crawl in Atlan individually. (Optional) To crawl Snowflake stages Atlan supports crawling metadata for Snowflake stages using the USAGE and READ privileges. For more information, see the Snowflake documentation for INFORMATION_SCHEMA.STAGES. To crawl stages from Snowflake: Grant USAGE and READ privileges on all existing stages at the database level: Replace with the name of your Snowflake database Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Replace with the role you've granted Atlan to use for crawling. Grant USAGE and READ privileges on all future stages at the database level: Grant USAGE and READ privileges on all future stages at the database level: Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Allowlist the Atlan IP If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support ticket from within Atlan, or submit a request . (If you aren't using the IP allowlist in your Snowflake instance, you can skip this step.) Create user and role in Snowflake Choose metadata fetching method Grant permissions for account usage method Grant permissions for information schema method Allowlist the Atlan IP"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/snowflake\/how-tos\/set-up-snowflake#to-crawl-existing-assets","title":"Set up Snowflake | Atlan Documentation","text":"You need your Snowflake administrator to run these commands - you may not have access yourself. Create user and role in Snowflake Create a role and user in Snowflake using the following commands: Create role Create a role in Snowflake using the following commands: Replace with the default warehouse to use when running the Snowflake crawler . Atlan requires the following privileges to: OPERATE enables Atlan to start the virtual warehouse to fetch metadata if the warehouse has stopped. USAGE enables Atlan to show or list metadata from Snowflake. This in turn enables the Snowflake crawler to run the SHOW query. Create a user Create a separate user to integrate into Atlan, using one of the following 3 options: See Snowflake's official guide for details on generating an RSA key-pair . To create a user with a key-pair, replace the value for rsa_public_key with the public key and run the following: Learn more about the SERVICE type property in Snowflake documentation . Atlan only supports encrypted private keys with a non-empty passphrase - generally recommended as more secure. An empty passphrase results in workflow failures. To generate an encrypted private key, omit the -nocrypt option. Refer to Snowflake documentation to learn more. Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. To create a user with a password, replace and run the following: Learn more about the LEGACY_SERVICE type property in Snowflake documentation . This method is currently only available if Okta is your IdP (Snowflake supports) authenticating natively through Okta : Create a user in your identity provider (IdP) and use federated authentication in Snowflake . The password for this user must be maintained solely in the IdP and multi-factor authentication (MFA) must be disabled. Grant role to user To grant the atlan_user_role to the new user: Configure OAuth (client credentials flow) with Microsoft Entra ID To configure OAuth authentication using Microsoft Entra ID (formerly Azure AD) with the client credentials flow: Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions In Snowflake, create a security integration using the following: CREATE SECURITY INTEGRATION external_oauth_azure_ad TYPE = external_oauth ENABLED = true EXTERNAL_OAUTH_TYPE = azure EXTERNAL_OAUTH_ISSUER = '\\ ' EXTERNAL_OAUTH_JWS_KEYS_URL = '\\ ' EXTERNAL_OAUTH_AUDIENCE_LIST = ( '\\ ' ) EXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM = 'sub' EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE = 'login_name' ; Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app In Snowflake, create a security integration using the following: Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: CREATE USER oauth_svc_user WITH LOGIN_NAME = '\\ ' -- Use Azure client OBJECT ID DEFAULT_ROLE = \\ DEFAULT_WAREHOUSE = \\ ; Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: Grant the configured role to this user: GRANT ROLE \\ TO USER oauth_svc_user ; Grant the configured role to this user: Choose metadata fetching method Atlan supports two methods for fetching metadata from Snowflake - account usage and information schema. You should choose one of these two methods to set up Snowflake: Grant permissions for account usage method If you want to set warehouse timeouts when using this method, set a large value initially for the workflow to succeed. Once the workflow has succeeded, adjust the value to be twice the extraction time. This method uses the views in SNOWFLAKE.ACCOUNT_USAGE (or a copied version of this schema) to fetch the metadata from Snowflake into Atlan. You can be more granular with permissions using this method, but there are limitations with this approach . To crawl assets, generate lineage, and import tags If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Snowflake stores all tag objects in the ACCOUNT_USAGE schema. If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner , you need to grant the same permissions to import tags as required for crawling Snowflake assets. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. To crawl streams To crawl streams, provide the following permissions: To crawl current streams: Replace with the Snowflake database name. Replace with the Snowflake database name. To crawl future streams: To crawl future streams: Replace with the Snowflake database name. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. Grant permissions for information schema method This method uses views in the INFORMATION_SCHEMA schema in Snowflake databases to fetch metadata. You still need to grant specific permissions to enable Atlan to crawl metadata, preview data, and query data with this method. To crawl existing assets Grant these permissions to crawl assets that already exist in Snowflake. If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Grant permissions to crawl existing assets: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) Grant permissions to crawl functions: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) For secure user-defined functions (UDFs), grant OWNERSHIP permissions to retrieve metadata: Replace the placeholders with the appropriate values: : The name of the schema that contains the user-defined function (UDF). : The name of the secure UDF that requires ownership permissions. : The role that gets assigned ownership of the secure UDF. The statements given on this page apply to all schemas, tables, and views in a database in Snowflake. If you want to limit access to only certain objects, you can instead specify the exact objects individually as well. To crawl future assets To crawl assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. To grant permissions at a database level: Replace with the database you want to crawl in Atlan. (Repeat the statements for every database you want to integrate into Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) To mine query history for lineage To also mine Snowflake's query history (for lineage), add these permissions. You can use either option: To mine query history direct from Snowflake's internal tables: To mine query history from a cloned or copied set of tables, where you can also remove any sensitive data: Replace with the name of the cloned database, and with the name of the cloned schema containing account usage details. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to be able to preview and query in Atlan. (Repeat the statements for every database and schema you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. (Optional) To import Snowflake tags Snowflake stores all tag objects in the ACCOUNT_USAGE schema. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To import tags from Snowflake , grant these permissions: To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. (Optional) To push updated tags to Snowflake To push tags updated for assets in Atlan to Snowflake , grant these permissions: You can learn more about tag privileges from Snowflake documentation . (Optional) To crawl dynamic tables Atlan currently supports fetching metadata for dynamic tables using the MONITOR privilege. Refer to Snowflake documentation to learn more. To crawl existing dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: To crawl future dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) (Optional) To crawl Iceberg tables Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method. To crawl Iceberg tables from Snowflake, grant the following permissions: To crawl existing Iceberg tables in Snowflake: To crawl future Iceberg tables in Snowflake: To crawl Iceberg catalog metadata for Iceberg tables in Snowflake: You must first grant permissions to crawl existing Iceberg tables for this permission to work on catalogs. You must also grant permissions to all the catalogs you want to crawl in Atlan individually. (Optional) To crawl Snowflake stages Atlan supports crawling metadata for Snowflake stages using the USAGE and READ privileges. For more information, see the Snowflake documentation for INFORMATION_SCHEMA.STAGES. To crawl stages from Snowflake: Grant USAGE and READ privileges on all existing stages at the database level: Replace with the name of your Snowflake database Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Replace with the role you've granted Atlan to use for crawling. Grant USAGE and READ privileges on all future stages at the database level: Grant USAGE and READ privileges on all future stages at the database level: Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Allowlist the Atlan IP If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support ticket from within Atlan, or submit a request . (If you aren't using the IP allowlist in your Snowflake instance, you can skip this step.) Create user and role in Snowflake Choose metadata fetching method Grant permissions for account usage method Grant permissions for information schema method Allowlist the Atlan IP"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/snowflake\/how-tos\/set-up-snowflake#to-crawl-future-assets","title":"Set up Snowflake | Atlan Documentation","text":"You need your Snowflake administrator to run these commands - you may not have access yourself. Create user and role in Snowflake Create a role and user in Snowflake using the following commands: Create role Create a role in Snowflake using the following commands: Replace with the default warehouse to use when running the Snowflake crawler . Atlan requires the following privileges to: OPERATE enables Atlan to start the virtual warehouse to fetch metadata if the warehouse has stopped. USAGE enables Atlan to show or list metadata from Snowflake. This in turn enables the Snowflake crawler to run the SHOW query. Create a user Create a separate user to integrate into Atlan, using one of the following 3 options: See Snowflake's official guide for details on generating an RSA key-pair . To create a user with a key-pair, replace the value for rsa_public_key with the public key and run the following: Learn more about the SERVICE type property in Snowflake documentation . Atlan only supports encrypted private keys with a non-empty passphrase - generally recommended as more secure. An empty passphrase results in workflow failures. To generate an encrypted private key, omit the -nocrypt option. Refer to Snowflake documentation to learn more. Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. To create a user with a password, replace and run the following: Learn more about the LEGACY_SERVICE type property in Snowflake documentation . This method is currently only available if Okta is your IdP (Snowflake supports) authenticating natively through Okta : Create a user in your identity provider (IdP) and use federated authentication in Snowflake . The password for this user must be maintained solely in the IdP and multi-factor authentication (MFA) must be disabled. Grant role to user To grant the atlan_user_role to the new user: Configure OAuth (client credentials flow) with Microsoft Entra ID To configure OAuth authentication using Microsoft Entra ID (formerly Azure AD) with the client credentials flow: Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions In Snowflake, create a security integration using the following: CREATE SECURITY INTEGRATION external_oauth_azure_ad TYPE = external_oauth ENABLED = true EXTERNAL_OAUTH_TYPE = azure EXTERNAL_OAUTH_ISSUER = '\\ ' EXTERNAL_OAUTH_JWS_KEYS_URL = '\\ ' EXTERNAL_OAUTH_AUDIENCE_LIST = ( '\\ ' ) EXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM = 'sub' EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE = 'login_name' ; Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app In Snowflake, create a security integration using the following: Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: CREATE USER oauth_svc_user WITH LOGIN_NAME = '\\ ' -- Use Azure client OBJECT ID DEFAULT_ROLE = \\ DEFAULT_WAREHOUSE = \\ ; Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: Grant the configured role to this user: GRANT ROLE \\ TO USER oauth_svc_user ; Grant the configured role to this user: Choose metadata fetching method Atlan supports two methods for fetching metadata from Snowflake - account usage and information schema. You should choose one of these two methods to set up Snowflake: Grant permissions for account usage method If you want to set warehouse timeouts when using this method, set a large value initially for the workflow to succeed. Once the workflow has succeeded, adjust the value to be twice the extraction time. This method uses the views in SNOWFLAKE.ACCOUNT_USAGE (or a copied version of this schema) to fetch the metadata from Snowflake into Atlan. You can be more granular with permissions using this method, but there are limitations with this approach . To crawl assets, generate lineage, and import tags If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Snowflake stores all tag objects in the ACCOUNT_USAGE schema. If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner , you need to grant the same permissions to import tags as required for crawling Snowflake assets. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. To crawl streams To crawl streams, provide the following permissions: To crawl current streams: Replace with the Snowflake database name. Replace with the Snowflake database name. To crawl future streams: To crawl future streams: Replace with the Snowflake database name. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. Grant permissions for information schema method This method uses views in the INFORMATION_SCHEMA schema in Snowflake databases to fetch metadata. You still need to grant specific permissions to enable Atlan to crawl metadata, preview data, and query data with this method. To crawl existing assets Grant these permissions to crawl assets that already exist in Snowflake. If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Grant permissions to crawl existing assets: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) Grant permissions to crawl functions: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) For secure user-defined functions (UDFs), grant OWNERSHIP permissions to retrieve metadata: Replace the placeholders with the appropriate values: : The name of the schema that contains the user-defined function (UDF). : The name of the secure UDF that requires ownership permissions. : The role that gets assigned ownership of the secure UDF. The statements given on this page apply to all schemas, tables, and views in a database in Snowflake. If you want to limit access to only certain objects, you can instead specify the exact objects individually as well. To crawl future assets To crawl assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. To grant permissions at a database level: Replace with the database you want to crawl in Atlan. (Repeat the statements for every database you want to integrate into Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) To mine query history for lineage To also mine Snowflake's query history (for lineage), add these permissions. You can use either option: To mine query history direct from Snowflake's internal tables: To mine query history from a cloned or copied set of tables, where you can also remove any sensitive data: Replace with the name of the cloned database, and with the name of the cloned schema containing account usage details. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to be able to preview and query in Atlan. (Repeat the statements for every database and schema you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. (Optional) To import Snowflake tags Snowflake stores all tag objects in the ACCOUNT_USAGE schema. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To import tags from Snowflake , grant these permissions: To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. (Optional) To push updated tags to Snowflake To push tags updated for assets in Atlan to Snowflake , grant these permissions: You can learn more about tag privileges from Snowflake documentation . (Optional) To crawl dynamic tables Atlan currently supports fetching metadata for dynamic tables using the MONITOR privilege. Refer to Snowflake documentation to learn more. To crawl existing dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: To crawl future dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) (Optional) To crawl Iceberg tables Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method. To crawl Iceberg tables from Snowflake, grant the following permissions: To crawl existing Iceberg tables in Snowflake: To crawl future Iceberg tables in Snowflake: To crawl Iceberg catalog metadata for Iceberg tables in Snowflake: You must first grant permissions to crawl existing Iceberg tables for this permission to work on catalogs. You must also grant permissions to all the catalogs you want to crawl in Atlan individually. (Optional) To crawl Snowflake stages Atlan supports crawling metadata for Snowflake stages using the USAGE and READ privileges. For more information, see the Snowflake documentation for INFORMATION_SCHEMA.STAGES. To crawl stages from Snowflake: Grant USAGE and READ privileges on all existing stages at the database level: Replace with the name of your Snowflake database Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Replace with the role you've granted Atlan to use for crawling. Grant USAGE and READ privileges on all future stages at the database level: Grant USAGE and READ privileges on all future stages at the database level: Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Allowlist the Atlan IP If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support ticket from within Atlan, or submit a request . (If you aren't using the IP allowlist in your Snowflake instance, you can skip this step.) Create user and role in Snowflake Choose metadata fetching method Grant permissions for account usage method Grant permissions for information schema method Allowlist the Atlan IP"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/snowflake\/how-tos\/set-up-snowflake#to-mine-query-history-for-lineage","title":"Set up Snowflake | Atlan Documentation","text":"You need your Snowflake administrator to run these commands - you may not have access yourself. Create user and role in Snowflake Create a role and user in Snowflake using the following commands: Create role Create a role in Snowflake using the following commands: Replace with the default warehouse to use when running the Snowflake crawler . Atlan requires the following privileges to: OPERATE enables Atlan to start the virtual warehouse to fetch metadata if the warehouse has stopped. USAGE enables Atlan to show or list metadata from Snowflake. This in turn enables the Snowflake crawler to run the SHOW query. Create a user Create a separate user to integrate into Atlan, using one of the following 3 options: See Snowflake's official guide for details on generating an RSA key-pair . To create a user with a key-pair, replace the value for rsa_public_key with the public key and run the following: Learn more about the SERVICE type property in Snowflake documentation . Atlan only supports encrypted private keys with a non-empty passphrase - generally recommended as more secure. An empty passphrase results in workflow failures. To generate an encrypted private key, omit the -nocrypt option. Refer to Snowflake documentation to learn more. Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. To create a user with a password, replace and run the following: Learn more about the LEGACY_SERVICE type property in Snowflake documentation . This method is currently only available if Okta is your IdP (Snowflake supports) authenticating natively through Okta : Create a user in your identity provider (IdP) and use federated authentication in Snowflake . The password for this user must be maintained solely in the IdP and multi-factor authentication (MFA) must be disabled. Grant role to user To grant the atlan_user_role to the new user: Configure OAuth (client credentials flow) with Microsoft Entra ID To configure OAuth authentication using Microsoft Entra ID (formerly Azure AD) with the client credentials flow: Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions In Snowflake, create a security integration using the following: CREATE SECURITY INTEGRATION external_oauth_azure_ad TYPE = external_oauth ENABLED = true EXTERNAL_OAUTH_TYPE = azure EXTERNAL_OAUTH_ISSUER = '\\ ' EXTERNAL_OAUTH_JWS_KEYS_URL = '\\ ' EXTERNAL_OAUTH_AUDIENCE_LIST = ( '\\ ' ) EXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM = 'sub' EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE = 'login_name' ; Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app In Snowflake, create a security integration using the following: Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: CREATE USER oauth_svc_user WITH LOGIN_NAME = '\\ ' -- Use Azure client OBJECT ID DEFAULT_ROLE = \\ DEFAULT_WAREHOUSE = \\ ; Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: Grant the configured role to this user: GRANT ROLE \\ TO USER oauth_svc_user ; Grant the configured role to this user: Choose metadata fetching method Atlan supports two methods for fetching metadata from Snowflake - account usage and information schema. You should choose one of these two methods to set up Snowflake: Grant permissions for account usage method If you want to set warehouse timeouts when using this method, set a large value initially for the workflow to succeed. Once the workflow has succeeded, adjust the value to be twice the extraction time. This method uses the views in SNOWFLAKE.ACCOUNT_USAGE (or a copied version of this schema) to fetch the metadata from Snowflake into Atlan. You can be more granular with permissions using this method, but there are limitations with this approach . To crawl assets, generate lineage, and import tags If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Snowflake stores all tag objects in the ACCOUNT_USAGE schema. If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner , you need to grant the same permissions to import tags as required for crawling Snowflake assets. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. To crawl streams To crawl streams, provide the following permissions: To crawl current streams: Replace with the Snowflake database name. Replace with the Snowflake database name. To crawl future streams: To crawl future streams: Replace with the Snowflake database name. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. Grant permissions for information schema method This method uses views in the INFORMATION_SCHEMA schema in Snowflake databases to fetch metadata. You still need to grant specific permissions to enable Atlan to crawl metadata, preview data, and query data with this method. To crawl existing assets Grant these permissions to crawl assets that already exist in Snowflake. If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Grant permissions to crawl existing assets: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) Grant permissions to crawl functions: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) For secure user-defined functions (UDFs), grant OWNERSHIP permissions to retrieve metadata: Replace the placeholders with the appropriate values: : The name of the schema that contains the user-defined function (UDF). : The name of the secure UDF that requires ownership permissions. : The role that gets assigned ownership of the secure UDF. The statements given on this page apply to all schemas, tables, and views in a database in Snowflake. If you want to limit access to only certain objects, you can instead specify the exact objects individually as well. To crawl future assets To crawl assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. To grant permissions at a database level: Replace with the database you want to crawl in Atlan. (Repeat the statements for every database you want to integrate into Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) To mine query history for lineage To also mine Snowflake's query history (for lineage), add these permissions. You can use either option: To mine query history direct from Snowflake's internal tables: To mine query history from a cloned or copied set of tables, where you can also remove any sensitive data: Replace with the name of the cloned database, and with the name of the cloned schema containing account usage details. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to be able to preview and query in Atlan. (Repeat the statements for every database and schema you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. (Optional) To import Snowflake tags Snowflake stores all tag objects in the ACCOUNT_USAGE schema. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To import tags from Snowflake , grant these permissions: To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. (Optional) To push updated tags to Snowflake To push tags updated for assets in Atlan to Snowflake , grant these permissions: You can learn more about tag privileges from Snowflake documentation . (Optional) To crawl dynamic tables Atlan currently supports fetching metadata for dynamic tables using the MONITOR privilege. Refer to Snowflake documentation to learn more. To crawl existing dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: To crawl future dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) (Optional) To crawl Iceberg tables Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method. To crawl Iceberg tables from Snowflake, grant the following permissions: To crawl existing Iceberg tables in Snowflake: To crawl future Iceberg tables in Snowflake: To crawl Iceberg catalog metadata for Iceberg tables in Snowflake: You must first grant permissions to crawl existing Iceberg tables for this permission to work on catalogs. You must also grant permissions to all the catalogs you want to crawl in Atlan individually. (Optional) To crawl Snowflake stages Atlan supports crawling metadata for Snowflake stages using the USAGE and READ privileges. For more information, see the Snowflake documentation for INFORMATION_SCHEMA.STAGES. To crawl stages from Snowflake: Grant USAGE and READ privileges on all existing stages at the database level: Replace with the name of your Snowflake database Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Replace with the role you've granted Atlan to use for crawling. Grant USAGE and READ privileges on all future stages at the database level: Grant USAGE and READ privileges on all future stages at the database level: Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Allowlist the Atlan IP If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support ticket from within Atlan, or submit a request . (If you aren't using the IP allowlist in your Snowflake instance, you can skip this step.) Create user and role in Snowflake Choose metadata fetching method Grant permissions for account usage method Grant permissions for information schema method Allowlist the Atlan IP"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/snowflake\/how-tos\/set-up-snowflake#optional-to-preview-and-query-existing-assets-1","title":"Set up Snowflake | Atlan Documentation","text":"You need your Snowflake administrator to run these commands - you may not have access yourself. Create user and role in Snowflake Create a role and user in Snowflake using the following commands: Create role Create a role in Snowflake using the following commands: Replace with the default warehouse to use when running the Snowflake crawler . Atlan requires the following privileges to: OPERATE enables Atlan to start the virtual warehouse to fetch metadata if the warehouse has stopped. USAGE enables Atlan to show or list metadata from Snowflake. This in turn enables the Snowflake crawler to run the SHOW query. Create a user Create a separate user to integrate into Atlan, using one of the following 3 options: See Snowflake's official guide for details on generating an RSA key-pair . To create a user with a key-pair, replace the value for rsa_public_key with the public key and run the following: Learn more about the SERVICE type property in Snowflake documentation . Atlan only supports encrypted private keys with a non-empty passphrase - generally recommended as more secure. An empty passphrase results in workflow failures. To generate an encrypted private key, omit the -nocrypt option. Refer to Snowflake documentation to learn more. Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. To create a user with a password, replace and run the following: Learn more about the LEGACY_SERVICE type property in Snowflake documentation . This method is currently only available if Okta is your IdP (Snowflake supports) authenticating natively through Okta : Create a user in your identity provider (IdP) and use federated authentication in Snowflake . The password for this user must be maintained solely in the IdP and multi-factor authentication (MFA) must be disabled. Grant role to user To grant the atlan_user_role to the new user: Configure OAuth (client credentials flow) with Microsoft Entra ID To configure OAuth authentication using Microsoft Entra ID (formerly Azure AD) with the client credentials flow: Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions In Snowflake, create a security integration using the following: CREATE SECURITY INTEGRATION external_oauth_azure_ad TYPE = external_oauth ENABLED = true EXTERNAL_OAUTH_TYPE = azure EXTERNAL_OAUTH_ISSUER = '\\ ' EXTERNAL_OAUTH_JWS_KEYS_URL = '\\ ' EXTERNAL_OAUTH_AUDIENCE_LIST = ( '\\ ' ) EXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM = 'sub' EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE = 'login_name' ; Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app In Snowflake, create a security integration using the following: Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: CREATE USER oauth_svc_user WITH LOGIN_NAME = '\\ ' -- Use Azure client OBJECT ID DEFAULT_ROLE = \\ DEFAULT_WAREHOUSE = \\ ; Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: Grant the configured role to this user: GRANT ROLE \\ TO USER oauth_svc_user ; Grant the configured role to this user: Choose metadata fetching method Atlan supports two methods for fetching metadata from Snowflake - account usage and information schema. You should choose one of these two methods to set up Snowflake: Grant permissions for account usage method If you want to set warehouse timeouts when using this method, set a large value initially for the workflow to succeed. Once the workflow has succeeded, adjust the value to be twice the extraction time. This method uses the views in SNOWFLAKE.ACCOUNT_USAGE (or a copied version of this schema) to fetch the metadata from Snowflake into Atlan. You can be more granular with permissions using this method, but there are limitations with this approach . To crawl assets, generate lineage, and import tags If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Snowflake stores all tag objects in the ACCOUNT_USAGE schema. If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner , you need to grant the same permissions to import tags as required for crawling Snowflake assets. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. To crawl streams To crawl streams, provide the following permissions: To crawl current streams: Replace with the Snowflake database name. Replace with the Snowflake database name. To crawl future streams: To crawl future streams: Replace with the Snowflake database name. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. Grant permissions for information schema method This method uses views in the INFORMATION_SCHEMA schema in Snowflake databases to fetch metadata. You still need to grant specific permissions to enable Atlan to crawl metadata, preview data, and query data with this method. To crawl existing assets Grant these permissions to crawl assets that already exist in Snowflake. If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Grant permissions to crawl existing assets: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) Grant permissions to crawl functions: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) For secure user-defined functions (UDFs), grant OWNERSHIP permissions to retrieve metadata: Replace the placeholders with the appropriate values: : The name of the schema that contains the user-defined function (UDF). : The name of the secure UDF that requires ownership permissions. : The role that gets assigned ownership of the secure UDF. The statements given on this page apply to all schemas, tables, and views in a database in Snowflake. If you want to limit access to only certain objects, you can instead specify the exact objects individually as well. To crawl future assets To crawl assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. To grant permissions at a database level: Replace with the database you want to crawl in Atlan. (Repeat the statements for every database you want to integrate into Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) To mine query history for lineage To also mine Snowflake's query history (for lineage), add these permissions. You can use either option: To mine query history direct from Snowflake's internal tables: To mine query history from a cloned or copied set of tables, where you can also remove any sensitive data: Replace with the name of the cloned database, and with the name of the cloned schema containing account usage details. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to be able to preview and query in Atlan. (Repeat the statements for every database and schema you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. (Optional) To import Snowflake tags Snowflake stores all tag objects in the ACCOUNT_USAGE schema. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To import tags from Snowflake , grant these permissions: To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. (Optional) To push updated tags to Snowflake To push tags updated for assets in Atlan to Snowflake , grant these permissions: You can learn more about tag privileges from Snowflake documentation . (Optional) To crawl dynamic tables Atlan currently supports fetching metadata for dynamic tables using the MONITOR privilege. Refer to Snowflake documentation to learn more. To crawl existing dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: To crawl future dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) (Optional) To crawl Iceberg tables Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method. To crawl Iceberg tables from Snowflake, grant the following permissions: To crawl existing Iceberg tables in Snowflake: To crawl future Iceberg tables in Snowflake: To crawl Iceberg catalog metadata for Iceberg tables in Snowflake: You must first grant permissions to crawl existing Iceberg tables for this permission to work on catalogs. You must also grant permissions to all the catalogs you want to crawl in Atlan individually. (Optional) To crawl Snowflake stages Atlan supports crawling metadata for Snowflake stages using the USAGE and READ privileges. For more information, see the Snowflake documentation for INFORMATION_SCHEMA.STAGES. To crawl stages from Snowflake: Grant USAGE and READ privileges on all existing stages at the database level: Replace with the name of your Snowflake database Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Replace with the role you've granted Atlan to use for crawling. Grant USAGE and READ privileges on all future stages at the database level: Grant USAGE and READ privileges on all future stages at the database level: Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Allowlist the Atlan IP If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support ticket from within Atlan, or submit a request . (If you aren't using the IP allowlist in your Snowflake instance, you can skip this step.) Create user and role in Snowflake Choose metadata fetching method Grant permissions for account usage method Grant permissions for information schema method Allowlist the Atlan IP"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/snowflake\/how-tos\/set-up-snowflake#optional-to-preview-and-query-future-assets-1","title":"Set up Snowflake | Atlan Documentation","text":"You need your Snowflake administrator to run these commands - you may not have access yourself. Create user and role in Snowflake Create a role and user in Snowflake using the following commands: Create role Create a role in Snowflake using the following commands: Replace with the default warehouse to use when running the Snowflake crawler . Atlan requires the following privileges to: OPERATE enables Atlan to start the virtual warehouse to fetch metadata if the warehouse has stopped. USAGE enables Atlan to show or list metadata from Snowflake. This in turn enables the Snowflake crawler to run the SHOW query. Create a user Create a separate user to integrate into Atlan, using one of the following 3 options: See Snowflake's official guide for details on generating an RSA key-pair . To create a user with a key-pair, replace the value for rsa_public_key with the public key and run the following: Learn more about the SERVICE type property in Snowflake documentation . Atlan only supports encrypted private keys with a non-empty passphrase - generally recommended as more secure. An empty passphrase results in workflow failures. To generate an encrypted private key, omit the -nocrypt option. Refer to Snowflake documentation to learn more. Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. To create a user with a password, replace and run the following: Learn more about the LEGACY_SERVICE type property in Snowflake documentation . This method is currently only available if Okta is your IdP (Snowflake supports) authenticating natively through Okta : Create a user in your identity provider (IdP) and use federated authentication in Snowflake . The password for this user must be maintained solely in the IdP and multi-factor authentication (MFA) must be disabled. Grant role to user To grant the atlan_user_role to the new user: Configure OAuth (client credentials flow) with Microsoft Entra ID To configure OAuth authentication using Microsoft Entra ID (formerly Azure AD) with the client credentials flow: Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions In Snowflake, create a security integration using the following: CREATE SECURITY INTEGRATION external_oauth_azure_ad TYPE = external_oauth ENABLED = true EXTERNAL_OAUTH_TYPE = azure EXTERNAL_OAUTH_ISSUER = '\\ ' EXTERNAL_OAUTH_JWS_KEYS_URL = '\\ ' EXTERNAL_OAUTH_AUDIENCE_LIST = ( '\\ ' ) EXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM = 'sub' EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE = 'login_name' ; Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app In Snowflake, create a security integration using the following: Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: CREATE USER oauth_svc_user WITH LOGIN_NAME = '\\ ' -- Use Azure client OBJECT ID DEFAULT_ROLE = \\ DEFAULT_WAREHOUSE = \\ ; Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: Grant the configured role to this user: GRANT ROLE \\ TO USER oauth_svc_user ; Grant the configured role to this user: Choose metadata fetching method Atlan supports two methods for fetching metadata from Snowflake - account usage and information schema. You should choose one of these two methods to set up Snowflake: Grant permissions for account usage method If you want to set warehouse timeouts when using this method, set a large value initially for the workflow to succeed. Once the workflow has succeeded, adjust the value to be twice the extraction time. This method uses the views in SNOWFLAKE.ACCOUNT_USAGE (or a copied version of this schema) to fetch the metadata from Snowflake into Atlan. You can be more granular with permissions using this method, but there are limitations with this approach . To crawl assets, generate lineage, and import tags If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Snowflake stores all tag objects in the ACCOUNT_USAGE schema. If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner , you need to grant the same permissions to import tags as required for crawling Snowflake assets. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. To crawl streams To crawl streams, provide the following permissions: To crawl current streams: Replace with the Snowflake database name. Replace with the Snowflake database name. To crawl future streams: To crawl future streams: Replace with the Snowflake database name. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. Grant permissions for information schema method This method uses views in the INFORMATION_SCHEMA schema in Snowflake databases to fetch metadata. You still need to grant specific permissions to enable Atlan to crawl metadata, preview data, and query data with this method. To crawl existing assets Grant these permissions to crawl assets that already exist in Snowflake. If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Grant permissions to crawl existing assets: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) Grant permissions to crawl functions: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) For secure user-defined functions (UDFs), grant OWNERSHIP permissions to retrieve metadata: Replace the placeholders with the appropriate values: : The name of the schema that contains the user-defined function (UDF). : The name of the secure UDF that requires ownership permissions. : The role that gets assigned ownership of the secure UDF. The statements given on this page apply to all schemas, tables, and views in a database in Snowflake. If you want to limit access to only certain objects, you can instead specify the exact objects individually as well. To crawl future assets To crawl assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. To grant permissions at a database level: Replace with the database you want to crawl in Atlan. (Repeat the statements for every database you want to integrate into Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) To mine query history for lineage To also mine Snowflake's query history (for lineage), add these permissions. You can use either option: To mine query history direct from Snowflake's internal tables: To mine query history from a cloned or copied set of tables, where you can also remove any sensitive data: Replace with the name of the cloned database, and with the name of the cloned schema containing account usage details. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to be able to preview and query in Atlan. (Repeat the statements for every database and schema you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. (Optional) To import Snowflake tags Snowflake stores all tag objects in the ACCOUNT_USAGE schema. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To import tags from Snowflake , grant these permissions: To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. (Optional) To push updated tags to Snowflake To push tags updated for assets in Atlan to Snowflake , grant these permissions: You can learn more about tag privileges from Snowflake documentation . (Optional) To crawl dynamic tables Atlan currently supports fetching metadata for dynamic tables using the MONITOR privilege. Refer to Snowflake documentation to learn more. To crawl existing dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: To crawl future dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) (Optional) To crawl Iceberg tables Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method. To crawl Iceberg tables from Snowflake, grant the following permissions: To crawl existing Iceberg tables in Snowflake: To crawl future Iceberg tables in Snowflake: To crawl Iceberg catalog metadata for Iceberg tables in Snowflake: You must first grant permissions to crawl existing Iceberg tables for this permission to work on catalogs. You must also grant permissions to all the catalogs you want to crawl in Atlan individually. (Optional) To crawl Snowflake stages Atlan supports crawling metadata for Snowflake stages using the USAGE and READ privileges. For more information, see the Snowflake documentation for INFORMATION_SCHEMA.STAGES. To crawl stages from Snowflake: Grant USAGE and READ privileges on all existing stages at the database level: Replace with the name of your Snowflake database Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Replace with the role you've granted Atlan to use for crawling. Grant USAGE and READ privileges on all future stages at the database level: Grant USAGE and READ privileges on all future stages at the database level: Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Allowlist the Atlan IP If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support ticket from within Atlan, or submit a request . (If you aren't using the IP allowlist in your Snowflake instance, you can skip this step.) Create user and role in Snowflake Choose metadata fetching method Grant permissions for account usage method Grant permissions for information schema method Allowlist the Atlan IP"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/snowflake\/how-tos\/set-up-snowflake#optional-to-import-snowflake-tags","title":"Set up Snowflake | Atlan Documentation","text":"You need your Snowflake administrator to run these commands - you may not have access yourself. Create user and role in Snowflake Create a role and user in Snowflake using the following commands: Create role Create a role in Snowflake using the following commands: Replace with the default warehouse to use when running the Snowflake crawler . Atlan requires the following privileges to: OPERATE enables Atlan to start the virtual warehouse to fetch metadata if the warehouse has stopped. USAGE enables Atlan to show or list metadata from Snowflake. This in turn enables the Snowflake crawler to run the SHOW query. Create a user Create a separate user to integrate into Atlan, using one of the following 3 options: See Snowflake's official guide for details on generating an RSA key-pair . To create a user with a key-pair, replace the value for rsa_public_key with the public key and run the following: Learn more about the SERVICE type property in Snowflake documentation . Atlan only supports encrypted private keys with a non-empty passphrase - generally recommended as more secure. An empty passphrase results in workflow failures. To generate an encrypted private key, omit the -nocrypt option. Refer to Snowflake documentation to learn more. Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. To create a user with a password, replace and run the following: Learn more about the LEGACY_SERVICE type property in Snowflake documentation . This method is currently only available if Okta is your IdP (Snowflake supports) authenticating natively through Okta : Create a user in your identity provider (IdP) and use federated authentication in Snowflake . The password for this user must be maintained solely in the IdP and multi-factor authentication (MFA) must be disabled. Grant role to user To grant the atlan_user_role to the new user: Configure OAuth (client credentials flow) with Microsoft Entra ID To configure OAuth authentication using Microsoft Entra ID (formerly Azure AD) with the client credentials flow: Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions In Snowflake, create a security integration using the following: CREATE SECURITY INTEGRATION external_oauth_azure_ad TYPE = external_oauth ENABLED = true EXTERNAL_OAUTH_TYPE = azure EXTERNAL_OAUTH_ISSUER = '\\ ' EXTERNAL_OAUTH_JWS_KEYS_URL = '\\ ' EXTERNAL_OAUTH_AUDIENCE_LIST = ( '\\ ' ) EXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM = 'sub' EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE = 'login_name' ; Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app In Snowflake, create a security integration using the following: Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: CREATE USER oauth_svc_user WITH LOGIN_NAME = '\\ ' -- Use Azure client OBJECT ID DEFAULT_ROLE = \\ DEFAULT_WAREHOUSE = \\ ; Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: Grant the configured role to this user: GRANT ROLE \\ TO USER oauth_svc_user ; Grant the configured role to this user: Choose metadata fetching method Atlan supports two methods for fetching metadata from Snowflake - account usage and information schema. You should choose one of these two methods to set up Snowflake: Grant permissions for account usage method If you want to set warehouse timeouts when using this method, set a large value initially for the workflow to succeed. Once the workflow has succeeded, adjust the value to be twice the extraction time. This method uses the views in SNOWFLAKE.ACCOUNT_USAGE (or a copied version of this schema) to fetch the metadata from Snowflake into Atlan. You can be more granular with permissions using this method, but there are limitations with this approach . To crawl assets, generate lineage, and import tags If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Snowflake stores all tag objects in the ACCOUNT_USAGE schema. If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner , you need to grant the same permissions to import tags as required for crawling Snowflake assets. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. To crawl streams To crawl streams, provide the following permissions: To crawl current streams: Replace with the Snowflake database name. Replace with the Snowflake database name. To crawl future streams: To crawl future streams: Replace with the Snowflake database name. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. Grant permissions for information schema method This method uses views in the INFORMATION_SCHEMA schema in Snowflake databases to fetch metadata. You still need to grant specific permissions to enable Atlan to crawl metadata, preview data, and query data with this method. To crawl existing assets Grant these permissions to crawl assets that already exist in Snowflake. If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Grant permissions to crawl existing assets: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) Grant permissions to crawl functions: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) For secure user-defined functions (UDFs), grant OWNERSHIP permissions to retrieve metadata: Replace the placeholders with the appropriate values: : The name of the schema that contains the user-defined function (UDF). : The name of the secure UDF that requires ownership permissions. : The role that gets assigned ownership of the secure UDF. The statements given on this page apply to all schemas, tables, and views in a database in Snowflake. If you want to limit access to only certain objects, you can instead specify the exact objects individually as well. To crawl future assets To crawl assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. To grant permissions at a database level: Replace with the database you want to crawl in Atlan. (Repeat the statements for every database you want to integrate into Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) To mine query history for lineage To also mine Snowflake's query history (for lineage), add these permissions. You can use either option: To mine query history direct from Snowflake's internal tables: To mine query history from a cloned or copied set of tables, where you can also remove any sensitive data: Replace with the name of the cloned database, and with the name of the cloned schema containing account usage details. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to be able to preview and query in Atlan. (Repeat the statements for every database and schema you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. (Optional) To import Snowflake tags Snowflake stores all tag objects in the ACCOUNT_USAGE schema. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To import tags from Snowflake , grant these permissions: To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. (Optional) To push updated tags to Snowflake To push tags updated for assets in Atlan to Snowflake , grant these permissions: You can learn more about tag privileges from Snowflake documentation . (Optional) To crawl dynamic tables Atlan currently supports fetching metadata for dynamic tables using the MONITOR privilege. Refer to Snowflake documentation to learn more. To crawl existing dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: To crawl future dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) (Optional) To crawl Iceberg tables Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method. To crawl Iceberg tables from Snowflake, grant the following permissions: To crawl existing Iceberg tables in Snowflake: To crawl future Iceberg tables in Snowflake: To crawl Iceberg catalog metadata for Iceberg tables in Snowflake: You must first grant permissions to crawl existing Iceberg tables for this permission to work on catalogs. You must also grant permissions to all the catalogs you want to crawl in Atlan individually. (Optional) To crawl Snowflake stages Atlan supports crawling metadata for Snowflake stages using the USAGE and READ privileges. For more information, see the Snowflake documentation for INFORMATION_SCHEMA.STAGES. To crawl stages from Snowflake: Grant USAGE and READ privileges on all existing stages at the database level: Replace with the name of your Snowflake database Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Replace with the role you've granted Atlan to use for crawling. Grant USAGE and READ privileges on all future stages at the database level: Grant USAGE and READ privileges on all future stages at the database level: Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Allowlist the Atlan IP If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support ticket from within Atlan, or submit a request . (If you aren't using the IP allowlist in your Snowflake instance, you can skip this step.) Create user and role in Snowflake Choose metadata fetching method Grant permissions for account usage method Grant permissions for information schema method Allowlist the Atlan IP"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/snowflake\/how-tos\/set-up-snowflake#optional-to-push-updated-tags-to-snowflake","title":"Set up Snowflake | Atlan Documentation","text":"You need your Snowflake administrator to run these commands - you may not have access yourself. Create user and role in Snowflake Create a role and user in Snowflake using the following commands: Create role Create a role in Snowflake using the following commands: Replace with the default warehouse to use when running the Snowflake crawler . Atlan requires the following privileges to: OPERATE enables Atlan to start the virtual warehouse to fetch metadata if the warehouse has stopped. USAGE enables Atlan to show or list metadata from Snowflake. This in turn enables the Snowflake crawler to run the SHOW query. Create a user Create a separate user to integrate into Atlan, using one of the following 3 options: See Snowflake's official guide for details on generating an RSA key-pair . To create a user with a key-pair, replace the value for rsa_public_key with the public key and run the following: Learn more about the SERVICE type property in Snowflake documentation . Atlan only supports encrypted private keys with a non-empty passphrase - generally recommended as more secure. An empty passphrase results in workflow failures. To generate an encrypted private key, omit the -nocrypt option. Refer to Snowflake documentation to learn more. Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. To create a user with a password, replace and run the following: Learn more about the LEGACY_SERVICE type property in Snowflake documentation . This method is currently only available if Okta is your IdP (Snowflake supports) authenticating natively through Okta : Create a user in your identity provider (IdP) and use federated authentication in Snowflake . The password for this user must be maintained solely in the IdP and multi-factor authentication (MFA) must be disabled. Grant role to user To grant the atlan_user_role to the new user: Configure OAuth (client credentials flow) with Microsoft Entra ID To configure OAuth authentication using Microsoft Entra ID (formerly Azure AD) with the client credentials flow: Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions In Snowflake, create a security integration using the following: CREATE SECURITY INTEGRATION external_oauth_azure_ad TYPE = external_oauth ENABLED = true EXTERNAL_OAUTH_TYPE = azure EXTERNAL_OAUTH_ISSUER = '\\ ' EXTERNAL_OAUTH_JWS_KEYS_URL = '\\ ' EXTERNAL_OAUTH_AUDIENCE_LIST = ( '\\ ' ) EXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM = 'sub' EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE = 'login_name' ; Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app In Snowflake, create a security integration using the following: Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: CREATE USER oauth_svc_user WITH LOGIN_NAME = '\\ ' -- Use Azure client OBJECT ID DEFAULT_ROLE = \\ DEFAULT_WAREHOUSE = \\ ; Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: Grant the configured role to this user: GRANT ROLE \\ TO USER oauth_svc_user ; Grant the configured role to this user: Choose metadata fetching method Atlan supports two methods for fetching metadata from Snowflake - account usage and information schema. You should choose one of these two methods to set up Snowflake: Grant permissions for account usage method If you want to set warehouse timeouts when using this method, set a large value initially for the workflow to succeed. Once the workflow has succeeded, adjust the value to be twice the extraction time. This method uses the views in SNOWFLAKE.ACCOUNT_USAGE (or a copied version of this schema) to fetch the metadata from Snowflake into Atlan. You can be more granular with permissions using this method, but there are limitations with this approach . To crawl assets, generate lineage, and import tags If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Snowflake stores all tag objects in the ACCOUNT_USAGE schema. If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner , you need to grant the same permissions to import tags as required for crawling Snowflake assets. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. To crawl streams To crawl streams, provide the following permissions: To crawl current streams: Replace with the Snowflake database name. Replace with the Snowflake database name. To crawl future streams: To crawl future streams: Replace with the Snowflake database name. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. Grant permissions for information schema method This method uses views in the INFORMATION_SCHEMA schema in Snowflake databases to fetch metadata. You still need to grant specific permissions to enable Atlan to crawl metadata, preview data, and query data with this method. To crawl existing assets Grant these permissions to crawl assets that already exist in Snowflake. If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Grant permissions to crawl existing assets: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) Grant permissions to crawl functions: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) For secure user-defined functions (UDFs), grant OWNERSHIP permissions to retrieve metadata: Replace the placeholders with the appropriate values: : The name of the schema that contains the user-defined function (UDF). : The name of the secure UDF that requires ownership permissions. : The role that gets assigned ownership of the secure UDF. The statements given on this page apply to all schemas, tables, and views in a database in Snowflake. If you want to limit access to only certain objects, you can instead specify the exact objects individually as well. To crawl future assets To crawl assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. To grant permissions at a database level: Replace with the database you want to crawl in Atlan. (Repeat the statements for every database you want to integrate into Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) To mine query history for lineage To also mine Snowflake's query history (for lineage), add these permissions. You can use either option: To mine query history direct from Snowflake's internal tables: To mine query history from a cloned or copied set of tables, where you can also remove any sensitive data: Replace with the name of the cloned database, and with the name of the cloned schema containing account usage details. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to be able to preview and query in Atlan. (Repeat the statements for every database and schema you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. (Optional) To import Snowflake tags Snowflake stores all tag objects in the ACCOUNT_USAGE schema. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To import tags from Snowflake , grant these permissions: To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. (Optional) To push updated tags to Snowflake To push tags updated for assets in Atlan to Snowflake , grant these permissions: You can learn more about tag privileges from Snowflake documentation . (Optional) To crawl dynamic tables Atlan currently supports fetching metadata for dynamic tables using the MONITOR privilege. Refer to Snowflake documentation to learn more. To crawl existing dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: To crawl future dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) (Optional) To crawl Iceberg tables Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method. To crawl Iceberg tables from Snowflake, grant the following permissions: To crawl existing Iceberg tables in Snowflake: To crawl future Iceberg tables in Snowflake: To crawl Iceberg catalog metadata for Iceberg tables in Snowflake: You must first grant permissions to crawl existing Iceberg tables for this permission to work on catalogs. You must also grant permissions to all the catalogs you want to crawl in Atlan individually. (Optional) To crawl Snowflake stages Atlan supports crawling metadata for Snowflake stages using the USAGE and READ privileges. For more information, see the Snowflake documentation for INFORMATION_SCHEMA.STAGES. To crawl stages from Snowflake: Grant USAGE and READ privileges on all existing stages at the database level: Replace with the name of your Snowflake database Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Replace with the role you've granted Atlan to use for crawling. Grant USAGE and READ privileges on all future stages at the database level: Grant USAGE and READ privileges on all future stages at the database level: Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Allowlist the Atlan IP If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support ticket from within Atlan, or submit a request . (If you aren't using the IP allowlist in your Snowflake instance, you can skip this step.) Create user and role in Snowflake Choose metadata fetching method Grant permissions for account usage method Grant permissions for information schema method Allowlist the Atlan IP"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/snowflake\/how-tos\/set-up-snowflake#optional-to-crawl-dynamic-tables","title":"Set up Snowflake | Atlan Documentation","text":"You need your Snowflake administrator to run these commands - you may not have access yourself. Create user and role in Snowflake Create a role and user in Snowflake using the following commands: Create role Create a role in Snowflake using the following commands: Replace with the default warehouse to use when running the Snowflake crawler . Atlan requires the following privileges to: OPERATE enables Atlan to start the virtual warehouse to fetch metadata if the warehouse has stopped. USAGE enables Atlan to show or list metadata from Snowflake. This in turn enables the Snowflake crawler to run the SHOW query. Create a user Create a separate user to integrate into Atlan, using one of the following 3 options: See Snowflake's official guide for details on generating an RSA key-pair . To create a user with a key-pair, replace the value for rsa_public_key with the public key and run the following: Learn more about the SERVICE type property in Snowflake documentation . Atlan only supports encrypted private keys with a non-empty passphrase - generally recommended as more secure. An empty passphrase results in workflow failures. To generate an encrypted private key, omit the -nocrypt option. Refer to Snowflake documentation to learn more. Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. To create a user with a password, replace and run the following: Learn more about the LEGACY_SERVICE type property in Snowflake documentation . This method is currently only available if Okta is your IdP (Snowflake supports) authenticating natively through Okta : Create a user in your identity provider (IdP) and use federated authentication in Snowflake . The password for this user must be maintained solely in the IdP and multi-factor authentication (MFA) must be disabled. Grant role to user To grant the atlan_user_role to the new user: Configure OAuth (client credentials flow) with Microsoft Entra ID To configure OAuth authentication using Microsoft Entra ID (formerly Azure AD) with the client credentials flow: Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions In Snowflake, create a security integration using the following: CREATE SECURITY INTEGRATION external_oauth_azure_ad TYPE = external_oauth ENABLED = true EXTERNAL_OAUTH_TYPE = azure EXTERNAL_OAUTH_ISSUER = '\\ ' EXTERNAL_OAUTH_JWS_KEYS_URL = '\\ ' EXTERNAL_OAUTH_AUDIENCE_LIST = ( '\\ ' ) EXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM = 'sub' EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE = 'login_name' ; Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app In Snowflake, create a security integration using the following: Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: CREATE USER oauth_svc_user WITH LOGIN_NAME = '\\ ' -- Use Azure client OBJECT ID DEFAULT_ROLE = \\ DEFAULT_WAREHOUSE = \\ ; Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: Grant the configured role to this user: GRANT ROLE \\ TO USER oauth_svc_user ; Grant the configured role to this user: Choose metadata fetching method Atlan supports two methods for fetching metadata from Snowflake - account usage and information schema. You should choose one of these two methods to set up Snowflake: Grant permissions for account usage method If you want to set warehouse timeouts when using this method, set a large value initially for the workflow to succeed. Once the workflow has succeeded, adjust the value to be twice the extraction time. This method uses the views in SNOWFLAKE.ACCOUNT_USAGE (or a copied version of this schema) to fetch the metadata from Snowflake into Atlan. You can be more granular with permissions using this method, but there are limitations with this approach . To crawl assets, generate lineage, and import tags If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Snowflake stores all tag objects in the ACCOUNT_USAGE schema. If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner , you need to grant the same permissions to import tags as required for crawling Snowflake assets. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. To crawl streams To crawl streams, provide the following permissions: To crawl current streams: Replace with the Snowflake database name. Replace with the Snowflake database name. To crawl future streams: To crawl future streams: Replace with the Snowflake database name. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. Grant permissions for information schema method This method uses views in the INFORMATION_SCHEMA schema in Snowflake databases to fetch metadata. You still need to grant specific permissions to enable Atlan to crawl metadata, preview data, and query data with this method. To crawl existing assets Grant these permissions to crawl assets that already exist in Snowflake. If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Grant permissions to crawl existing assets: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) Grant permissions to crawl functions: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) For secure user-defined functions (UDFs), grant OWNERSHIP permissions to retrieve metadata: Replace the placeholders with the appropriate values: : The name of the schema that contains the user-defined function (UDF). : The name of the secure UDF that requires ownership permissions. : The role that gets assigned ownership of the secure UDF. The statements given on this page apply to all schemas, tables, and views in a database in Snowflake. If you want to limit access to only certain objects, you can instead specify the exact objects individually as well. To crawl future assets To crawl assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. To grant permissions at a database level: Replace with the database you want to crawl in Atlan. (Repeat the statements for every database you want to integrate into Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) To mine query history for lineage To also mine Snowflake's query history (for lineage), add these permissions. You can use either option: To mine query history direct from Snowflake's internal tables: To mine query history from a cloned or copied set of tables, where you can also remove any sensitive data: Replace with the name of the cloned database, and with the name of the cloned schema containing account usage details. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to be able to preview and query in Atlan. (Repeat the statements for every database and schema you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. (Optional) To import Snowflake tags Snowflake stores all tag objects in the ACCOUNT_USAGE schema. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To import tags from Snowflake , grant these permissions: To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. (Optional) To push updated tags to Snowflake To push tags updated for assets in Atlan to Snowflake , grant these permissions: You can learn more about tag privileges from Snowflake documentation . (Optional) To crawl dynamic tables Atlan currently supports fetching metadata for dynamic tables using the MONITOR privilege. Refer to Snowflake documentation to learn more. To crawl existing dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: To crawl future dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) (Optional) To crawl Iceberg tables Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method. To crawl Iceberg tables from Snowflake, grant the following permissions: To crawl existing Iceberg tables in Snowflake: To crawl future Iceberg tables in Snowflake: To crawl Iceberg catalog metadata for Iceberg tables in Snowflake: You must first grant permissions to crawl existing Iceberg tables for this permission to work on catalogs. You must also grant permissions to all the catalogs you want to crawl in Atlan individually. (Optional) To crawl Snowflake stages Atlan supports crawling metadata for Snowflake stages using the USAGE and READ privileges. For more information, see the Snowflake documentation for INFORMATION_SCHEMA.STAGES. To crawl stages from Snowflake: Grant USAGE and READ privileges on all existing stages at the database level: Replace with the name of your Snowflake database Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Replace with the role you've granted Atlan to use for crawling. Grant USAGE and READ privileges on all future stages at the database level: Grant USAGE and READ privileges on all future stages at the database level: Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Allowlist the Atlan IP If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support ticket from within Atlan, or submit a request . (If you aren't using the IP allowlist in your Snowflake instance, you can skip this step.) Create user and role in Snowflake Choose metadata fetching method Grant permissions for account usage method Grant permissions for information schema method Allowlist the Atlan IP"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/snowflake\/how-tos\/set-up-snowflake#optional-to-crawl-iceberg-tables","title":"Set up Snowflake | Atlan Documentation","text":"You need your Snowflake administrator to run these commands - you may not have access yourself. Create user and role in Snowflake Create a role and user in Snowflake using the following commands: Create role Create a role in Snowflake using the following commands: Replace with the default warehouse to use when running the Snowflake crawler . Atlan requires the following privileges to: OPERATE enables Atlan to start the virtual warehouse to fetch metadata if the warehouse has stopped. USAGE enables Atlan to show or list metadata from Snowflake. This in turn enables the Snowflake crawler to run the SHOW query. Create a user Create a separate user to integrate into Atlan, using one of the following 3 options: See Snowflake's official guide for details on generating an RSA key-pair . To create a user with a key-pair, replace the value for rsa_public_key with the public key and run the following: Learn more about the SERVICE type property in Snowflake documentation . Atlan only supports encrypted private keys with a non-empty passphrase - generally recommended as more secure. An empty passphrase results in workflow failures. To generate an encrypted private key, omit the -nocrypt option. Refer to Snowflake documentation to learn more. Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. To create a user with a password, replace and run the following: Learn more about the LEGACY_SERVICE type property in Snowflake documentation . This method is currently only available if Okta is your IdP (Snowflake supports) authenticating natively through Okta : Create a user in your identity provider (IdP) and use federated authentication in Snowflake . The password for this user must be maintained solely in the IdP and multi-factor authentication (MFA) must be disabled. Grant role to user To grant the atlan_user_role to the new user: Configure OAuth (client credentials flow) with Microsoft Entra ID To configure OAuth authentication using Microsoft Entra ID (formerly Azure AD) with the client credentials flow: Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions In Snowflake, create a security integration using the following: CREATE SECURITY INTEGRATION external_oauth_azure_ad TYPE = external_oauth ENABLED = true EXTERNAL_OAUTH_TYPE = azure EXTERNAL_OAUTH_ISSUER = '\\ ' EXTERNAL_OAUTH_JWS_KEYS_URL = '\\ ' EXTERNAL_OAUTH_AUDIENCE_LIST = ( '\\ ' ) EXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM = 'sub' EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE = 'login_name' ; Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app In Snowflake, create a security integration using the following: Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: CREATE USER oauth_svc_user WITH LOGIN_NAME = '\\ ' -- Use Azure client OBJECT ID DEFAULT_ROLE = \\ DEFAULT_WAREHOUSE = \\ ; Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: Grant the configured role to this user: GRANT ROLE \\ TO USER oauth_svc_user ; Grant the configured role to this user: Choose metadata fetching method Atlan supports two methods for fetching metadata from Snowflake - account usage and information schema. You should choose one of these two methods to set up Snowflake: Grant permissions for account usage method If you want to set warehouse timeouts when using this method, set a large value initially for the workflow to succeed. Once the workflow has succeeded, adjust the value to be twice the extraction time. This method uses the views in SNOWFLAKE.ACCOUNT_USAGE (or a copied version of this schema) to fetch the metadata from Snowflake into Atlan. You can be more granular with permissions using this method, but there are limitations with this approach . To crawl assets, generate lineage, and import tags If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Snowflake stores all tag objects in the ACCOUNT_USAGE schema. If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner , you need to grant the same permissions to import tags as required for crawling Snowflake assets. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. To crawl streams To crawl streams, provide the following permissions: To crawl current streams: Replace with the Snowflake database name. Replace with the Snowflake database name. To crawl future streams: To crawl future streams: Replace with the Snowflake database name. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. Grant permissions for information schema method This method uses views in the INFORMATION_SCHEMA schema in Snowflake databases to fetch metadata. You still need to grant specific permissions to enable Atlan to crawl metadata, preview data, and query data with this method. To crawl existing assets Grant these permissions to crawl assets that already exist in Snowflake. If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Grant permissions to crawl existing assets: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) Grant permissions to crawl functions: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) For secure user-defined functions (UDFs), grant OWNERSHIP permissions to retrieve metadata: Replace the placeholders with the appropriate values: : The name of the schema that contains the user-defined function (UDF). : The name of the secure UDF that requires ownership permissions. : The role that gets assigned ownership of the secure UDF. The statements given on this page apply to all schemas, tables, and views in a database in Snowflake. If you want to limit access to only certain objects, you can instead specify the exact objects individually as well. To crawl future assets To crawl assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. To grant permissions at a database level: Replace with the database you want to crawl in Atlan. (Repeat the statements for every database you want to integrate into Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) To mine query history for lineage To also mine Snowflake's query history (for lineage), add these permissions. You can use either option: To mine query history direct from Snowflake's internal tables: To mine query history from a cloned or copied set of tables, where you can also remove any sensitive data: Replace with the name of the cloned database, and with the name of the cloned schema containing account usage details. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to be able to preview and query in Atlan. (Repeat the statements for every database and schema you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. (Optional) To import Snowflake tags Snowflake stores all tag objects in the ACCOUNT_USAGE schema. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To import tags from Snowflake , grant these permissions: To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. (Optional) To push updated tags to Snowflake To push tags updated for assets in Atlan to Snowflake , grant these permissions: You can learn more about tag privileges from Snowflake documentation . (Optional) To crawl dynamic tables Atlan currently supports fetching metadata for dynamic tables using the MONITOR privilege. Refer to Snowflake documentation to learn more. To crawl existing dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: To crawl future dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) (Optional) To crawl Iceberg tables Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method. To crawl Iceberg tables from Snowflake, grant the following permissions: To crawl existing Iceberg tables in Snowflake: To crawl future Iceberg tables in Snowflake: To crawl Iceberg catalog metadata for Iceberg tables in Snowflake: You must first grant permissions to crawl existing Iceberg tables for this permission to work on catalogs. You must also grant permissions to all the catalogs you want to crawl in Atlan individually. (Optional) To crawl Snowflake stages Atlan supports crawling metadata for Snowflake stages using the USAGE and READ privileges. For more information, see the Snowflake documentation for INFORMATION_SCHEMA.STAGES. To crawl stages from Snowflake: Grant USAGE and READ privileges on all existing stages at the database level: Replace with the name of your Snowflake database Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Replace with the role you've granted Atlan to use for crawling. Grant USAGE and READ privileges on all future stages at the database level: Grant USAGE and READ privileges on all future stages at the database level: Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Allowlist the Atlan IP If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support ticket from within Atlan, or submit a request . (If you aren't using the IP allowlist in your Snowflake instance, you can skip this step.) Create user and role in Snowflake Choose metadata fetching method Grant permissions for account usage method Grant permissions for information schema method Allowlist the Atlan IP"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/snowflake\/how-tos\/set-up-snowflake#optional-to-crawl-snowflake-stages","title":"Set up Snowflake | Atlan Documentation","text":"You need your Snowflake administrator to run these commands - you may not have access yourself. Create user and role in Snowflake Create a role and user in Snowflake using the following commands: Create role Create a role in Snowflake using the following commands: Replace with the default warehouse to use when running the Snowflake crawler . Atlan requires the following privileges to: OPERATE enables Atlan to start the virtual warehouse to fetch metadata if the warehouse has stopped. USAGE enables Atlan to show or list metadata from Snowflake. This in turn enables the Snowflake crawler to run the SHOW query. Create a user Create a separate user to integrate into Atlan, using one of the following 3 options: See Snowflake's official guide for details on generating an RSA key-pair . To create a user with a key-pair, replace the value for rsa_public_key with the public key and run the following: Learn more about the SERVICE type property in Snowflake documentation . Atlan only supports encrypted private keys with a non-empty passphrase - generally recommended as more secure. An empty passphrase results in workflow failures. To generate an encrypted private key, omit the -nocrypt option. Refer to Snowflake documentation to learn more. Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. To create a user with a password, replace and run the following: Learn more about the LEGACY_SERVICE type property in Snowflake documentation . This method is currently only available if Okta is your IdP (Snowflake supports) authenticating natively through Okta : Create a user in your identity provider (IdP) and use federated authentication in Snowflake . The password for this user must be maintained solely in the IdP and multi-factor authentication (MFA) must be disabled. Grant role to user To grant the atlan_user_role to the new user: Configure OAuth (client credentials flow) with Microsoft Entra ID To configure OAuth authentication using Microsoft Entra ID (formerly Azure AD) with the client credentials flow: Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions In Snowflake, create a security integration using the following: CREATE SECURITY INTEGRATION external_oauth_azure_ad TYPE = external_oauth ENABLED = true EXTERNAL_OAUTH_TYPE = azure EXTERNAL_OAUTH_ISSUER = '\\ ' EXTERNAL_OAUTH_JWS_KEYS_URL = '\\ ' EXTERNAL_OAUTH_AUDIENCE_LIST = ( '\\ ' ) EXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM = 'sub' EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE = 'login_name' ; Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app In Snowflake, create a security integration using the following: Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: CREATE USER oauth_svc_user WITH LOGIN_NAME = '\\ ' -- Use Azure client OBJECT ID DEFAULT_ROLE = \\ DEFAULT_WAREHOUSE = \\ ; Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: Grant the configured role to this user: GRANT ROLE \\ TO USER oauth_svc_user ; Grant the configured role to this user: Choose metadata fetching method Atlan supports two methods for fetching metadata from Snowflake - account usage and information schema. You should choose one of these two methods to set up Snowflake: Grant permissions for account usage method If you want to set warehouse timeouts when using this method, set a large value initially for the workflow to succeed. Once the workflow has succeeded, adjust the value to be twice the extraction time. This method uses the views in SNOWFLAKE.ACCOUNT_USAGE (or a copied version of this schema) to fetch the metadata from Snowflake into Atlan. You can be more granular with permissions using this method, but there are limitations with this approach . To crawl assets, generate lineage, and import tags If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Snowflake stores all tag objects in the ACCOUNT_USAGE schema. If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner , you need to grant the same permissions to import tags as required for crawling Snowflake assets. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. To crawl streams To crawl streams, provide the following permissions: To crawl current streams: Replace with the Snowflake database name. Replace with the Snowflake database name. To crawl future streams: To crawl future streams: Replace with the Snowflake database name. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. Grant permissions for information schema method This method uses views in the INFORMATION_SCHEMA schema in Snowflake databases to fetch metadata. You still need to grant specific permissions to enable Atlan to crawl metadata, preview data, and query data with this method. To crawl existing assets Grant these permissions to crawl assets that already exist in Snowflake. If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Grant permissions to crawl existing assets: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) Grant permissions to crawl functions: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) For secure user-defined functions (UDFs), grant OWNERSHIP permissions to retrieve metadata: Replace the placeholders with the appropriate values: : The name of the schema that contains the user-defined function (UDF). : The name of the secure UDF that requires ownership permissions. : The role that gets assigned ownership of the secure UDF. The statements given on this page apply to all schemas, tables, and views in a database in Snowflake. If you want to limit access to only certain objects, you can instead specify the exact objects individually as well. To crawl future assets To crawl assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. To grant permissions at a database level: Replace with the database you want to crawl in Atlan. (Repeat the statements for every database you want to integrate into Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) To mine query history for lineage To also mine Snowflake's query history (for lineage), add these permissions. You can use either option: To mine query history direct from Snowflake's internal tables: To mine query history from a cloned or copied set of tables, where you can also remove any sensitive data: Replace with the name of the cloned database, and with the name of the cloned schema containing account usage details. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to be able to preview and query in Atlan. (Repeat the statements for every database and schema you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. (Optional) To import Snowflake tags Snowflake stores all tag objects in the ACCOUNT_USAGE schema. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To import tags from Snowflake , grant these permissions: To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. (Optional) To push updated tags to Snowflake To push tags updated for assets in Atlan to Snowflake , grant these permissions: You can learn more about tag privileges from Snowflake documentation . (Optional) To crawl dynamic tables Atlan currently supports fetching metadata for dynamic tables using the MONITOR privilege. Refer to Snowflake documentation to learn more. To crawl existing dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: To crawl future dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) (Optional) To crawl Iceberg tables Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method. To crawl Iceberg tables from Snowflake, grant the following permissions: To crawl existing Iceberg tables in Snowflake: To crawl future Iceberg tables in Snowflake: To crawl Iceberg catalog metadata for Iceberg tables in Snowflake: You must first grant permissions to crawl existing Iceberg tables for this permission to work on catalogs. You must also grant permissions to all the catalogs you want to crawl in Atlan individually. (Optional) To crawl Snowflake stages Atlan supports crawling metadata for Snowflake stages using the USAGE and READ privileges. For more information, see the Snowflake documentation for INFORMATION_SCHEMA.STAGES. To crawl stages from Snowflake: Grant USAGE and READ privileges on all existing stages at the database level: Replace with the name of your Snowflake database Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Replace with the role you've granted Atlan to use for crawling. Grant USAGE and READ privileges on all future stages at the database level: Grant USAGE and READ privileges on all future stages at the database level: Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Allowlist the Atlan IP If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support ticket from within Atlan, or submit a request . (If you aren't using the IP allowlist in your Snowflake instance, you can skip this step.) Create user and role in Snowflake Choose metadata fetching method Grant permissions for account usage method Grant permissions for information schema method Allowlist the Atlan IP"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/snowflake\/how-tos\/set-up-snowflake#allowlist-the-atlan-ip","title":"Set up Snowflake | Atlan Documentation","text":"You need your Snowflake administrator to run these commands - you may not have access yourself. Create user and role in Snowflake Create a role and user in Snowflake using the following commands: Create role Create a role in Snowflake using the following commands: Replace with the default warehouse to use when running the Snowflake crawler . Atlan requires the following privileges to: OPERATE enables Atlan to start the virtual warehouse to fetch metadata if the warehouse has stopped. USAGE enables Atlan to show or list metadata from Snowflake. This in turn enables the Snowflake crawler to run the SHOW query. Create a user Create a separate user to integrate into Atlan, using one of the following 3 options: See Snowflake's official guide for details on generating an RSA key-pair . To create a user with a key-pair, replace the value for rsa_public_key with the public key and run the following: Learn more about the SERVICE type property in Snowflake documentation . Atlan only supports encrypted private keys with a non-empty passphrase - generally recommended as more secure. An empty passphrase results in workflow failures. To generate an encrypted private key, omit the -nocrypt option. Refer to Snowflake documentation to learn more. Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. To create a user with a password, replace and run the following: Learn more about the LEGACY_SERVICE type property in Snowflake documentation . This method is currently only available if Okta is your IdP (Snowflake supports) authenticating natively through Okta : Create a user in your identity provider (IdP) and use federated authentication in Snowflake . The password for this user must be maintained solely in the IdP and multi-factor authentication (MFA) must be disabled. Grant role to user To grant the atlan_user_role to the new user: Configure OAuth (client credentials flow) with Microsoft Entra ID To configure OAuth authentication using Microsoft Entra ID (formerly Azure AD) with the client credentials flow: Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions In Snowflake, create a security integration using the following: CREATE SECURITY INTEGRATION external_oauth_azure_ad TYPE = external_oauth ENABLED = true EXTERNAL_OAUTH_TYPE = azure EXTERNAL_OAUTH_ISSUER = '\\ ' EXTERNAL_OAUTH_JWS_KEYS_URL = '\\ ' EXTERNAL_OAUTH_AUDIENCE_LIST = ( '\\ ' ) EXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM = 'sub' EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE = 'login_name' ; Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app In Snowflake, create a security integration using the following: Replace the placeholders with actual values from your Azure AD app: Your tenant's OAuth 2.0 issuer URL Azure JWKs URI Application ID URI of the Azure app Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: CREATE USER oauth_svc_user WITH LOGIN_NAME = '\\ ' -- Use Azure client OBJECT ID DEFAULT_ROLE = \\ DEFAULT_WAREHOUSE = \\ ; Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: Grant the configured role to this user: GRANT ROLE \\ TO USER oauth_svc_user ; Grant the configured role to this user: Choose metadata fetching method Atlan supports two methods for fetching metadata from Snowflake - account usage and information schema. You should choose one of these two methods to set up Snowflake: Grant permissions for account usage method If you want to set warehouse timeouts when using this method, set a large value initially for the workflow to succeed. Once the workflow has succeeded, adjust the value to be twice the extraction time. This method uses the views in SNOWFLAKE.ACCOUNT_USAGE (or a copied version of this schema) to fetch the metadata from Snowflake into Atlan. You can be more granular with permissions using this method, but there are limitations with this approach . To crawl assets, generate lineage, and import tags If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Snowflake stores all tag objects in the ACCOUNT_USAGE schema. If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner , you need to grant the same permissions to import tags as required for crawling Snowflake assets. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. To crawl streams To crawl streams, provide the following permissions: To crawl current streams: Replace with the Snowflake database name. Replace with the Snowflake database name. To crawl future streams: To crawl future streams: Replace with the Snowflake database name. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. Grant permissions for information schema method This method uses views in the INFORMATION_SCHEMA schema in Snowflake databases to fetch metadata. You still need to grant specific permissions to enable Atlan to crawl metadata, preview data, and query data with this method. To crawl existing assets Grant these permissions to crawl assets that already exist in Snowflake. If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Grant permissions to crawl existing assets: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) Grant permissions to crawl functions: Replace with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) For secure user-defined functions (UDFs), grant OWNERSHIP permissions to retrieve metadata: Replace the placeholders with the appropriate values: : The name of the schema that contains the user-defined function (UDF). : The name of the secure UDF that requires ownership permissions. : The role that gets assigned ownership of the secure UDF. The statements given on this page apply to all schemas, tables, and views in a database in Snowflake. If you want to limit access to only certain objects, you can instead specify the exact objects individually as well. To crawl future assets To crawl assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. To grant permissions at a database level: Replace with the database you want to crawl in Atlan. (Repeat the statements for every database you want to integrate into Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) To mine query history for lineage To also mine Snowflake's query history (for lineage), add these permissions. You can use either option: To mine query history direct from Snowflake's internal tables: To mine query history from a cloned or copied set of tables, where you can also remove any sensitive data: Replace with the name of the cloned database, and with the name of the cloned schema containing account usage details. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. (Optional) To preview and query existing assets To query and preview data within assets that already exist in Snowflake, add these permissions: Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. Replace with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: Replace with the database and with the schema you want to be able to preview and query in Atlan. (Repeat the statements for every database and schema you want to preview and query in Atlan.) Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. (Optional) To import Snowflake tags Snowflake stores all tag objects in the ACCOUNT_USAGE schema. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To import tags from Snowflake , grant these permissions: To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: Replace with the copied Snowflake database name. Replace with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. (Optional) To push updated tags to Snowflake To push tags updated for assets in Atlan to Snowflake , grant these permissions: You can learn more about tag privileges from Snowflake documentation . (Optional) To crawl dynamic tables Atlan currently supports fetching metadata for dynamic tables using the MONITOR privilege. Refer to Snowflake documentation to learn more. To crawl existing dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: To crawl future dynamic tables from Snowflake: Grant permissions at a database level: Grant permissions at a schema level: Replace with the database and with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) (Optional) To crawl Iceberg tables Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method. To crawl Iceberg tables from Snowflake, grant the following permissions: To crawl existing Iceberg tables in Snowflake: To crawl future Iceberg tables in Snowflake: To crawl Iceberg catalog metadata for Iceberg tables in Snowflake: You must first grant permissions to crawl existing Iceberg tables for this permission to work on catalogs. You must also grant permissions to all the catalogs you want to crawl in Atlan individually. (Optional) To crawl Snowflake stages Atlan supports crawling metadata for Snowflake stages using the USAGE and READ privileges. For more information, see the Snowflake documentation for INFORMATION_SCHEMA.STAGES. To crawl stages from Snowflake: Grant USAGE and READ privileges on all existing stages at the database level: Replace with the name of your Snowflake database Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Replace with the role you've granted Atlan to use for crawling. Grant USAGE and READ privileges on all future stages at the database level: Grant USAGE and READ privileges on all future stages at the database level: Replace with the name of your Snowflake database Replace with the role you've granted Atlan to use for crawling. Allowlist the Atlan IP If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support ticket from within Atlan, or submit a request . (If you aren't using the IP allowlist in your Snowflake instance, you can skip this step.) Create user and role in Snowflake Choose metadata fetching method Grant permissions for account usage method Grant permissions for information schema method Allowlist the Atlan IP"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-databricks#__docusaurus_skipToContent_fallback","title":"Set up Databricks | Atlan Documentation","text":"Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: SQL warehouse (formerly SQL endpoint) To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after . minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Create a service principal You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Once you've copied the client ID and secret, click Done . Azure service principal authentication You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Tenant ID (directory ID) Create a service principal To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : usage and popularity metrics Enable system.access schema You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage system.query.history (to mine query history for usage and popularity metrics) You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, ) USE SCHEMA and SELECT on the schema (for example, . ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. (Optional) Grant view permissions to access Databricks entities via APIs Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3 6 for each catalog you want to crawl in Atlan. SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-databricks#personal-access-token-authentication","title":"Set up Databricks | Atlan Documentation","text":"Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: SQL warehouse (formerly SQL endpoint) To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after . minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Create a service principal You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Once you've copied the client ID and secret, click Done . Azure service principal authentication You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Tenant ID (directory ID) Create a service principal To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : usage and popularity metrics Enable system.access schema You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage system.query.history (to mine query history for usage and popularity metrics) You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, ) USE SCHEMA and SELECT on the schema (for example, . ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. (Optional) Grant view permissions to access Databricks entities via APIs Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3 6 for each catalog you want to crawl in Atlan. SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-databricks#aws-service-principal-authentication","title":"Set up Databricks | Atlan Documentation","text":"Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: SQL warehouse (formerly SQL endpoint) To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after . minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Create a service principal You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Once you've copied the client ID and secret, click Done . Azure service principal authentication You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Tenant ID (directory ID) Create a service principal To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : usage and popularity metrics Enable system.access schema You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage system.query.history (to mine query history for usage and popularity metrics) You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, ) USE SCHEMA and SELECT on the schema (for example, . ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. (Optional) Grant view permissions to access Databricks entities via APIs Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3 6 for each catalog you want to crawl in Atlan. SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-databricks#azure-service-principal-authentication","title":"Set up Databricks | Atlan Documentation","text":"Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: SQL warehouse (formerly SQL endpoint) To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after . minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Create a service principal You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Once you've copied the client ID and secret, click Done . Azure service principal authentication You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Tenant ID (directory ID) Create a service principal To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : usage and popularity metrics Enable system.access schema You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage system.query.history (to mine query history for usage and popularity metrics) You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, ) USE SCHEMA and SELECT on the schema (for example, . ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. (Optional) Grant view permissions to access Databricks entities via APIs Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3 6 for each catalog you want to crawl in Atlan. SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-databricks#grant-user-access-to-workspace","title":"Set up Databricks | Atlan Documentation","text":"Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: SQL warehouse (formerly SQL endpoint) To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after . minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Create a service principal You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Once you've copied the client ID and secret, click Done . Azure service principal authentication You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Tenant ID (directory ID) Create a service principal To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : usage and popularity metrics Enable system.access schema You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage system.query.history (to mine query history for usage and popularity metrics) You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, ) USE SCHEMA and SELECT on the schema (for example, . ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. (Optional) Grant view permissions to access Databricks entities via APIs Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3 6 for each catalog you want to crawl in Atlan. SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-databricks#generate-a-personal-access-token","title":"Set up Databricks | Atlan Documentation","text":"Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: SQL warehouse (formerly SQL endpoint) To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after . minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Create a service principal You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Once you've copied the client ID and secret, click Done . Azure service principal authentication You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Tenant ID (directory ID) Create a service principal To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : usage and popularity metrics Enable system.access schema You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage system.query.history (to mine query history for usage and popularity metrics) You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, ) USE SCHEMA and SELECT on the schema (for example, . ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. (Optional) Grant view permissions to access Databricks entities via APIs Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3 6 for each catalog you want to crawl in Atlan. SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/crawl-databricks","title":"Crawl Databricks | Atlan Documentation","text":"Once you have configured the Databricks access permissions , you can establish a connection between Atlan and your Databricks instance. (If you are also using AWS PrivateLink or Azure Private Link for Databricks, you will need to set that up first, too.) To crawl metadata from your Databricks instance, review the order of operations and then complete the following steps. Select the source To select Databricks as your source: In the top right corner of any screen, navigate to New and then click New Workflow . In the top right corner of any screen, navigate to New and then click New Workflow . From the list of packages, select Databricks Assets , and click Setup Workflow . From the list of packages, select Databricks Assets , and click Setup Workflow . Provide credentials Choose your extraction method: In Direct extraction, Atlan connects to your database and crawls metadata directly. Next, select an authentication method: In JDBC , you will need a personal access token and HTTP path for authentication . In AWS Service , you will need a client ID and client secret for AWS service principal authentication . In Azure Service , you will need a tenant ID, client ID, and client secret for Azure service principal authentication . In JDBC , you will need a personal access token and HTTP path for authentication . In AWS Service , you will need a client ID and client secret for AWS service principal authentication . In Azure Service , you will need a tenant ID, client ID, and client secret for Azure service principal authentication . In Offline extraction, you will need to first extract metadata yourself and make it available in S3 . In Agent extraction, Atlan's secure agent executes metadata extraction within the organization's environment. Direct extraction method To enter your Databricks credentials: For Host , enter the hostname, AWS PrivateLink endpoint , or Azure Private Link endpoint for your Databricks instance. For Port , enter the port number of your Databricks instance. For Personal Access Token , enter the access token you generated when setting up access . For HTTP Path , enter one of the following: A path starting with \/sql\/1.0\/warehouses to use the Databricks SQL warehouse . A path starting with sql\/protocolv1\/o to use the Databricks interactive cluster . A path starting with \/sql\/1.0\/warehouses to use the Databricks SQL warehouse . A path starting with sql\/protocolv1\/o to use the Databricks interactive cluster . Click Test Authentication to confirm connectivity to Databricks using these details. Once successful, at the bottom of the screen click Next . Make sure your Databricks instance (SQL warehouse or interactive cluster) is up and running, otherwise the Test Authentication step times out. To enter your Databricks credentials: For Host , enter the hostname or AWS PrivateLink endpoint for your Databricks instance. For Port , enter the port number of your Databricks instance. For Client ID , enter the client ID for your AWS service principal . For Client Secret , enter the client secret for your AWS service principal . Click Test Authentication to confirm connectivity to Databricks using these details. Once successful, at the bottom of the screen click Next . To enter your Databricks credentials: For Host , enter the hostname or Azure Private Link endpoint for your Databricks instance. For Port , enter the port number of your Databricks instance. For Client ID , enter the application (client) ID for your Azure service principal . For Client Secret , enter the client secret for your Azure service principal . For Tenant ID , enter the directory (tenant) ID for your Azure service principal . Click Test Authentication to confirm connectivity to Databricks using these details. Once successful, at the bottom of the screen click Next . Offline extraction method Atlan supports the offline extraction method for fetching metadata from Databricks. This method uses Atlan's databricks-extractor tool to fetch metadata. You need to first extract the metadata yourself and then make it available in S3 . To enter your S3 details: For Bucket name , enter the name of your S3 bucket. For Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include output\/databricks-example\/catalogs\/success\/result-0.json , output\/databricks-example\/schemas\/{{catalog_name}}\/success\/result-0.json , output\/databricks-example\/tables\/{{catalog_name}}\/success\/result-0.json , and similar files. (Optional) For Bucket region , enter the name of the S3 region. When complete, at the bottom of the screen, click Next . Agent extraction method Atlan supports using a Secure Agent for fetching metadata from Databricks. To use a Secure Agent, follow these steps: Select the Agent tab. Configure the Databricks data source by adding the secret keys for your secret store. For details on the required fields, refer to the Direct extraction section. Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. Click Next after completing the configuration. Configure the connection To complete the Databricks connection configuration: Provide a Connection Name that represents your source environment. For example, you might want to use values like production , development , gold , or analytics . Provide a Connection Name that represents your source environment. For example, you might want to use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you don't specify any user or group, nobody can manage the connection - not even admins. (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . If you don't specify any user or group, nobody can manage the connection - not even admins. (Optional) To prevent users from querying any Databricks data, change Enable SQL Query to No . (Optional) To prevent users from querying any Databricks data, change Enable SQL Query to No . (Optional) To prevent users from previewing any Databricks data, change Enable Data Preview to No . (Optional) To prevent users from previewing any Databricks data, change Enable Data Preview to No . (Optional) To prevent users from running large queries, change Max Row Limit or keep the default selection. (Optional) To prevent users from running large queries, change Max Row Limit or keep the default selection. At the bottom of the screen, click the Next button to proceed. At the bottom of the screen, click the Next button to proceed. Configure the crawler Before running the Databricks crawler, you can further configure it. System tables extraction method The system metadata extraction method is only available for Unity Catalog-enabled workspaces . It provides access to detailed metadata from system tables and supports all three authentication types. You can extract metadata from your Databricks workspace using this method. Follow these steps: Set up authentication using one of the following: Personal access token AWS service principal Azure service principal Set up authentication using one of the following: Personal access token AWS service principal Azure service principal The default options can work as is. You may choose to override the defaults for any of the remaining options: For Asset selection , select a filtering option: For SQL warehouse , click the dropdown to select the SQL warehouse you want to configure. To select the assets you want to include in crawling, click Include by hierarchy and filter for assets down to the database or schema level. (This defaults to all assets, if none are specified.) To have the crawler include Databases , Schemas , or Tables & Views based on a naming convention, click Include by regex and specify a regular expression - for example, specifying ATLAN_EXAMPLE_DB.* for Databases includes all the matching databases and their child assets. To select the assets you want to exclude from crawling, click Exclude by hierarchy and filter for assets down to the database or schema level. (This defaults to no assets, if none are specified.) To have the crawler ignore Databases , Schemas , or Tables & Views based on a naming convention, click Exclude by regex and specify a regular expression - for example, specifying ATLAN_EXAMPLE_TABLES.* for Tables & Views excludes all the matching tables and views. Click + to add more filters. If you add multiple filters, assets are crawled based on matching all the filtering conditions you have set. To import tags from Databricks to Atlan , change Import Tags to Yes . Note that you must have a Unity Catalog-enabled workspace to import Databricks tags in Atlan. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. The default options can work as is. You may choose to override the defaults for any of the remaining options: For Asset selection , select a filtering option: For SQL warehouse , click the dropdown to select the SQL warehouse you want to configure. To select the assets you want to include in crawling, click Include by hierarchy and filter for assets down to the database or schema level. (This defaults to all assets, if none are specified.) To have the crawler include Databases , Schemas , or Tables & Views based on a naming convention, click Include by regex and specify a regular expression - for example, specifying ATLAN_EXAMPLE_DB.* for Databases includes all the matching databases and their child assets. To select the assets you want to exclude from crawling, click Exclude by hierarchy and filter for assets down to the database or schema level. (This defaults to no assets, if none are specified.) To have the crawler ignore Databases , Schemas , or Tables & Views based on a naming convention, click Exclude by regex and specify a regular expression - for example, specifying ATLAN_EXAMPLE_TABLES.* for Tables & Views excludes all the matching tables and views. Click + to add more filters. If you add multiple filters, assets are crawled based on matching all the filtering conditions you have set. To import tags from Databricks to Atlan , change Import Tags to Yes . Note that you must have a Unity Catalog-enabled workspace to import Databricks tags in Atlan. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. For Asset selection , select a filtering option: For SQL warehouse , click the dropdown to select the SQL warehouse you want to configure. To select the assets you want to include in crawling, click Include by hierarchy and filter for assets down to the database or schema level. (This defaults to all assets, if none are specified.) To have the crawler include Databases , Schemas , or Tables & Views based on a naming convention, click Include by regex and specify a regular expression - for example, specifying ATLAN_EXAMPLE_DB.* for Databases includes all the matching databases and their child assets. To select the assets you want to exclude from crawling, click Exclude by hierarchy and filter for assets down to the database or schema level. (This defaults to no assets, if none are specified.) To have the crawler ignore Databases , Schemas , or Tables & Views based on a naming convention, click Exclude by regex and specify a regular expression - for example, specifying ATLAN_EXAMPLE_TABLES.* for Tables & Views excludes all the matching tables and views. Click + to add more filters. If you add multiple filters, assets are crawled based on matching all the filtering conditions you have set. To import tags from Databricks to Atlan , change Import Tags to Yes . Note that you must have a Unity Catalog-enabled workspace to import Databricks tags in Atlan. If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Toggle incremental extraction, for a faster and more efficient metadata extraction. JDBC extraction method The JDBC extraction method uses JDBC queries to extract metadata from your Databricks instance. This was the original extraction method provided by Databricks. This extraction method is only supported for personal access token authentication . You can override the defaults for any of these options: To select the assets you want to include in crawling, click Include Metadata . (This will default to all assets, if none are specified.) To select the assets you want to exclude from crawling, click Exclude Metadata . (This will default to no assets if none are specified.) To have the crawler ignore tables and views based on a naming convention, specify a regular expression in the Exclude regex for tables & views field. For View Definition Lineage , keep the default Yes to generate upstream lineage for views based on the tables referenced in the views or click No to exclude from crawling. For Advanced Config , keep Default for the default configuration or click Advanced to further configure the crawler: To enable or disable schema-level filtering at source, click Enable Source Level Filtering and select True to enable it or False to disable it. To enable or disable schema-level filtering at source, click Enable Source Level Filtering and select True to enable it or False to disable it. REST API extraction method The REST API extraction method uses Unity Catalog to extract metadata from your Databricks instance. This extraction method is supported for all three authentication options: personal access token , AWS service principal , and Azure service principal . This method is only supported by Unity Catalog-enabled workspaces. If you enable an existing workspace, you also need to upgrade your tables and views to Unity Catalog . While REST APIs are used to extract metadata, JDBC queries are still used for querying purposes. You can override the defaults for any of these options: Change the extraction method under Extraction method to REST API . To select the assets you want to include in crawling, click Include Metadata . (This will default to all assets, if none are specified.) To select the assets you want to exclude from crawling, click Exclude Metadata . (This will default to no assets if none are specified.) To import tags from Databricks to Atlan , change Import Tags to Yes . Note that you must have a Unity Catalog-enabled workspace to import Databricks tags in Atlan. For SQL warehouse , click the dropdown to select the SQL warehouse you have configured. For SQL warehouse , click the dropdown to select the SQL warehouse you have configured. If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler Follow these steps to run the Databricks crawler: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets in Atlan's asset page! Ã° Select the source Configure the connection Configure the crawler Run the crawler"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-databricks#select-a-cluster","title":"Set up Databricks | Atlan Documentation","text":"Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: SQL warehouse (formerly SQL endpoint) To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after . minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Create a service principal You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Once you've copied the client ID and secret, click Done . Azure service principal authentication You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Tenant ID (directory ID) Create a service principal To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : usage and popularity metrics Enable system.access schema You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage system.query.history (to mine query history for usage and popularity metrics) You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, ) USE SCHEMA and SELECT on the schema (for example, . ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. (Optional) Grant view permissions to access Databricks entities via APIs Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3 6 for each catalog you want to crawl in Atlan. SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-databricks#interactive-cluster","title":"Set up Databricks | Atlan Documentation","text":"Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: SQL warehouse (formerly SQL endpoint) To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after . minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Create a service principal You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Once you've copied the client ID and secret, click Done . Azure service principal authentication You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Tenant ID (directory ID) Create a service principal To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : usage and popularity metrics Enable system.access schema You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage system.query.history (to mine query history for usage and popularity metrics) You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, ) USE SCHEMA and SELECT on the schema (for example, . ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. (Optional) Grant view permissions to access Databricks entities via APIs Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3 6 for each catalog you want to crawl in Atlan. SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-databricks#sql-warehouse-formerly-sql-endpoint","title":"Set up Databricks | Atlan Documentation","text":"Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: SQL warehouse (formerly SQL endpoint) To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after . minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Create a service principal You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Once you've copied the client ID and secret, click Done . Azure service principal authentication You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Tenant ID (directory ID) Create a service principal To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : usage and popularity metrics Enable system.access schema You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage system.query.history (to mine query history for usage and popularity metrics) You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, ) USE SCHEMA and SELECT on the schema (for example, . ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. (Optional) Grant view permissions to access Databricks entities via APIs Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3 6 for each catalog you want to crawl in Atlan. SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-databricks#create-a-service-principal","title":"Set up Databricks | Atlan Documentation","text":"Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: SQL warehouse (formerly SQL endpoint) To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after . minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Create a service principal You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Once you've copied the client ID and secret, click Done . Azure service principal authentication You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Tenant ID (directory ID) Create a service principal To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : usage and popularity metrics Enable system.access schema You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage system.query.history (to mine query history for usage and popularity metrics) You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, ) USE SCHEMA and SELECT on the schema (for example, . ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. (Optional) Grant view permissions to access Databricks entities via APIs Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3 6 for each catalog you want to crawl in Atlan. SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-databricks#identity-federation-enabled","title":"Set up Databricks | Atlan Documentation","text":"Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: SQL warehouse (formerly SQL endpoint) To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after . minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Create a service principal You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Once you've copied the client ID and secret, click Done . Azure service principal authentication You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Tenant ID (directory ID) Create a service principal To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : usage and popularity metrics Enable system.access schema You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage system.query.history (to mine query history for usage and popularity metrics) You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, ) USE SCHEMA and SELECT on the schema (for example, . ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. (Optional) Grant view permissions to access Databricks entities via APIs Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3 6 for each catalog you want to crawl in Atlan. SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-databricks#identity-federation-disabled","title":"Set up Databricks | Atlan Documentation","text":"Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: SQL warehouse (formerly SQL endpoint) To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after . minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Create a service principal You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Once you've copied the client ID and secret, click Done . Azure service principal authentication You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Tenant ID (directory ID) Create a service principal To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : usage and popularity metrics Enable system.access schema You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage system.query.history (to mine query history for usage and popularity metrics) You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, ) USE SCHEMA and SELECT on the schema (for example, . ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. (Optional) Grant view permissions to access Databricks entities via APIs Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3 6 for each catalog you want to crawl in Atlan. SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-databricks#create-an-oauth-secret-for-the-service-principal","title":"Set up Databricks | Atlan Documentation","text":"Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: SQL warehouse (formerly SQL endpoint) To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after . minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Create a service principal You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Once you've copied the client ID and secret, click Done . Azure service principal authentication You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Tenant ID (directory ID) Create a service principal To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : usage and popularity metrics Enable system.access schema You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage system.query.history (to mine query history for usage and popularity metrics) You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, ) USE SCHEMA and SELECT on the schema (for example, . ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. (Optional) Grant view permissions to access Databricks entities via APIs Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3 6 for each catalog you want to crawl in Atlan. SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-databricks#create-a-service-principal-1","title":"Set up Databricks | Atlan Documentation","text":"Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: SQL warehouse (formerly SQL endpoint) To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after . minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Create a service principal You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Once you've copied the client ID and secret, click Done . Azure service principal authentication You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Tenant ID (directory ID) Create a service principal To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : usage and popularity metrics Enable system.access schema You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage system.query.history (to mine query history for usage and popularity metrics) You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, ) USE SCHEMA and SELECT on the schema (for example, . ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. (Optional) Grant view permissions to access Databricks entities via APIs Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3 6 for each catalog you want to crawl in Atlan. SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-databricks#add-a-service-principal-to-your-account","title":"Set up Databricks | Atlan Documentation","text":"Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: SQL warehouse (formerly SQL endpoint) To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after . minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Create a service principal You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Once you've copied the client ID and secret, click Done . Azure service principal authentication You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Tenant ID (directory ID) Create a service principal To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : usage and popularity metrics Enable system.access schema You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage system.query.history (to mine query history for usage and popularity metrics) You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, ) USE SCHEMA and SELECT on the schema (for example, . ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. (Optional) Grant view permissions to access Databricks entities via APIs Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3 6 for each catalog you want to crawl in Atlan. SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-databricks#assign-a-service-principal-to-a-workspace","title":"Set up Databricks | Atlan Documentation","text":"Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: SQL warehouse (formerly SQL endpoint) To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after . minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Create a service principal You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Once you've copied the client ID and secret, click Done . Azure service principal authentication You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Tenant ID (directory ID) Create a service principal To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : usage and popularity metrics Enable system.access schema You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage system.query.history (to mine query history for usage and popularity metrics) You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, ) USE SCHEMA and SELECT on the schema (for example, . ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. (Optional) Grant view permissions to access Databricks entities via APIs Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3 6 for each catalog you want to crawl in Atlan. SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-databricks#identity-federation-enabled-1","title":"Set up Databricks | Atlan Documentation","text":"Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: SQL warehouse (formerly SQL endpoint) To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after . minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Create a service principal You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Once you've copied the client ID and secret, click Done . Azure service principal authentication You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Tenant ID (directory ID) Create a service principal To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : usage and popularity metrics Enable system.access schema You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage system.query.history (to mine query history for usage and popularity metrics) You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, ) USE SCHEMA and SELECT on the schema (for example, . ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. (Optional) Grant view permissions to access Databricks entities via APIs Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3 6 for each catalog you want to crawl in Atlan. SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-databricks#identity-federation-disabled-1","title":"Set up Databricks | Atlan Documentation","text":"Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: SQL warehouse (formerly SQL endpoint) To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after . minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Create a service principal You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Once you've copied the client ID and secret, click Done . Azure service principal authentication You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Tenant ID (directory ID) Create a service principal To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : usage and popularity metrics Enable system.access schema You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage system.query.history (to mine query history for usage and popularity metrics) You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, ) USE SCHEMA and SELECT on the schema (for example, . ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. (Optional) Grant view permissions to access Databricks entities via APIs Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3 6 for each catalog you want to crawl in Atlan. SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-databricks#grant-permissions-to-crawl-metadata","title":"Set up Databricks | Atlan Documentation","text":"Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: SQL warehouse (formerly SQL endpoint) To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after . minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Create a service principal You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Once you've copied the client ID and secret, click Done . Azure service principal authentication You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Tenant ID (directory ID) Create a service principal To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : usage and popularity metrics Enable system.access schema You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage system.query.history (to mine query history for usage and popularity metrics) You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, ) USE SCHEMA and SELECT on the schema (for example, . ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. (Optional) Grant view permissions to access Databricks entities via APIs Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3 6 for each catalog you want to crawl in Atlan. SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-databricks#system-tables-extraction-method","title":"Set up Databricks | Atlan Documentation","text":"Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: SQL warehouse (formerly SQL endpoint) To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after . minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Create a service principal You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Once you've copied the client ID and secret, click Done . Azure service principal authentication You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Tenant ID (directory ID) Create a service principal To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : usage and popularity metrics Enable system.access schema You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage system.query.history (to mine query history for usage and popularity metrics) You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, ) USE SCHEMA and SELECT on the schema (for example, . ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. (Optional) Grant view permissions to access Databricks entities via APIs Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3 6 for each catalog you want to crawl in Atlan. SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-databricks#cross-workspace-extraction","title":"Set up Databricks | Atlan Documentation","text":"Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: SQL warehouse (formerly SQL endpoint) To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after . minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Create a service principal You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Once you've copied the client ID and secret, click Done . Azure service principal authentication You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Tenant ID (directory ID) Create a service principal To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : usage and popularity metrics Enable system.access schema You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage system.query.history (to mine query history for usage and popularity metrics) You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, ) USE SCHEMA and SELECT on the schema (for example, . ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. (Optional) Grant view permissions to access Databricks entities via APIs Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3 6 for each catalog you want to crawl in Atlan. SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-cross-workspace-extraction","title":"Set up cross-workspace extraction | Atlan Documentation","text":"Eliminate the need for separate crawler configurations by using a single service principal to crawl metadata from all workspaces within a Databricks metastore. This guide walks you through configuring the necessary permissions to enable cross-workspace extraction. Cross-workspace extraction isn't supported for REST API or JDBC extraction methods. Before you begin, make sure you have: A Unity Catalog-enabled Databricks workspace Account admin access to create and manage service principals Workspace admin access to grant permissions across all target workspaces At least one active SQL warehouse in each workspace you intend to crawl Set up Databricks authentication completed with one of the supported authentication methods System table extraction enabled for lineage and usage extraction Permissions required The service principal needs the following permissions to enable cross-workspace extraction: CAN_USE on SQL warehouses in each workspace SELECT on system.access.workspace_latest table USE CATALOG , BROWSE , and SELECT on all catalogs you want to crawl Add service principal to all workspaces You must use a single, common service principal that has been granted access to all Databricks workspaces you intend to crawl within the metastore. Log in to your Databricks account console as an account admin From the left menu, click Workspaces and select a workspace From the tabs along the top, click the Permissions tab In the upper right, click Add permissions In the Add permissions dialog: For User, group, or service principal , select your service principal For Permission , select workspace User Click Add For User, group, or service principal , select your service principal For Permission , select workspace User Repeat steps 2-5 for each workspace you intend to crawl Grant permissions Configure the necessary permissions for the service principal to access and extract metadata from all workspaces within the metastore. SQL workspace permissions: The service principal must have usage permissions on at least one active SQL warehouse within each workspace . The extractor uses the smallest available warehouse to run its discovery queries. Connect to your Databricks workspace using a SQL client or the SQL editor Connect to your Databricks workspace using a SQL client or the SQL editor Run the following command for each workspace, replacing the placeholders: GRANT CAN_USE ON WAREHOUSE TO ` ` ; Replace with your actual warehouse name Replace with your service principal's application ID Example GRANT CAN_USE ON WAREHOUSE production - warehouse TO ` 12345678-1234-1234-1234-123456789012 ` ; Run the following command for each workspace, replacing the placeholders: Replace with your actual warehouse name Replace with your service principal's application ID Log in to your Databricks workspace as a workspace admin From the left menu, click SQL Warehouses On the Compute page, for each SQL warehouse, click the 3-dot icon and then click Permissions Select Can use permission Click Add to assign the permission System table permissions: Access to the system schema is essential for workspace and lineage discovery. Via SQL Via UI Connect to your Databricks workspace using a SQL client or the SQL editor Run the following command, replacing the placeholder: GRANT SELECT ON TABLE system . access . workspace_latest TO ` ` ; Replace with your service principal's application ID Example GRANT SELECT ON TABLE system . access . workspace_latest TO ` 12345678-1234-1234-1234-123456789012 ` ; Log in to your Databricks workspace as a workspace admin From the left menu, click Catalog In the Catalog Explorer , navigate to system > access Click on the workspace_latest table Click the Permissions tab and then click Grant In the Grant permissions dialog: Under Principals , select your service principal Under Privileges , check SELECT Click Grant to apply the permissions System table permissions: Access to the system schema is essential for workspace and lineage discovery. Connect to your Databricks workspace using a SQL client or the SQL editor Connect to your Databricks workspace using a SQL client or the SQL editor Run the following command, replacing the placeholder: GRANT SELECT ON TABLE system . access . workspace_latest TO ` ` ; Replace with your service principal's application ID Example GRANT SELECT ON TABLE system . access . workspace_latest TO ` 12345678-1234-1234-1234-123456789012 ` ; Run the following command, replacing the placeholder: Replace with your service principal's application ID Log in to your Databricks workspace as a workspace admin From the left menu, click Catalog In the Catalog Explorer , navigate to system > access Click on the workspace_latest table Click the Permissions tab and then click Grant In the Grant permissions dialog: Under Principals , select your service principal Under Privileges , check SELECT Click Grant to apply the permissions Under Principals , select your service principal Under Privileges , check SELECT Click Grant to apply the permissions Asset permissions: The service principal requires permissions to \"see\" and \"read\" the metadata for all data assets you wish to extract. These grants must be applied to all private, public, and shared catalogs that are in scope for crawling. Important! For private catalogs, grant permissions from each workspace. For public catalogs, grant from any workspace. Via SQL Via UI Connect to your Databricks workspace using a SQL client or the SQL editor Grant catalog-level permissions (required even when using BROWSE - BROWSE automatically grants access to all schemas and tables): GRANT USE CATALOG ON CATALOG TO ` ` ; GRANT BROWSE ON CATALOG TO ` ` ; Replace with your actual catalog name Replace with your service principal's application ID If not using BROWSE, along with catalog permissions, grant additional permissions: Grant schema-level permissions: GRANT USE SCHEMA ON SCHEMA . TO ` ` ; Replace and with your actual values Replace with your service principal's application ID Example GRANT USE CATALOG ON CATALOG main TO ` 12345678-1234-1234-1234-123456789012 ` ; GRANT BROWSE ON CATALOG main TO ` 12345678-1234-1234-1234-123456789012 ` ; Log in to your Databricks workspace as a workspace admin From the left menu, click Catalog In the Catalog Explorer , navigate to the catalog you want to grant permissions on (for example, main ) Click the Permissions tab and then click Grant In the Grant permissions dialog: Under Principals , select your service principal Under Privileges , check the following permissions: USE CATALOG USE SCHEMA BROWSE SELECT Click Grant to apply the permissions Repeat steps 3-5 for each catalog you want to crawl in Atlan Asset permissions: The service principal requires permissions to \"see\" and \"read\" the metadata for all data assets you wish to extract. These grants must be applied to all private, public, and shared catalogs that are in scope for crawling. For private catalogs, grant permissions from each workspace. For public catalogs, grant from any workspace. Connect to your Databricks workspace using a SQL client or the SQL editor Connect to your Databricks workspace using a SQL client or the SQL editor Grant catalog-level permissions (required even when using BROWSE - BROWSE automatically grants access to all schemas and tables): GRANT USE CATALOG ON CATALOG TO ` ` ; GRANT BROWSE ON CATALOG TO ` ` ; Replace with your actual catalog name Replace with your service principal's application ID Grant catalog-level permissions (required even when using BROWSE - BROWSE automatically grants access to all schemas and tables): Replace with your actual catalog name Replace with your service principal's application ID If not using BROWSE, along with catalog permissions, grant additional permissions: Grant schema-level permissions: GRANT USE SCHEMA ON SCHEMA . TO ` ` ; Replace and with your actual values Replace with your service principal's application ID If not using BROWSE, along with catalog permissions, grant additional permissions: Grant schema-level permissions: GRANT USE SCHEMA ON SCHEMA . TO ` ` ; Replace and with your actual values Replace with your service principal's application ID Grant schema-level permissions: Replace and with your actual values Replace with your service principal's application ID Log in to your Databricks workspace as a workspace admin From the left menu, click Catalog In the Catalog Explorer , navigate to the catalog you want to grant permissions on (for example, main ) Click the Permissions tab and then click Grant In the Grant permissions dialog: Under Principals , select your service principal Under Privileges , check the following permissions: USE CATALOG USE SCHEMA BROWSE SELECT Click Grant to apply the permissions Under Principals , select your service principal Under Privileges , check the following permissions: Click Grant to apply the permissions Repeat steps 3-5 for each catalog you want to crawl in Atlan Need help? Check Troubleshooting Databricks connectivity for common issues Contact Atlan support for help with setup or integration Next steps Add service principal to all workspaces"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-databricks#optional-grant-permissions-to-query-and-preview-data","title":"Set up Databricks | Atlan Documentation","text":"Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: SQL warehouse (formerly SQL endpoint) To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after . minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Create a service principal You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Once you've copied the client ID and secret, click Done . Azure service principal authentication You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Tenant ID (directory ID) Create a service principal To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : usage and popularity metrics Enable system.access schema You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage system.query.history (to mine query history for usage and popularity metrics) You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, ) USE SCHEMA and SELECT on the schema (for example, . ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. (Optional) Grant view permissions to access Databricks entities via APIs Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3 6 for each catalog you want to crawl in Atlan. SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"}
{"url":"https:\/\/docs.atlan.com\/product\/capabilities\/insights\/how-tos\/query-data","title":"query data | Atlan Documentation","text":"There are two ways to query data in Atlan: writing your own SQL using the Visual Query Builder Atlan pushes all queries to the source (no data is stored in Atlan). In addition, Atlan applies access policies to the results before displaying them. Write your own SQL Anyone with the knowledge to write SQL. Any Atlan user with data access to the asset can query data. To query an asset with your own SQL: From the left menu of any screen, click Insights . Use the Select database dropdown to choose another database, if necessary. Hover over the table or view, and click the play icon. This writes and runs a basic preview query. (Optional) Click the open asset sidebar icon to view more details in the asset sidebar. (Optional) Click the eye icon to view a preview of the query results. (Optional) Click the 3-dot icon for more options: Click Set editor context to set the same connection, database, and schema name in the query editor as selected in the Explorer tab. Click Place name in editor to view the asset name in the query editor. Click Copy path to copy the full path of the asset, including database and schema names. (Optional) Click the open asset sidebar icon to view more details in the asset sidebar. (Optional) Click the eye icon to view a preview of the query results. (Optional) Click the 3-dot icon for more options: Click Set editor context to set the same connection, database, and schema name in the query editor as selected in the Explorer tab. Click Place name in editor to view the asset name in the query editor. Click Copy path to copy the full path of the asset, including database and schema names. Click Set editor context to set the same connection, database, and schema name in the query editor as selected in the Explorer tab. Click Place name in editor to view the asset name in the query editor. Click Copy path to copy the full path of the asset, including database and schema names. Under the Untitled tab on the right, change the sample query or write your own - separate multiple queries with a semicolon ; . Click the Run button in the upper right to test your query as you write it. (Optional) Click the downward arrow next to the Run button to export query results via email or schedule the query . (Optional) Click the downward arrow next to the Run button to export query results via email or schedule the query . (Optional) If you have multiple tabs open in the query editor, right-click a tab to open the tabs menu. You can close a specific tab or all tabs, or duplicate the query. (Optional) From the top right of the query editor, click the 3-dot icon for additional query editor actions or to customize it further: Click or hover over Duplicate query to create a duplicate version of your query. Click or hover over Open command palette to view the actions you can run inside the query editor. Click or hover over Themes and then select your preferred theme for the query editor. Click or hover over Tab spacing to change the tab spacing for your queries. Click or hover over Font size to change the font size for your queries. Click or hover over Cursor to change the cursor position in the query editor. Click or hover over Autosuggestions to turn off autosuggestions for assets in the query editor. Click or hover over Duplicate query to create a duplicate version of your query. Click or hover over Open command palette to view the actions you can run inside the query editor. Click or hover over Themes and then select your preferred theme for the query editor. Click or hover over Tab spacing to change the tab spacing for your queries. Click or hover over Font size to change the font size for your queries. Click or hover over Cursor to change the cursor position in the query editor. Click or hover over Autosuggestions to turn off autosuggestions for assets in the query editor. The editor supports all read-based SQL statements, including JOIN . The editor will not run any write-based statements. The following SQL statements are not supported: You can select the context for your query to the left of the Run button. Then you won't need to fully qualify table names with schema and database names. Use the Visual Query Builder Any Atlan user with data access to the asset . No SQL knowledge required! To query an asset using the Visual Query Builder: From the left menu of any screen, click Insights . At the top of the screen, to the right of the Untitled tab, click the + button and select New visual query . Under Select from choose the table or view you want to query. (Optional) In the column selector to the right, select the column you want to query. Then develop your query: Click the Run button to run the query and preview its results. Click the blue circular + button to add an action to the query. Repeat these steps until your query is complete. Click the Run button to run the query and preview its results. Click the blue circular + button to add an action to the query. Repeat these steps until your query is complete. (Optional) If there are any errors in your query, click Auto fix for Atlan to recommend a fix. (Optional) In the query results set, click Copy to copy the query results or click Download to export them. You can learn more about the query builder actions in this example . Write your own SQL Use the Visual Query Builder"}
{"url":"https:\/\/docs.atlan.com\/product\/capabilities\/discovery\/references\/provide-credentials-to-view-sample-data","title":"Provide credentials to view sample data | Atlan Documentation","text":"Once your connection admins have configured bring your own credentials (BYOC) in Atlan, users will need to provide their own credentials before they can view the sample data in the asset profile. This will help you enforce better governance across your organization. Any Atlan user with data access to the asset and their own credentials for the data store. Atlan will display a 100-row sample of the data . Use your own credentials to view sample data Atlan supports both basic username and password as well as key pair authentication of your credentials. Atlan also supports SSO authentication . To set up your own credentials for viewing sample data: On the Assets page, click on an asset to view its asset profile. In the asset profile, click Sample Data . To set up your credentials for viewing the sample data, click Get Started . In the popup window, click Get Started once again to proceed. In the User credential setup dialog box, Basic is selected as the default authentication option. Enter the following: For Username , enter the username for the connection. For Password , enter the password for that connection. For Role , enter your role for that connection. For Warehouse , enter the name of the warehouse. For Username , enter the username for the connection. For Password , enter the password for that connection. For Role , enter your role for that connection. For Warehouse , enter the name of the warehouse. Click the Test Authentication button to confirm your credentials. Once authentication is successful, click Done . You can now view sample data using your own credentials! Ã° When using the key pair method, you'll need to enter your encrypted private key and the private key password to complete the authentication process. Once you've set up your credentials for viewing sample data, you can also manage your credentials . If your admin has enabled sample data download , you can export sample data in a CSV file. Use your own credentials to view sample data"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-databricks#optional-grant-permissions-to-import-and-update-tags","title":"Set up Databricks | Atlan Documentation","text":"Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: SQL warehouse (formerly SQL endpoint) To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after . minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Create a service principal You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Once you've copied the client ID and secret, click Done . Azure service principal authentication You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Tenant ID (directory ID) Create a service principal To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : usage and popularity metrics Enable system.access schema You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage system.query.history (to mine query history for usage and popularity metrics) You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, ) USE SCHEMA and SELECT on the schema (for example, . ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. (Optional) Grant view permissions to access Databricks entities via APIs Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3 6 for each catalog you want to crawl in Atlan. SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/manage-databricks-tags","title":"Manage Databricks tags | Atlan Documentation","text":"You must have a Unity Catalog-enabled workspace to import Databricks tags in Atlan. Atlan enables you to import your Databricks tags , update your Databricks assets with the imported tags, and push the tag updates back to Databricks: Import tags - crawl Databricks tags from Databricks to Atlan Reverse sync - sync Databricks tag updates from Atlan to Databricks Once you've imported your Databricks tags to Atlan: Your Databricks assets in Atlan will be automatically enriched with their Databricks tags. Imported Databricks tags will be mapped to corresponding Atlan tags through case-insensitive name match - multiple Databricks tags can be matched to a single tag in Atlan. You can also attach Databricks tags , including tag values, to your Databricks assets in Atlan - allowing you to categorize your assets at a more granular level. You can filter your assets by Databricks tags and tag values. You can enable reverse sync to push any tag updates for your Databricks assets back to Databricks - including tag values added to assets in Atlan. Enabling reverse sync will only update existing tags in Databricks. It will neither create nor delete any tags in Databricks. You must have a Unity Catalog-enabled workspace and SQL warehouse configured to import Databricks tags in Atlan. Before you can import tags from and push tag updates to Databricks using personal access token , AWS service principal , or Azure service principal authentication, you will need to do the following: Ensure that you have a Unity Catalog-enabled workspace and a SQL warehouse configured. Create tags or have existing tags in Databricks. Grant permissions to import tags from and push tag updates to Databricks. Import Databricks tags to Atlan You will need to be an admin user in Atlan to import Databricks tags to Atlan. You will also need to work with your Databricks administrator to grant permissions to import tags from Databricks - you may not have access yourself. You can import your Databricks tags to Atlan through one-way tag sync. The synced Databricks tags will be matched to corresponding tags in Atlan through case-insensitive name match and your Databricks assets will be enriched with their synced tags from Databricks. To import Databricks tags to Atlan, you can either: Create a new Databricks workflow and configure the crawler to import tags. Modify the crawler's configuration for an existing Databricks workflow to change Import Tags to Yes . If you subsequently modify the workflow to disable tag import, for any tags already imported, Atlan will preserve those tags. Once the crawler has completed running, tags imported from Databricks will be available to use for tagging assets ! Ã° View Databricks tags in Atlan Once you've imported your Databricks tags, you will be able to view and manage your Databricks tags in Atlan. To view Databricks tags: From the left menu of any screen, click Governance . Under the Governance heading of the _Governance cente_r, click Tags . (Optional) Under Tags , click the funnel icon to filter tags by source type. Click Databricks to filter for tags imported from Databricks. From the left menu under Tags , select a synced tag. (Optional) Click the Linked assets tab to view linked assets for your Databricks tag. (Optional) In the top right, click the pencil icon to add a description and change the tag icon . You cannot rename tags synced from Databricks. Push tag updates to Databricks Any admin or member user in Atlan can configure reverse sync for tag updates to Databricks. You will also need to work with your Databricks administrator to grant additional permissions to push updates - you may not have access yourself. You can enable reverse sync for your imported Databricks tags in Atlan and push all tag updates for your Databricks assets back to source. Once you have enabled reverse sync, any Databricks assets with tags updated in Atlan will also be updated in Databricks. To enable reverse sync for imported Databricks tags: From the left menu of any screen, click Governance . Under the Governance heading of the _Governance cente_r, click Tags . (Optional) Under Tags , click the funnel icon to filter tags by source type. Click Databricks to filter for tags imported from Databricks. In the left menu under Tags , select a synced Databricks tag - synced tags will display the Databricks icon next to the tag name. Under Synced tags , in the upper right, turn on Enable reverse sync to synchronize tag updates from Atlan to Databricks. In the corresponding confirmation dialog, click Yes, enable it to enable reverse tag sync or click Cancel . Now when you attach Databricks tags to your Databricks assets in Atlan, these tag updates will also be pushed to Databricks! Ã° Enabling reverse sync will not trigger any updates in Databricks until synced tags are attached to Databricks assets in Atlan. Import Databricks tags to Atlan View Databricks tags in Atlan Push tag updates to Databricks"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-databricks#optional-grant-permissions-to-extract-lineage-and-usage-from-system-tables","title":"Set up Databricks | Atlan Documentation","text":"Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: SQL warehouse (formerly SQL endpoint) To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after . minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Create a service principal You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Once you've copied the client ID and secret, click Done . Azure service principal authentication You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Tenant ID (directory ID) Create a service principal To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : usage and popularity metrics Enable system.access schema You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage system.query.history (to mine query history for usage and popularity metrics) You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, ) USE SCHEMA and SELECT on the schema (for example, . ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. (Optional) Grant view permissions to access Databricks entities via APIs Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3 6 for each catalog you want to crawl in Atlan. SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/extract-lineage-and-usage-from-databricks","title":"extract lineage and usage from Databricks | Atlan Documentation","text":"Once you have crawled assets from Databricks , you can retrieve lineage from Unity Catalog and usage and popularity metrics from query history or system tables. This is supported for all three authentication methods : personal access token, AWS service principal, and Azure service principal. Both Atlan and Databricks strongly recommend using the system tables method to extract lineage and usage and popularity metrics from Databricks. Usage and popularity metrics can be retrieved for all Databricks users. However, your Databricks workspace must be Unity Catalog-enabled for the retrieval of lineage and usage and popularity metrics to succeed. You may also need to upgrade existing tables and views to Unity Catalog , as well as reach out to your Databricks account executive to enable lineage in Unity Catalog. (As of publishing, the feature is still in preview from Databricks on AWS and Azure.) To retrieve lineage and usage from Databricks, rev iew the order of operations and then complete the following steps. Select the extractor To select the Databricks lineage and usage extractor: In the top right of any screen, navigate to New and then click New Workflow . From the filters along the top, click Miner . From the list of packages, select Databricks Miner and click on Setup Workflow . Configure the lineage extractor Choose your lineage extraction method: In REST API , Atlan connects to your database and extracts lineage directly. In Offline , you will need to first extract lineage yourself and make it available in S3 . In System Table , Atlan connects to your database and queries system tables to extract lineage directly. REST API To configure the Databricks lineage extractor: For Connection , select the connection to extract. (To select a connection, the crawler must have already run.) Click Next to proceed. Offline extraction method Atlan supports the offline extraction method for extracting lineage from Databricks This method uses Atlan's databricks-extractor tool to extract lineage. You will need to first extract lineage yourself and make it available in S3 . To enter your S3 details: For Connection , select the connection to extract. (To select a connection, the crawler must have already run.) For Bucket name , enter the name of your S3 bucket. For Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include extracted-lineage\/result-0.json , extracted-query-history\/result-0.json , and so on. For Bucket region , enter the name of the S3 region. When complete, at the bottom of the screen, click Next . System table To configure the Databricks lineage extractor: For Connection , select the connection to extract. (To select a connection, the crawler must have already run.) * Extraction Catalog Type : Default : Select to fetch lineage from the system catalog and access schema. Cloned_catalog : Select to fetch lineage from a cloned catalog and schema. Before proceeding, make sure the following prerequisites are met: You have already created cloned views named column_lineage and table_lineage in your schema. If not, follow the steps in Create cloned views of system tables . The atlan-user must have SELECT permissions on both views to access lineage data. Then, provide values for the following fields: Cloned Catalog Name Catalog containing the cloned views. Cloned Schema Name Schema containing the cloned views. Default : Select to fetch lineage from the system catalog and access schema. Default : Select to fetch lineage from the system catalog and access schema. Cloned_catalog : Select to fetch lineage from a cloned catalog and schema. Before proceeding, make sure the following prerequisites are met: You have already created cloned views named column_lineage and table_lineage in your schema. If not, follow the steps in Create cloned views of system tables . The atlan-user must have SELECT permissions on both views to access lineage data. Then, provide values for the following fields: Cloned Catalog Name Catalog containing the cloned views. Cloned Schema Name Schema containing the cloned views. Cloned_catalog : Select to fetch lineage from a cloned catalog and schema. Before proceeding, make sure the following prerequisites are met: You have already created cloned views named column_lineage and table_lineage in your schema. If not, follow the steps in Create cloned views of system tables . The atlan-user must have SELECT permissions on both views to access lineage data. Then, provide values for the following fields: Cloned Catalog Name Catalog containing the cloned views. Cloned Schema Name Schema containing the cloned views. For SQL Warehouse ID , enter the ID you copied from your SQL warehouse . Click Next to proceed. (Optional) Configure the usage extractor Atlan extracts usage and popularity metrics from: This feature is currently limited to queries on SQL warehouses - queries on interactive clusters are not supported. Additionally, expensive queries and compute costs for Databricks assets are currently unavailable due to limitations of the Databricks APIs . To configure the Databricks usage and popularity extractor: For Fetch Query History and Calculate Popularity , click Yes to retrieve usage and popularity metrics for your Databricks assets. For Popularity Extraction Method : Choose one of the following methods to extract usage and popularity metrics:: Click REST API to extract usage and popularity metrics from query history. Click System table to extract metrics directly from system tables: Extraction catalog type for popularity : Choose where to fetch popularity data from: Default : Uses the system catalog and query schema to fetch popularity metrics. Cloned_catalog : Select to fetch popularity from cloned views in a separate catalog and schema. Before proceeding: The query_history view must exist in the provided schema. The atlan-user must have SELECT permission on the view. Then provide: Cloned Catalog Name The catalog that contains the query_history view. Cloned Schema Name The schema that contains the query_history view. For more information, see Create cloned views of system tables . For SQL Warehouse ID , enter the ID you copied from your SQL warehouse . Click REST API to extract usage and popularity metrics from query history. Click System table to extract metrics directly from system tables: Extraction catalog type for popularity : Choose where to fetch popularity data from: Default : Uses the system catalog and query schema to fetch popularity metrics. Cloned_catalog : Select to fetch popularity from cloned views in a separate catalog and schema. Before proceeding: The query_history view must exist in the provided schema. The atlan-user must have SELECT permission on the view. Then provide: Cloned Catalog Name The catalog that contains the query_history view. Cloned Schema Name The schema that contains the query_history view. For more information, see Create cloned views of system tables . For SQL Warehouse ID , enter the ID you copied from your SQL warehouse . Extraction catalog type for popularity : Choose where to fetch popularity data from: Default : Uses the system catalog and query schema to fetch popularity metrics. Cloned_catalog : Select to fetch popularity from cloned views in a separate catalog and schema. Before proceeding: The query_history view must exist in the provided schema. The atlan-user must have SELECT permission on the view. Then provide: Cloned Catalog Name The catalog that contains the query_history view. Cloned Schema Name The schema that contains the query_history view. For more information, see Create cloned views of system tables . Extraction catalog type for popularity : Choose where to fetch popularity data from: Default : Uses the system catalog and query schema to fetch popularity metrics. Cloned_catalog : Select to fetch popularity from cloned views in a separate catalog and schema. Before proceeding: The query_history view must exist in the provided schema. The atlan-user must have SELECT permission on the view. Cloned Catalog Name The catalog that contains the query_history view. Cloned Schema Name The schema that contains the query_history view. For more information, see Create cloned views of system tables . For SQL Warehouse ID , enter the ID you copied from your SQL warehouse . For SQL Warehouse ID , enter the ID you copied from your SQL warehouse . Configure the usage extractor: For Popularity Window (days) , 30 days is the maximum limit. You can set a shorter popularity window of less than 30 days. For Start time , choose the earliest date from which to mine query history. If you're using the offline extraction method to extract query history from Databricks, skip to the next step. For Excluded Users , type the names of users to be excluded while calculating usage metrics for Databricks assets. Press enter after each name to add more names. For Popularity Window (days) , 30 days is the maximum limit. You can set a shorter popularity window of less than 30 days. For Start time , choose the earliest date from which to mine query history. If you're using the offline extraction method to extract query history from Databricks, skip to the next step. For Excluded Users , type the names of users to be excluded while calculating usage metrics for Databricks assets. Press enter after each name to add more names. If running the miner for the first time, Atlan recommends setting a start date around three days prior to the current date and then scheduling it daily to build up to two weeks of query history. Mining two weeks of query history on the first miner run may cause delays. For all subsequent runs, Atlan requires a minimum lag of 24 to 48 hours to capture all the relevant transformations that were part of a session. Learn more about the miner logic here . Run the extractor To run the Databricks lineage and popularity extractor, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . This is currently only supported when using REST API and offline extraction methods. If you're using system tables, skip to step 2. You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the extractor has completed running, you will see lineage for Databricks assets! Ã° Select the extractor Configure the lineage extractor (Optional) Configure the usage extractor Run the extractor"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-databricks#enable-systemaccess-schema","title":"Set up Databricks | Atlan Documentation","text":"Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: SQL warehouse (formerly SQL endpoint) To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after . minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Create a service principal You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Once you've copied the client ID and secret, click Done . Azure service principal authentication You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Tenant ID (directory ID) Create a service principal To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : usage and popularity metrics Enable system.access schema You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage system.query.history (to mine query history for usage and popularity metrics) You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, ) USE SCHEMA and SELECT on the schema (for example, . ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. (Optional) Grant view permissions to access Databricks entities via APIs Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3 6 for each catalog you want to crawl in Atlan. SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-databricks#optional-enable-systeminformation_schematable","title":"Set up Databricks | Atlan Documentation","text":"Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: SQL warehouse (formerly SQL endpoint) To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after . minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Create a service principal You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Once you've copied the client ID and secret, click Done . Azure service principal authentication You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Tenant ID (directory ID) Create a service principal To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : usage and popularity metrics Enable system.access schema You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage system.query.history (to mine query history for usage and popularity metrics) You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, ) USE SCHEMA and SELECT on the schema (for example, . ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. (Optional) Grant view permissions to access Databricks entities via APIs Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3 6 for each catalog you want to crawl in Atlan. SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-databricks#grant-permissions","title":"Set up Databricks | Atlan Documentation","text":"Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: SQL warehouse (formerly SQL endpoint) To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after . minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Create a service principal You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Once you've copied the client ID and secret, click Done . Azure service principal authentication You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Tenant ID (directory ID) Create a service principal To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : usage and popularity metrics Enable system.access schema You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage system.query.history (to mine query history for usage and popularity metrics) You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, ) USE SCHEMA and SELECT on the schema (for example, . ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. (Optional) Grant view permissions to access Databricks entities via APIs Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3 6 for each catalog you want to crawl in Atlan. SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-databricks#optional-enable-systemquery-schema","title":"Set up Databricks | Atlan Documentation","text":"Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: SQL warehouse (formerly SQL endpoint) To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after . minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Create a service principal You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Once you've copied the client ID and secret, click Done . Azure service principal authentication You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Tenant ID (directory ID) Create a service principal To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : usage and popularity metrics Enable system.access schema You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage system.query.history (to mine query history for usage and popularity metrics) You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, ) USE SCHEMA and SELECT on the schema (for example, . ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. (Optional) Grant view permissions to access Databricks entities via APIs Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3 6 for each catalog you want to crawl in Atlan. SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"}
{"url":"https:\/\/docs.atlan.com\/product\/capabilities\/usage-and-popularity\/how-tos\/interpret-usage-metrics","title":"Interpret usage metrics | Atlan Documentation","text":"Atlan currently supports usage and popularity metrics for the following connectors: Amazon Redshift - tables, views, and columns. Expensive queries and compute costs for Amazon Redshift assets are currently unavailable due to limitations at source. Databricks - tables, views, and columns. Expensive queries and compute costs for Databricks assets are currently unavailable due to limitations of the Databricks APIs . Google BigQuery - tables, views, and columns Microsoft Power BI - reports and dashboards Snowflake - tables, views, and columns Powered by Atlan's enhanced query-mining capabilities, you can view popularity metrics for supported assets: The popularity score of an asset is computed using both the number of queries and the number of users who have queried that asset in the last 30 days. The popularity score of an asset helps determine its relative popularity. All assets with a popularity score are then slotted into one of four percentile groups - Least popular , Less popular , Popular , and Most popular . Popularity score is calculated using the following formula: number of distinct users * log (total number of read queries) Time period = 30 days Popularity score is calculated using the following formula: number of distinct users * log (total number of read queries) Popularity score is calculated using the following formula: Time period = 30 days Time period = 30 days The popularity indicator is displayed for all supported assets that have been queried in the last 30 days. This indicator visualizes the relative popularity of an asset on a scale of 1 to 4 blue bars - 1 being the lowest score and 4 being the highest. View popularity metrics To view popularity metrics for your assets, complete these steps. Identify popular assets Being able to identify your most relevant and trusted data assets can help you increase their adoption and drive usage within your organization. To view popularity metrics for an asset: From the left menu in Atlan, click Assets . For Connector on the Assets page, select a supported connector - for this example, we'll select Snowflake . From the Popularity sorting menu, click Most popular to view most used assets or Least popular to view least used assets. Your assets will now have a popularity indicator. To view the popularity popover for an asset, click or hover over the popularity indicator . You'll now be able to see all the relevant popularity metrics for your asset! Ã° View usage metrics in the asset sidebar The new Usage tab in the asset sidebar helps you view usage metadata for your assets. For example, if you'd like to appoint a data steward for your data assets, you'll be able to determine the right candidate based on the top users for that asset. You'll also be able to review popular queries or users for a particular table while checking for data compliance. To view usage details for an asset: From the left menu in Atlan, click Assets . For Connector on the Assets page, select a supported connector - for this example, we'll select Snowflake . From the Popularity sorting menu, click Most popular to view most used assets or Least popular to view least used assets. In the bottom right of any asset card, click or hover over the popularity indicator to open the popularity popover. In the popularity popover, click View usage details to view the following: For Usage , view top and recent users in the last 30 days. For Queries , view top five queries by context - Popular , Slow , and Expensive . Only read queries or SELECT statements are shown for these queries. For Compute , view the total compute cost for an asset. The compute cost is split between read and write queries, allowing you to better understand the cost breakdown for individual assets: Read queries - SELECT statements. Write queries - all non- SELECT statements, for example, UPDATE , INSERT , CREATE , and more. For Usage , view top and recent users in the last 30 days. For Queries , view top five queries by context - Popular , Slow , and Expensive . Only read queries or SELECT statements are shown for these queries. For Compute , view the total compute cost for an asset. The compute cost is split between read and write queries, allowing you to better understand the cost breakdown for individual assets: Read queries - SELECT statements. Write queries - all non- SELECT statements, for example, UPDATE , INSERT , CREATE , and more. Read queries - SELECT statements. Write queries - all non- SELECT statements, for example, UPDATE , INSERT , CREATE , and more. The usage details for the asset will now appear in the asset sidebar! Ã° View and sort columns by popularity For any Snowflake, Databricks, or Google BigQuery table or view sorted by popularity, you'll also be able to view and sort the columns by popularity in the asset profile. To view column assets by popularity: From the left menu in Atlan, click Assets . For Connector on the Assets page, select a supported connector - for this example, we'll select Snowflake . From the Popularity sorting menu, click Most popular to view most used assets or Least popular to view least used assets. Click any asset to open to its asset profile. In the Column preview tab of the asset profile, hover over the popularity indicator to view the popularity popover for your columns. You'll now be able to view the popularity score, number of queries and users, and timestamp for last queried for your columns! Ã° View queries by context Get the context you need before querying an asset to help you optimize your queries. Query popular, slow, or expensive queries from the Usage tab directly in Insights. To view and work with queries by context: From the left menu in Atlan, click Assets . For Connector on the Assets page, select a supported connector - for this example, we'll select Snowflake . From the Popularity sorting menu, click Most popular to view most used assets or Least popular to view least used assets. In the bottom right of any asset card, click or hover over the popularity indicator to open the popularity popover. In the popularity popover, click View usage details . In the Usage tab in the asset sidebar, navigate to Queries and depending on the type of query you'd like to see: Click Popular to see the top five most popular queries. Click Slow to see queries sorted by average duration and last run. Click Expensive to see the top five most expensive queries. Click Popular to see the top five most popular queries. Click Slow to see queries sorted by average duration and last run. Click Expensive to see the top five most expensive queries. Once you've selected the relevant query type, hover over a query card to: Click the expand icon to see the query details. Click the copy icon to copy the query and use it as a template for writing your own queries. Click the code icon to open the query directly in Insights and run it. Click the expand icon to see the query details. Click the copy icon to copy the query and use it as a template for writing your own queries. Click the code icon to open the query directly in Insights and run it. If you have any questions about usage and popularity metrics, head over here . View popularity metrics View queries by context"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-databricks#optional-create-cloned-views-of-system-tables","title":"Set up Databricks | Atlan Documentation","text":"Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: SQL warehouse (formerly SQL endpoint) To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after . minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Create a service principal You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Once you've copied the client ID and secret, click Done . Azure service principal authentication You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Tenant ID (directory ID) Create a service principal To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : usage and popularity metrics Enable system.access schema You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage system.query.history (to mine query history for usage and popularity metrics) You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, ) USE SCHEMA and SELECT on the schema (for example, . ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. (Optional) Grant view permissions to access Databricks entities via APIs Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3 6 for each catalog you want to crawl in Atlan. SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-databricks#grant-permissions-1","title":"Set up Databricks | Atlan Documentation","text":"Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: SQL warehouse (formerly SQL endpoint) To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after . minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Create a service principal You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Once you've copied the client ID and secret, click Done . Azure service principal authentication You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Tenant ID (directory ID) Create a service principal To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : usage and popularity metrics Enable system.access schema You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage system.query.history (to mine query history for usage and popularity metrics) You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, ) USE SCHEMA and SELECT on the schema (for example, . ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. (Optional) Grant view permissions to access Databricks entities via APIs Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3 6 for each catalog you want to crawl in Atlan. SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-databricks#grant-permissions-2","title":"Set up Databricks | Atlan Documentation","text":"Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: SQL warehouse (formerly SQL endpoint) To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after . minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Create a service principal You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Once you've copied the client ID and secret, click Done . Azure service principal authentication You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Tenant ID (directory ID) Create a service principal To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : usage and popularity metrics Enable system.access schema You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage system.query.history (to mine query history for usage and popularity metrics) You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, ) USE SCHEMA and SELECT on the schema (for example, . ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. (Optional) Grant view permissions to access Databricks entities via APIs Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3 6 for each catalog you want to crawl in Atlan. SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-databricks#locate-warehouse-id","title":"Set up Databricks | Atlan Documentation","text":"Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: SQL warehouse (formerly SQL endpoint) To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after . minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Create a service principal You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Once you've copied the client ID and secret, click Done . Azure service principal authentication You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Tenant ID (directory ID) Create a service principal To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : usage and popularity metrics Enable system.access schema You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage system.query.history (to mine query history for usage and popularity metrics) You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, ) USE SCHEMA and SELECT on the schema (for example, . ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. (Optional) Grant view permissions to access Databricks entities via APIs Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3 6 for each catalog you want to crawl in Atlan. SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-databricks#optional-grant-view-permissions-to-access-databricks-entities-via-apis","title":"Set up Databricks | Atlan Documentation","text":"Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: SQL warehouse (formerly SQL endpoint) To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after . minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Create a service principal You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Once you've copied the client ID and secret, click Done . Azure service principal authentication You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Tenant ID (directory ID) Create a service principal To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : usage and popularity metrics Enable system.access schema You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage system.query.history (to mine query history for usage and popularity metrics) You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, ) USE SCHEMA and SELECT on the schema (for example, . ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. (Optional) Grant view permissions to access Databricks entities via APIs Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3 6 for each catalog you want to crawl in Atlan. SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-databricks#optional-grant-permissions-for-views-and-materialized-views","title":"Set up Databricks | Atlan Documentation","text":"Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: SQL warehouse (formerly SQL endpoint) To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after . minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Create a service principal You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Once you've copied the client ID and secret, click Done . Azure service principal authentication You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Tenant ID (directory ID) Create a service principal To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : usage and popularity metrics Enable system.access schema You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage system.query.history (to mine query history for usage and popularity metrics) You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, ) USE SCHEMA and SELECT on the schema (for example, . ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. (Optional) Grant view permissions to access Databricks entities via APIs Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3 6 for each catalog you want to crawl in Atlan. SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/how-tos\/set-up-databricks#optional-grant-permissions-to-mine-query-history","title":"Set up Databricks | Atlan Documentation","text":"Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: SQL warehouse (formerly SQL endpoint) To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after . minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . Under the Advanced options expandable, open the JDBC\/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Create a service principal You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Once you've copied the client ID and secret, click Done . Azure service principal authentication You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Tenant ID (directory ID) Create a service principal To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : usage and popularity metrics Enable system.access schema You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage system.query.history (to mine query history for usage and popularity metrics) You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Ã° Âª Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create a catalog and schema to store cloned views. Use meaningful and unique names for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW . . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW . . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW . . query_history AS SELECT * FROM system . query . history ; Replace and with the catalog and schema names used in your environment. Replace and with the catalog and schema names used in your environment. Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, ) USE SCHEMA and SELECT on the schema (for example, . ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. (Optional) Grant view permissions to access Databricks entities via APIs Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Notebook API ( \/api\/2.0\/workspace\/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Queries API ( \/api\/2.0\/sql\/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Job API ( \/api\/2.2\/jobs\/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . Pipeline API ( \/api\/2.0\/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3 6 for each catalog you want to crawl in Atlan. SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/business-intelligence\/microsoft-power-bi\/how-tos\/set-up-microsoft-power-bi#__docusaurus_skipToContent_fallback","title":"Set up Microsoft Power BI | Atlan Documentation","text":"Depending on the authentication method you choose, you may need a combination of your Cloud Application Administrator or Application Administrator for Microsoft Entra ID, Microsoft 365 administrator for Microsoft 365, and Fabric Administrator ( formerly known as Power BI Administrator ) for Microsoft Power BI to complete these tasks -> you may not have access yourself. This guide outlines how to set up Microsoft Power BI so it can connect with Atlan for metadata extraction and lineage tracking. Before you begin Register application in Microsoft Entra ID You need your Cloud Application Administrator or Application Administrator to complete these steps > you may not have access yourself. This is required if the creation of registered applications isn't enabled for the entire organization. To register a new application in Microsoft Entra ID: Log in to the Azure portal . Click App registrations from the left menu. Click + New registration . Enter a name for your client application and click Register . Application (client) ID Directory (tenant) ID Click Certificates & secrets from the left menu. Under Client secrets , click + New client secret . Enter a description, select an expiry time, and click Add . Copy and securely store the client secret Value . Create security group in Microsoft Entra ID You need your Cloud Application Administrator or Application Administrator to complete these steps - you may not have access yourself. To create a security group for your application: Log in to the Azure portal . Click Groups under the Manage section. Click New group . Set the Group type to Security . Enter a Group name and optional description. Click No members selected . Click Select and then Create . By the end of these steps, you have registered an application with Microsoft Entra ID and created a Security Group with the appropriate member. Configure authentication options Atlan supports two authentication methods for fetching metadata from Microsoft Power BI: Service principal authentication (recommended) When using Service Principal authentication, you must decide how the connector shall access metadata to catalog assets and build lineage. There are two supported options: This option grants permissions that let the service principal to access only admin-level Power BI APIs. In this mode, Atlan extracts metadata exclusively using administrative endpoints. This option is recommended for stricter access control environments. You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks - you may not have access yourself. To configure admin API access: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Admin API settings : Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group under Specific security groups Click Apply Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group under Specific security groups Click Apply Add your security group under Specific security groups Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Add your security group This option grants permissions that let the service principal to access both admin and non-admin Power BI APIs. This enables Atlan to extract richer metadata and build detailed lineage across Power BI assets. You need to be at least a member of the Microsoft Power BI workspace to which you want to add the security group to complete these steps - you may not have access yourself. Make sure that you add the security group from the homepage and not the admin portal. To assign a Microsoft Power BI workspace role to the security group: Open the Microsoft Power BI homepage . Open Workspaces and select the workspace you want to access from Atlan. Click Access . In the panel: Enter the name of your security group where it says Enter email addresses Choose one of the following roles: Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Click Add . Enter the name of your security group where it says Enter email addresses Choose one of the following roles: Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Click Add . You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks - you may not have access yourself. To enable both admin and non-admin API access: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Developer settings : Expand Service principals can use Fabric APIs and set to Enabled Add your security group under Specific security groups Click Apply Expand Service principals can use Fabric APIs and set to Enabled Add your security group under Specific security groups Click Apply Add your security group under Specific security groups Under Admin API settings : Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Add your security group After making these changes, you typically need to wait 15-30 minutes for the settings to take effect across Microsoft's services. Delegated user authentication Atlan doesn't recommend using delegated user authentication as it's also not recommended by Microsoft. You need your Microsoft 365 administrator to complete these steps - you may not have access yourself. To assign the delegated user to the Fabric Administrator role: Open the Microsoft 365 admin portal . Click Users and then Active users from the left menu. Select the delegated user. Under Roles , click Manage roles . Expand Show all by category . Under Collaboration , select Fabric Administrator . Click Save changes . You need your Cloud Application Administrator or Application Administrator to complete these steps, you may not have access yourself. The following permissions are only required for delegated user authentication. If using service principal authentication, you don't need to configure any delegated permissions for a service principal it's recommended that you avoid adding these permissions. These are never used and can cause errors that may be hard to troubleshoot. To add permissions for the registered application : In your app registration, click API permissions under the Manage section. Click Add a permission . Click Delegated permissions and select: Capacity.Read.All Dataflow.Read.All Dataset.Read.All Report.Read.All Tenant.Read.All Workspace.Read.All Click Grant Admin consent (If you only see the Add permissions button, you aren't an administrator). You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks, you may not have access yourself. To enable the Microsoft Power BI admin API: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Admin API settings : Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply . Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply . Add your security group Click Apply . Before you begin Configure authentication options"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/business-intelligence\/microsoft-power-bi\/how-tos\/set-up-microsoft-power-bi#before-you-begin","title":"Set up Microsoft Power BI | Atlan Documentation","text":"Depending on the authentication method you choose, you may need a combination of your Cloud Application Administrator or Application Administrator for Microsoft Entra ID, Microsoft 365 administrator for Microsoft 365, and Fabric Administrator ( formerly known as Power BI Administrator ) for Microsoft Power BI to complete these tasks -> you may not have access yourself. This guide outlines how to set up Microsoft Power BI so it can connect with Atlan for metadata extraction and lineage tracking. Before you begin Register application in Microsoft Entra ID You need your Cloud Application Administrator or Application Administrator to complete these steps > you may not have access yourself. This is required if the creation of registered applications isn't enabled for the entire organization. To register a new application in Microsoft Entra ID: Log in to the Azure portal . Click App registrations from the left menu. Click + New registration . Enter a name for your client application and click Register . Application (client) ID Directory (tenant) ID Click Certificates & secrets from the left menu. Under Client secrets , click + New client secret . Enter a description, select an expiry time, and click Add . Copy and securely store the client secret Value . Create security group in Microsoft Entra ID You need your Cloud Application Administrator or Application Administrator to complete these steps - you may not have access yourself. To create a security group for your application: Log in to the Azure portal . Click Groups under the Manage section. Click New group . Set the Group type to Security . Enter a Group name and optional description. Click No members selected . Click Select and then Create . By the end of these steps, you have registered an application with Microsoft Entra ID and created a Security Group with the appropriate member. Configure authentication options Atlan supports two authentication methods for fetching metadata from Microsoft Power BI: Service principal authentication (recommended) When using Service Principal authentication, you must decide how the connector shall access metadata to catalog assets and build lineage. There are two supported options: This option grants permissions that let the service principal to access only admin-level Power BI APIs. In this mode, Atlan extracts metadata exclusively using administrative endpoints. This option is recommended for stricter access control environments. You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks - you may not have access yourself. To configure admin API access: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Admin API settings : Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group under Specific security groups Click Apply Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group under Specific security groups Click Apply Add your security group under Specific security groups Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Add your security group This option grants permissions that let the service principal to access both admin and non-admin Power BI APIs. This enables Atlan to extract richer metadata and build detailed lineage across Power BI assets. You need to be at least a member of the Microsoft Power BI workspace to which you want to add the security group to complete these steps - you may not have access yourself. Make sure that you add the security group from the homepage and not the admin portal. To assign a Microsoft Power BI workspace role to the security group: Open the Microsoft Power BI homepage . Open Workspaces and select the workspace you want to access from Atlan. Click Access . In the panel: Enter the name of your security group where it says Enter email addresses Choose one of the following roles: Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Click Add . Enter the name of your security group where it says Enter email addresses Choose one of the following roles: Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Click Add . You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks - you may not have access yourself. To enable both admin and non-admin API access: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Developer settings : Expand Service principals can use Fabric APIs and set to Enabled Add your security group under Specific security groups Click Apply Expand Service principals can use Fabric APIs and set to Enabled Add your security group under Specific security groups Click Apply Add your security group under Specific security groups Under Admin API settings : Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Add your security group After making these changes, you typically need to wait 15-30 minutes for the settings to take effect across Microsoft's services. Delegated user authentication Atlan doesn't recommend using delegated user authentication as it's also not recommended by Microsoft. You need your Microsoft 365 administrator to complete these steps - you may not have access yourself. To assign the delegated user to the Fabric Administrator role: Open the Microsoft 365 admin portal . Click Users and then Active users from the left menu. Select the delegated user. Under Roles , click Manage roles . Expand Show all by category . Under Collaboration , select Fabric Administrator . Click Save changes . You need your Cloud Application Administrator or Application Administrator to complete these steps, you may not have access yourself. The following permissions are only required for delegated user authentication. If using service principal authentication, you don't need to configure any delegated permissions for a service principal it's recommended that you avoid adding these permissions. These are never used and can cause errors that may be hard to troubleshoot. To add permissions for the registered application : In your app registration, click API permissions under the Manage section. Click Add a permission . Click Delegated permissions and select: Capacity.Read.All Dataflow.Read.All Dataset.Read.All Report.Read.All Tenant.Read.All Workspace.Read.All Click Grant Admin consent (If you only see the Add permissions button, you aren't an administrator). You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks, you may not have access yourself. To enable the Microsoft Power BI admin API: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Admin API settings : Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply . Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply . Add your security group Click Apply . Before you begin Configure authentication options"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/business-intelligence\/microsoft-power-bi\/how-tos\/set-up-microsoft-power-bi#register-application-in-microsoft-entra-id","title":"Set up Microsoft Power BI | Atlan Documentation","text":"Depending on the authentication method you choose, you may need a combination of your Cloud Application Administrator or Application Administrator for Microsoft Entra ID, Microsoft 365 administrator for Microsoft 365, and Fabric Administrator ( formerly known as Power BI Administrator ) for Microsoft Power BI to complete these tasks -> you may not have access yourself. This guide outlines how to set up Microsoft Power BI so it can connect with Atlan for metadata extraction and lineage tracking. Before you begin Register application in Microsoft Entra ID You need your Cloud Application Administrator or Application Administrator to complete these steps > you may not have access yourself. This is required if the creation of registered applications isn't enabled for the entire organization. To register a new application in Microsoft Entra ID: Log in to the Azure portal . Click App registrations from the left menu. Click + New registration . Enter a name for your client application and click Register . Application (client) ID Directory (tenant) ID Click Certificates & secrets from the left menu. Under Client secrets , click + New client secret . Enter a description, select an expiry time, and click Add . Copy and securely store the client secret Value . Create security group in Microsoft Entra ID You need your Cloud Application Administrator or Application Administrator to complete these steps - you may not have access yourself. To create a security group for your application: Log in to the Azure portal . Click Groups under the Manage section. Click New group . Set the Group type to Security . Enter a Group name and optional description. Click No members selected . Click Select and then Create . By the end of these steps, you have registered an application with Microsoft Entra ID and created a Security Group with the appropriate member. Configure authentication options Atlan supports two authentication methods for fetching metadata from Microsoft Power BI: Service principal authentication (recommended) When using Service Principal authentication, you must decide how the connector shall access metadata to catalog assets and build lineage. There are two supported options: This option grants permissions that let the service principal to access only admin-level Power BI APIs. In this mode, Atlan extracts metadata exclusively using administrative endpoints. This option is recommended for stricter access control environments. You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks - you may not have access yourself. To configure admin API access: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Admin API settings : Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group under Specific security groups Click Apply Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group under Specific security groups Click Apply Add your security group under Specific security groups Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Add your security group This option grants permissions that let the service principal to access both admin and non-admin Power BI APIs. This enables Atlan to extract richer metadata and build detailed lineage across Power BI assets. You need to be at least a member of the Microsoft Power BI workspace to which you want to add the security group to complete these steps - you may not have access yourself. Make sure that you add the security group from the homepage and not the admin portal. To assign a Microsoft Power BI workspace role to the security group: Open the Microsoft Power BI homepage . Open Workspaces and select the workspace you want to access from Atlan. Click Access . In the panel: Enter the name of your security group where it says Enter email addresses Choose one of the following roles: Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Click Add . Enter the name of your security group where it says Enter email addresses Choose one of the following roles: Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Click Add . You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks - you may not have access yourself. To enable both admin and non-admin API access: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Developer settings : Expand Service principals can use Fabric APIs and set to Enabled Add your security group under Specific security groups Click Apply Expand Service principals can use Fabric APIs and set to Enabled Add your security group under Specific security groups Click Apply Add your security group under Specific security groups Under Admin API settings : Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Add your security group After making these changes, you typically need to wait 15-30 minutes for the settings to take effect across Microsoft's services. Delegated user authentication Atlan doesn't recommend using delegated user authentication as it's also not recommended by Microsoft. You need your Microsoft 365 administrator to complete these steps - you may not have access yourself. To assign the delegated user to the Fabric Administrator role: Open the Microsoft 365 admin portal . Click Users and then Active users from the left menu. Select the delegated user. Under Roles , click Manage roles . Expand Show all by category . Under Collaboration , select Fabric Administrator . Click Save changes . You need your Cloud Application Administrator or Application Administrator to complete these steps, you may not have access yourself. The following permissions are only required for delegated user authentication. If using service principal authentication, you don't need to configure any delegated permissions for a service principal it's recommended that you avoid adding these permissions. These are never used and can cause errors that may be hard to troubleshoot. To add permissions for the registered application : In your app registration, click API permissions under the Manage section. Click Add a permission . Click Delegated permissions and select: Capacity.Read.All Dataflow.Read.All Dataset.Read.All Report.Read.All Tenant.Read.All Workspace.Read.All Click Grant Admin consent (If you only see the Add permissions button, you aren't an administrator). You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks, you may not have access yourself. To enable the Microsoft Power BI admin API: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Admin API settings : Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply . Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply . Add your security group Click Apply . Before you begin Configure authentication options"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/business-intelligence\/microsoft-power-bi\/how-tos\/set-up-microsoft-power-bi#create-security-group-in-microsoft-entra-id","title":"Set up Microsoft Power BI | Atlan Documentation","text":"Depending on the authentication method you choose, you may need a combination of your Cloud Application Administrator or Application Administrator for Microsoft Entra ID, Microsoft 365 administrator for Microsoft 365, and Fabric Administrator ( formerly known as Power BI Administrator ) for Microsoft Power BI to complete these tasks -> you may not have access yourself. This guide outlines how to set up Microsoft Power BI so it can connect with Atlan for metadata extraction and lineage tracking. Before you begin Register application in Microsoft Entra ID You need your Cloud Application Administrator or Application Administrator to complete these steps > you may not have access yourself. This is required if the creation of registered applications isn't enabled for the entire organization. To register a new application in Microsoft Entra ID: Log in to the Azure portal . Click App registrations from the left menu. Click + New registration . Enter a name for your client application and click Register . Application (client) ID Directory (tenant) ID Click Certificates & secrets from the left menu. Under Client secrets , click + New client secret . Enter a description, select an expiry time, and click Add . Copy and securely store the client secret Value . Create security group in Microsoft Entra ID You need your Cloud Application Administrator or Application Administrator to complete these steps - you may not have access yourself. To create a security group for your application: Log in to the Azure portal . Click Groups under the Manage section. Click New group . Set the Group type to Security . Enter a Group name and optional description. Click No members selected . Click Select and then Create . By the end of these steps, you have registered an application with Microsoft Entra ID and created a Security Group with the appropriate member. Configure authentication options Atlan supports two authentication methods for fetching metadata from Microsoft Power BI: Service principal authentication (recommended) When using Service Principal authentication, you must decide how the connector shall access metadata to catalog assets and build lineage. There are two supported options: This option grants permissions that let the service principal to access only admin-level Power BI APIs. In this mode, Atlan extracts metadata exclusively using administrative endpoints. This option is recommended for stricter access control environments. You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks - you may not have access yourself. To configure admin API access: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Admin API settings : Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group under Specific security groups Click Apply Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group under Specific security groups Click Apply Add your security group under Specific security groups Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Add your security group This option grants permissions that let the service principal to access both admin and non-admin Power BI APIs. This enables Atlan to extract richer metadata and build detailed lineage across Power BI assets. You need to be at least a member of the Microsoft Power BI workspace to which you want to add the security group to complete these steps - you may not have access yourself. Make sure that you add the security group from the homepage and not the admin portal. To assign a Microsoft Power BI workspace role to the security group: Open the Microsoft Power BI homepage . Open Workspaces and select the workspace you want to access from Atlan. Click Access . In the panel: Enter the name of your security group where it says Enter email addresses Choose one of the following roles: Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Click Add . Enter the name of your security group where it says Enter email addresses Choose one of the following roles: Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Click Add . You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks - you may not have access yourself. To enable both admin and non-admin API access: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Developer settings : Expand Service principals can use Fabric APIs and set to Enabled Add your security group under Specific security groups Click Apply Expand Service principals can use Fabric APIs and set to Enabled Add your security group under Specific security groups Click Apply Add your security group under Specific security groups Under Admin API settings : Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Add your security group After making these changes, you typically need to wait 15-30 minutes for the settings to take effect across Microsoft's services. Delegated user authentication Atlan doesn't recommend using delegated user authentication as it's also not recommended by Microsoft. You need your Microsoft 365 administrator to complete these steps - you may not have access yourself. To assign the delegated user to the Fabric Administrator role: Open the Microsoft 365 admin portal . Click Users and then Active users from the left menu. Select the delegated user. Under Roles , click Manage roles . Expand Show all by category . Under Collaboration , select Fabric Administrator . Click Save changes . You need your Cloud Application Administrator or Application Administrator to complete these steps, you may not have access yourself. The following permissions are only required for delegated user authentication. If using service principal authentication, you don't need to configure any delegated permissions for a service principal it's recommended that you avoid adding these permissions. These are never used and can cause errors that may be hard to troubleshoot. To add permissions for the registered application : In your app registration, click API permissions under the Manage section. Click Add a permission . Click Delegated permissions and select: Capacity.Read.All Dataflow.Read.All Dataset.Read.All Report.Read.All Tenant.Read.All Workspace.Read.All Click Grant Admin consent (If you only see the Add permissions button, you aren't an administrator). You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks, you may not have access yourself. To enable the Microsoft Power BI admin API: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Admin API settings : Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply . Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply . Add your security group Click Apply . Before you begin Configure authentication options"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/business-intelligence\/microsoft-power-bi\/how-tos\/set-up-microsoft-power-bi#configure-authentication-options","title":"Set up Microsoft Power BI | Atlan Documentation","text":"Depending on the authentication method you choose, you may need a combination of your Cloud Application Administrator or Application Administrator for Microsoft Entra ID, Microsoft 365 administrator for Microsoft 365, and Fabric Administrator ( formerly known as Power BI Administrator ) for Microsoft Power BI to complete these tasks -> you may not have access yourself. This guide outlines how to set up Microsoft Power BI so it can connect with Atlan for metadata extraction and lineage tracking. Before you begin Register application in Microsoft Entra ID You need your Cloud Application Administrator or Application Administrator to complete these steps > you may not have access yourself. This is required if the creation of registered applications isn't enabled for the entire organization. To register a new application in Microsoft Entra ID: Log in to the Azure portal . Click App registrations from the left menu. Click + New registration . Enter a name for your client application and click Register . Application (client) ID Directory (tenant) ID Click Certificates & secrets from the left menu. Under Client secrets , click + New client secret . Enter a description, select an expiry time, and click Add . Copy and securely store the client secret Value . Create security group in Microsoft Entra ID You need your Cloud Application Administrator or Application Administrator to complete these steps - you may not have access yourself. To create a security group for your application: Log in to the Azure portal . Click Groups under the Manage section. Click New group . Set the Group type to Security . Enter a Group name and optional description. Click No members selected . Click Select and then Create . By the end of these steps, you have registered an application with Microsoft Entra ID and created a Security Group with the appropriate member. Configure authentication options Atlan supports two authentication methods for fetching metadata from Microsoft Power BI: Service principal authentication (recommended) When using Service Principal authentication, you must decide how the connector shall access metadata to catalog assets and build lineage. There are two supported options: This option grants permissions that let the service principal to access only admin-level Power BI APIs. In this mode, Atlan extracts metadata exclusively using administrative endpoints. This option is recommended for stricter access control environments. You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks - you may not have access yourself. To configure admin API access: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Admin API settings : Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group under Specific security groups Click Apply Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group under Specific security groups Click Apply Add your security group under Specific security groups Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Add your security group This option grants permissions that let the service principal to access both admin and non-admin Power BI APIs. This enables Atlan to extract richer metadata and build detailed lineage across Power BI assets. You need to be at least a member of the Microsoft Power BI workspace to which you want to add the security group to complete these steps - you may not have access yourself. Make sure that you add the security group from the homepage and not the admin portal. To assign a Microsoft Power BI workspace role to the security group: Open the Microsoft Power BI homepage . Open Workspaces and select the workspace you want to access from Atlan. Click Access . In the panel: Enter the name of your security group where it says Enter email addresses Choose one of the following roles: Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Click Add . Enter the name of your security group where it says Enter email addresses Choose one of the following roles: Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Click Add . You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks - you may not have access yourself. To enable both admin and non-admin API access: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Developer settings : Expand Service principals can use Fabric APIs and set to Enabled Add your security group under Specific security groups Click Apply Expand Service principals can use Fabric APIs and set to Enabled Add your security group under Specific security groups Click Apply Add your security group under Specific security groups Under Admin API settings : Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Add your security group After making these changes, you typically need to wait 15-30 minutes for the settings to take effect across Microsoft's services. Delegated user authentication Atlan doesn't recommend using delegated user authentication as it's also not recommended by Microsoft. You need your Microsoft 365 administrator to complete these steps - you may not have access yourself. To assign the delegated user to the Fabric Administrator role: Open the Microsoft 365 admin portal . Click Users and then Active users from the left menu. Select the delegated user. Under Roles , click Manage roles . Expand Show all by category . Under Collaboration , select Fabric Administrator . Click Save changes . You need your Cloud Application Administrator or Application Administrator to complete these steps, you may not have access yourself. The following permissions are only required for delegated user authentication. If using service principal authentication, you don't need to configure any delegated permissions for a service principal it's recommended that you avoid adding these permissions. These are never used and can cause errors that may be hard to troubleshoot. To add permissions for the registered application : In your app registration, click API permissions under the Manage section. Click Add a permission . Click Delegated permissions and select: Capacity.Read.All Dataflow.Read.All Dataset.Read.All Report.Read.All Tenant.Read.All Workspace.Read.All Click Grant Admin consent (If you only see the Add permissions button, you aren't an administrator). You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks, you may not have access yourself. To enable the Microsoft Power BI admin API: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Admin API settings : Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply . Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply . Add your security group Click Apply . Before you begin Configure authentication options"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/business-intelligence\/microsoft-power-bi\/how-tos\/set-up-microsoft-power-bi#service-principal-authentication-recommended","title":"Set up Microsoft Power BI | Atlan Documentation","text":"Depending on the authentication method you choose, you may need a combination of your Cloud Application Administrator or Application Administrator for Microsoft Entra ID, Microsoft 365 administrator for Microsoft 365, and Fabric Administrator ( formerly known as Power BI Administrator ) for Microsoft Power BI to complete these tasks -> you may not have access yourself. This guide outlines how to set up Microsoft Power BI so it can connect with Atlan for metadata extraction and lineage tracking. Before you begin Register application in Microsoft Entra ID You need your Cloud Application Administrator or Application Administrator to complete these steps > you may not have access yourself. This is required if the creation of registered applications isn't enabled for the entire organization. To register a new application in Microsoft Entra ID: Log in to the Azure portal . Click App registrations from the left menu. Click + New registration . Enter a name for your client application and click Register . Application (client) ID Directory (tenant) ID Click Certificates & secrets from the left menu. Under Client secrets , click + New client secret . Enter a description, select an expiry time, and click Add . Copy and securely store the client secret Value . Create security group in Microsoft Entra ID You need your Cloud Application Administrator or Application Administrator to complete these steps - you may not have access yourself. To create a security group for your application: Log in to the Azure portal . Click Groups under the Manage section. Click New group . Set the Group type to Security . Enter a Group name and optional description. Click No members selected . Click Select and then Create . By the end of these steps, you have registered an application with Microsoft Entra ID and created a Security Group with the appropriate member. Configure authentication options Atlan supports two authentication methods for fetching metadata from Microsoft Power BI: Service principal authentication (recommended) When using Service Principal authentication, you must decide how the connector shall access metadata to catalog assets and build lineage. There are two supported options: This option grants permissions that let the service principal to access only admin-level Power BI APIs. In this mode, Atlan extracts metadata exclusively using administrative endpoints. This option is recommended for stricter access control environments. You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks - you may not have access yourself. To configure admin API access: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Admin API settings : Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group under Specific security groups Click Apply Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group under Specific security groups Click Apply Add your security group under Specific security groups Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Add your security group This option grants permissions that let the service principal to access both admin and non-admin Power BI APIs. This enables Atlan to extract richer metadata and build detailed lineage across Power BI assets. You need to be at least a member of the Microsoft Power BI workspace to which you want to add the security group to complete these steps - you may not have access yourself. Make sure that you add the security group from the homepage and not the admin portal. To assign a Microsoft Power BI workspace role to the security group: Open the Microsoft Power BI homepage . Open Workspaces and select the workspace you want to access from Atlan. Click Access . In the panel: Enter the name of your security group where it says Enter email addresses Choose one of the following roles: Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Click Add . Enter the name of your security group where it says Enter email addresses Choose one of the following roles: Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Click Add . You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks - you may not have access yourself. To enable both admin and non-admin API access: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Developer settings : Expand Service principals can use Fabric APIs and set to Enabled Add your security group under Specific security groups Click Apply Expand Service principals can use Fabric APIs and set to Enabled Add your security group under Specific security groups Click Apply Add your security group under Specific security groups Under Admin API settings : Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Add your security group After making these changes, you typically need to wait 15-30 minutes for the settings to take effect across Microsoft's services. Delegated user authentication Atlan doesn't recommend using delegated user authentication as it's also not recommended by Microsoft. You need your Microsoft 365 administrator to complete these steps - you may not have access yourself. To assign the delegated user to the Fabric Administrator role: Open the Microsoft 365 admin portal . Click Users and then Active users from the left menu. Select the delegated user. Under Roles , click Manage roles . Expand Show all by category . Under Collaboration , select Fabric Administrator . Click Save changes . You need your Cloud Application Administrator or Application Administrator to complete these steps, you may not have access yourself. The following permissions are only required for delegated user authentication. If using service principal authentication, you don't need to configure any delegated permissions for a service principal it's recommended that you avoid adding these permissions. These are never used and can cause errors that may be hard to troubleshoot. To add permissions for the registered application : In your app registration, click API permissions under the Manage section. Click Add a permission . Click Delegated permissions and select: Capacity.Read.All Dataflow.Read.All Dataset.Read.All Report.Read.All Tenant.Read.All Workspace.Read.All Click Grant Admin consent (If you only see the Add permissions button, you aren't an administrator). You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks, you may not have access yourself. To enable the Microsoft Power BI admin API: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Admin API settings : Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply . Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply . Add your security group Click Apply . Before you begin Configure authentication options"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/business-intelligence\/microsoft-power-bi\/how-tos\/set-up-microsoft-power-bi#admin-api-only","title":"Set up Microsoft Power BI | Atlan Documentation","text":"Depending on the authentication method you choose, you may need a combination of your Cloud Application Administrator or Application Administrator for Microsoft Entra ID, Microsoft 365 administrator for Microsoft 365, and Fabric Administrator ( formerly known as Power BI Administrator ) for Microsoft Power BI to complete these tasks -> you may not have access yourself. This guide outlines how to set up Microsoft Power BI so it can connect with Atlan for metadata extraction and lineage tracking. Before you begin Register application in Microsoft Entra ID You need your Cloud Application Administrator or Application Administrator to complete these steps > you may not have access yourself. This is required if the creation of registered applications isn't enabled for the entire organization. To register a new application in Microsoft Entra ID: Log in to the Azure portal . Click App registrations from the left menu. Click + New registration . Enter a name for your client application and click Register . Application (client) ID Directory (tenant) ID Click Certificates & secrets from the left menu. Under Client secrets , click + New client secret . Enter a description, select an expiry time, and click Add . Copy and securely store the client secret Value . Create security group in Microsoft Entra ID You need your Cloud Application Administrator or Application Administrator to complete these steps - you may not have access yourself. To create a security group for your application: Log in to the Azure portal . Click Groups under the Manage section. Click New group . Set the Group type to Security . Enter a Group name and optional description. Click No members selected . Click Select and then Create . By the end of these steps, you have registered an application with Microsoft Entra ID and created a Security Group with the appropriate member. Configure authentication options Atlan supports two authentication methods for fetching metadata from Microsoft Power BI: Service principal authentication (recommended) When using Service Principal authentication, you must decide how the connector shall access metadata to catalog assets and build lineage. There are two supported options: This option grants permissions that let the service principal to access only admin-level Power BI APIs. In this mode, Atlan extracts metadata exclusively using administrative endpoints. This option is recommended for stricter access control environments. You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks - you may not have access yourself. To configure admin API access: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Admin API settings : Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group under Specific security groups Click Apply Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group under Specific security groups Click Apply Add your security group under Specific security groups Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Add your security group This option grants permissions that let the service principal to access both admin and non-admin Power BI APIs. This enables Atlan to extract richer metadata and build detailed lineage across Power BI assets. You need to be at least a member of the Microsoft Power BI workspace to which you want to add the security group to complete these steps - you may not have access yourself. Make sure that you add the security group from the homepage and not the admin portal. To assign a Microsoft Power BI workspace role to the security group: Open the Microsoft Power BI homepage . Open Workspaces and select the workspace you want to access from Atlan. Click Access . In the panel: Enter the name of your security group where it says Enter email addresses Choose one of the following roles: Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Click Add . Enter the name of your security group where it says Enter email addresses Choose one of the following roles: Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Click Add . You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks - you may not have access yourself. To enable both admin and non-admin API access: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Developer settings : Expand Service principals can use Fabric APIs and set to Enabled Add your security group under Specific security groups Click Apply Expand Service principals can use Fabric APIs and set to Enabled Add your security group under Specific security groups Click Apply Add your security group under Specific security groups Under Admin API settings : Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Add your security group After making these changes, you typically need to wait 15-30 minutes for the settings to take effect across Microsoft's services. Delegated user authentication Atlan doesn't recommend using delegated user authentication as it's also not recommended by Microsoft. You need your Microsoft 365 administrator to complete these steps - you may not have access yourself. To assign the delegated user to the Fabric Administrator role: Open the Microsoft 365 admin portal . Click Users and then Active users from the left menu. Select the delegated user. Under Roles , click Manage roles . Expand Show all by category . Under Collaboration , select Fabric Administrator . Click Save changes . You need your Cloud Application Administrator or Application Administrator to complete these steps, you may not have access yourself. The following permissions are only required for delegated user authentication. If using service principal authentication, you don't need to configure any delegated permissions for a service principal it's recommended that you avoid adding these permissions. These are never used and can cause errors that may be hard to troubleshoot. To add permissions for the registered application : In your app registration, click API permissions under the Manage section. Click Add a permission . Click Delegated permissions and select: Capacity.Read.All Dataflow.Read.All Dataset.Read.All Report.Read.All Tenant.Read.All Workspace.Read.All Click Grant Admin consent (If you only see the Add permissions button, you aren't an administrator). You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks, you may not have access yourself. To enable the Microsoft Power BI admin API: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Admin API settings : Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply . Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply . Add your security group Click Apply . Before you begin Configure authentication options"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/business-intelligence\/microsoft-power-bi\/how-tos\/set-up-microsoft-power-bi#admin-and-non-admin-apis","title":"Set up Microsoft Power BI | Atlan Documentation","text":"Depending on the authentication method you choose, you may need a combination of your Cloud Application Administrator or Application Administrator for Microsoft Entra ID, Microsoft 365 administrator for Microsoft 365, and Fabric Administrator ( formerly known as Power BI Administrator ) for Microsoft Power BI to complete these tasks -> you may not have access yourself. This guide outlines how to set up Microsoft Power BI so it can connect with Atlan for metadata extraction and lineage tracking. Before you begin Register application in Microsoft Entra ID You need your Cloud Application Administrator or Application Administrator to complete these steps > you may not have access yourself. This is required if the creation of registered applications isn't enabled for the entire organization. To register a new application in Microsoft Entra ID: Log in to the Azure portal . Click App registrations from the left menu. Click + New registration . Enter a name for your client application and click Register . Application (client) ID Directory (tenant) ID Click Certificates & secrets from the left menu. Under Client secrets , click + New client secret . Enter a description, select an expiry time, and click Add . Copy and securely store the client secret Value . Create security group in Microsoft Entra ID You need your Cloud Application Administrator or Application Administrator to complete these steps - you may not have access yourself. To create a security group for your application: Log in to the Azure portal . Click Groups under the Manage section. Click New group . Set the Group type to Security . Enter a Group name and optional description. Click No members selected . Click Select and then Create . By the end of these steps, you have registered an application with Microsoft Entra ID and created a Security Group with the appropriate member. Configure authentication options Atlan supports two authentication methods for fetching metadata from Microsoft Power BI: Service principal authentication (recommended) When using Service Principal authentication, you must decide how the connector shall access metadata to catalog assets and build lineage. There are two supported options: This option grants permissions that let the service principal to access only admin-level Power BI APIs. In this mode, Atlan extracts metadata exclusively using administrative endpoints. This option is recommended for stricter access control environments. You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks - you may not have access yourself. To configure admin API access: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Admin API settings : Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group under Specific security groups Click Apply Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group under Specific security groups Click Apply Add your security group under Specific security groups Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Add your security group This option grants permissions that let the service principal to access both admin and non-admin Power BI APIs. This enables Atlan to extract richer metadata and build detailed lineage across Power BI assets. You need to be at least a member of the Microsoft Power BI workspace to which you want to add the security group to complete these steps - you may not have access yourself. Make sure that you add the security group from the homepage and not the admin portal. To assign a Microsoft Power BI workspace role to the security group: Open the Microsoft Power BI homepage . Open Workspaces and select the workspace you want to access from Atlan. Click Access . In the panel: Enter the name of your security group where it says Enter email addresses Choose one of the following roles: Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Click Add . Enter the name of your security group where it says Enter email addresses Choose one of the following roles: Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Click Add . You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks - you may not have access yourself. To enable both admin and non-admin API access: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Developer settings : Expand Service principals can use Fabric APIs and set to Enabled Add your security group under Specific security groups Click Apply Expand Service principals can use Fabric APIs and set to Enabled Add your security group under Specific security groups Click Apply Add your security group under Specific security groups Under Admin API settings : Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Add your security group After making these changes, you typically need to wait 15-30 minutes for the settings to take effect across Microsoft's services. Delegated user authentication Atlan doesn't recommend using delegated user authentication as it's also not recommended by Microsoft. You need your Microsoft 365 administrator to complete these steps - you may not have access yourself. To assign the delegated user to the Fabric Administrator role: Open the Microsoft 365 admin portal . Click Users and then Active users from the left menu. Select the delegated user. Under Roles , click Manage roles . Expand Show all by category . Under Collaboration , select Fabric Administrator . Click Save changes . You need your Cloud Application Administrator or Application Administrator to complete these steps, you may not have access yourself. The following permissions are only required for delegated user authentication. If using service principal authentication, you don't need to configure any delegated permissions for a service principal it's recommended that you avoid adding these permissions. These are never used and can cause errors that may be hard to troubleshoot. To add permissions for the registered application : In your app registration, click API permissions under the Manage section. Click Add a permission . Click Delegated permissions and select: Capacity.Read.All Dataflow.Read.All Dataset.Read.All Report.Read.All Tenant.Read.All Workspace.Read.All Click Grant Admin consent (If you only see the Add permissions button, you aren't an administrator). You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks, you may not have access yourself. To enable the Microsoft Power BI admin API: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Admin API settings : Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply . Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply . Add your security group Click Apply . Before you begin Configure authentication options"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/business-intelligence\/microsoft-power-bi\/how-tos\/set-up-microsoft-power-bi#assign-security-group-to-power-bi-workspaces-in-powerbi-service-portal","title":"Set up Microsoft Power BI | Atlan Documentation","text":"Depending on the authentication method you choose, you may need a combination of your Cloud Application Administrator or Application Administrator for Microsoft Entra ID, Microsoft 365 administrator for Microsoft 365, and Fabric Administrator ( formerly known as Power BI Administrator ) for Microsoft Power BI to complete these tasks -> you may not have access yourself. This guide outlines how to set up Microsoft Power BI so it can connect with Atlan for metadata extraction and lineage tracking. Before you begin Register application in Microsoft Entra ID You need your Cloud Application Administrator or Application Administrator to complete these steps > you may not have access yourself. This is required if the creation of registered applications isn't enabled for the entire organization. To register a new application in Microsoft Entra ID: Log in to the Azure portal . Click App registrations from the left menu. Click + New registration . Enter a name for your client application and click Register . Application (client) ID Directory (tenant) ID Click Certificates & secrets from the left menu. Under Client secrets , click + New client secret . Enter a description, select an expiry time, and click Add . Copy and securely store the client secret Value . Create security group in Microsoft Entra ID You need your Cloud Application Administrator or Application Administrator to complete these steps - you may not have access yourself. To create a security group for your application: Log in to the Azure portal . Click Groups under the Manage section. Click New group . Set the Group type to Security . Enter a Group name and optional description. Click No members selected . Click Select and then Create . By the end of these steps, you have registered an application with Microsoft Entra ID and created a Security Group with the appropriate member. Configure authentication options Atlan supports two authentication methods for fetching metadata from Microsoft Power BI: Service principal authentication (recommended) When using Service Principal authentication, you must decide how the connector shall access metadata to catalog assets and build lineage. There are two supported options: This option grants permissions that let the service principal to access only admin-level Power BI APIs. In this mode, Atlan extracts metadata exclusively using administrative endpoints. This option is recommended for stricter access control environments. You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks - you may not have access yourself. To configure admin API access: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Admin API settings : Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group under Specific security groups Click Apply Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group under Specific security groups Click Apply Add your security group under Specific security groups Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Add your security group This option grants permissions that let the service principal to access both admin and non-admin Power BI APIs. This enables Atlan to extract richer metadata and build detailed lineage across Power BI assets. You need to be at least a member of the Microsoft Power BI workspace to which you want to add the security group to complete these steps - you may not have access yourself. Make sure that you add the security group from the homepage and not the admin portal. To assign a Microsoft Power BI workspace role to the security group: Open the Microsoft Power BI homepage . Open Workspaces and select the workspace you want to access from Atlan. Click Access . In the panel: Enter the name of your security group where it says Enter email addresses Choose one of the following roles: Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Click Add . Enter the name of your security group where it says Enter email addresses Choose one of the following roles: Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Click Add . You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks - you may not have access yourself. To enable both admin and non-admin API access: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Developer settings : Expand Service principals can use Fabric APIs and set to Enabled Add your security group under Specific security groups Click Apply Expand Service principals can use Fabric APIs and set to Enabled Add your security group under Specific security groups Click Apply Add your security group under Specific security groups Under Admin API settings : Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Add your security group After making these changes, you typically need to wait 15-30 minutes for the settings to take effect across Microsoft's services. Delegated user authentication Atlan doesn't recommend using delegated user authentication as it's also not recommended by Microsoft. You need your Microsoft 365 administrator to complete these steps - you may not have access yourself. To assign the delegated user to the Fabric Administrator role: Open the Microsoft 365 admin portal . Click Users and then Active users from the left menu. Select the delegated user. Under Roles , click Manage roles . Expand Show all by category . Under Collaboration , select Fabric Administrator . Click Save changes . You need your Cloud Application Administrator or Application Administrator to complete these steps, you may not have access yourself. The following permissions are only required for delegated user authentication. If using service principal authentication, you don't need to configure any delegated permissions for a service principal it's recommended that you avoid adding these permissions. These are never used and can cause errors that may be hard to troubleshoot. To add permissions for the registered application : In your app registration, click API permissions under the Manage section. Click Add a permission . Click Delegated permissions and select: Capacity.Read.All Dataflow.Read.All Dataset.Read.All Report.Read.All Tenant.Read.All Workspace.Read.All Click Grant Admin consent (If you only see the Add permissions button, you aren't an administrator). You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks, you may not have access yourself. To enable the Microsoft Power BI admin API: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Admin API settings : Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply . Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply . Add your security group Click Apply . Before you begin Configure authentication options"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/business-intelligence\/microsoft-power-bi\/how-tos\/set-up-microsoft-power-bi#configure-admin-and-non-admin-api-access-in-powerbi-service-portal","title":"Set up Microsoft Power BI | Atlan Documentation","text":"Depending on the authentication method you choose, you may need a combination of your Cloud Application Administrator or Application Administrator for Microsoft Entra ID, Microsoft 365 administrator for Microsoft 365, and Fabric Administrator ( formerly known as Power BI Administrator ) for Microsoft Power BI to complete these tasks -> you may not have access yourself. This guide outlines how to set up Microsoft Power BI so it can connect with Atlan for metadata extraction and lineage tracking. Before you begin Register application in Microsoft Entra ID You need your Cloud Application Administrator or Application Administrator to complete these steps > you may not have access yourself. This is required if the creation of registered applications isn't enabled for the entire organization. To register a new application in Microsoft Entra ID: Log in to the Azure portal . Click App registrations from the left menu. Click + New registration . Enter a name for your client application and click Register . Application (client) ID Directory (tenant) ID Click Certificates & secrets from the left menu. Under Client secrets , click + New client secret . Enter a description, select an expiry time, and click Add . Copy and securely store the client secret Value . Create security group in Microsoft Entra ID You need your Cloud Application Administrator or Application Administrator to complete these steps - you may not have access yourself. To create a security group for your application: Log in to the Azure portal . Click Groups under the Manage section. Click New group . Set the Group type to Security . Enter a Group name and optional description. Click No members selected . Click Select and then Create . By the end of these steps, you have registered an application with Microsoft Entra ID and created a Security Group with the appropriate member. Configure authentication options Atlan supports two authentication methods for fetching metadata from Microsoft Power BI: Service principal authentication (recommended) When using Service Principal authentication, you must decide how the connector shall access metadata to catalog assets and build lineage. There are two supported options: This option grants permissions that let the service principal to access only admin-level Power BI APIs. In this mode, Atlan extracts metadata exclusively using administrative endpoints. This option is recommended for stricter access control environments. You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks - you may not have access yourself. To configure admin API access: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Admin API settings : Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group under Specific security groups Click Apply Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group under Specific security groups Click Apply Add your security group under Specific security groups Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Add your security group This option grants permissions that let the service principal to access both admin and non-admin Power BI APIs. This enables Atlan to extract richer metadata and build detailed lineage across Power BI assets. You need to be at least a member of the Microsoft Power BI workspace to which you want to add the security group to complete these steps - you may not have access yourself. Make sure that you add the security group from the homepage and not the admin portal. To assign a Microsoft Power BI workspace role to the security group: Open the Microsoft Power BI homepage . Open Workspaces and select the workspace you want to access from Atlan. Click Access . In the panel: Enter the name of your security group where it says Enter email addresses Choose one of the following roles: Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Click Add . Enter the name of your security group where it says Enter email addresses Choose one of the following roles: Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Click Add . You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks - you may not have access yourself. To enable both admin and non-admin API access: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Developer settings : Expand Service principals can use Fabric APIs and set to Enabled Add your security group under Specific security groups Click Apply Expand Service principals can use Fabric APIs and set to Enabled Add your security group under Specific security groups Click Apply Add your security group under Specific security groups Under Admin API settings : Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Add your security group After making these changes, you typically need to wait 15-30 minutes for the settings to take effect across Microsoft's services. Delegated user authentication Atlan doesn't recommend using delegated user authentication as it's also not recommended by Microsoft. You need your Microsoft 365 administrator to complete these steps - you may not have access yourself. To assign the delegated user to the Fabric Administrator role: Open the Microsoft 365 admin portal . Click Users and then Active users from the left menu. Select the delegated user. Under Roles , click Manage roles . Expand Show all by category . Under Collaboration , select Fabric Administrator . Click Save changes . You need your Cloud Application Administrator or Application Administrator to complete these steps, you may not have access yourself. The following permissions are only required for delegated user authentication. If using service principal authentication, you don't need to configure any delegated permissions for a service principal it's recommended that you avoid adding these permissions. These are never used and can cause errors that may be hard to troubleshoot. To add permissions for the registered application : In your app registration, click API permissions under the Manage section. Click Add a permission . Click Delegated permissions and select: Capacity.Read.All Dataflow.Read.All Dataset.Read.All Report.Read.All Tenant.Read.All Workspace.Read.All Click Grant Admin consent (If you only see the Add permissions button, you aren't an administrator). You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks, you may not have access yourself. To enable the Microsoft Power BI admin API: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Admin API settings : Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply . Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply . Add your security group Click Apply . Before you begin Configure authentication options"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/business-intelligence\/microsoft-power-bi\/how-tos\/set-up-microsoft-power-bi#delegated-user-authentication","title":"Set up Microsoft Power BI | Atlan Documentation","text":"Depending on the authentication method you choose, you may need a combination of your Cloud Application Administrator or Application Administrator for Microsoft Entra ID, Microsoft 365 administrator for Microsoft 365, and Fabric Administrator ( formerly known as Power BI Administrator ) for Microsoft Power BI to complete these tasks -> you may not have access yourself. This guide outlines how to set up Microsoft Power BI so it can connect with Atlan for metadata extraction and lineage tracking. Before you begin Register application in Microsoft Entra ID You need your Cloud Application Administrator or Application Administrator to complete these steps > you may not have access yourself. This is required if the creation of registered applications isn't enabled for the entire organization. To register a new application in Microsoft Entra ID: Log in to the Azure portal . Click App registrations from the left menu. Click + New registration . Enter a name for your client application and click Register . Application (client) ID Directory (tenant) ID Click Certificates & secrets from the left menu. Under Client secrets , click + New client secret . Enter a description, select an expiry time, and click Add . Copy and securely store the client secret Value . Create security group in Microsoft Entra ID You need your Cloud Application Administrator or Application Administrator to complete these steps - you may not have access yourself. To create a security group for your application: Log in to the Azure portal . Click Groups under the Manage section. Click New group . Set the Group type to Security . Enter a Group name and optional description. Click No members selected . Click Select and then Create . By the end of these steps, you have registered an application with Microsoft Entra ID and created a Security Group with the appropriate member. Configure authentication options Atlan supports two authentication methods for fetching metadata from Microsoft Power BI: Service principal authentication (recommended) When using Service Principal authentication, you must decide how the connector shall access metadata to catalog assets and build lineage. There are two supported options: This option grants permissions that let the service principal to access only admin-level Power BI APIs. In this mode, Atlan extracts metadata exclusively using administrative endpoints. This option is recommended for stricter access control environments. You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks - you may not have access yourself. To configure admin API access: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Admin API settings : Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group under Specific security groups Click Apply Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group under Specific security groups Click Apply Add your security group under Specific security groups Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Add your security group This option grants permissions that let the service principal to access both admin and non-admin Power BI APIs. This enables Atlan to extract richer metadata and build detailed lineage across Power BI assets. You need to be at least a member of the Microsoft Power BI workspace to which you want to add the security group to complete these steps - you may not have access yourself. Make sure that you add the security group from the homepage and not the admin portal. To assign a Microsoft Power BI workspace role to the security group: Open the Microsoft Power BI homepage . Open Workspaces and select the workspace you want to access from Atlan. Click Access . In the panel: Enter the name of your security group where it says Enter email addresses Choose one of the following roles: Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Click Add . Enter the name of your security group where it says Enter email addresses Choose one of the following roles: Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Click Add . You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks - you may not have access yourself. To enable both admin and non-admin API access: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Developer settings : Expand Service principals can use Fabric APIs and set to Enabled Add your security group under Specific security groups Click Apply Expand Service principals can use Fabric APIs and set to Enabled Add your security group under Specific security groups Click Apply Add your security group under Specific security groups Under Admin API settings : Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Add your security group After making these changes, you typically need to wait 15-30 minutes for the settings to take effect across Microsoft's services. Delegated user authentication Atlan doesn't recommend using delegated user authentication as it's also not recommended by Microsoft. You need your Microsoft 365 administrator to complete these steps - you may not have access yourself. To assign the delegated user to the Fabric Administrator role: Open the Microsoft 365 admin portal . Click Users and then Active users from the left menu. Select the delegated user. Under Roles , click Manage roles . Expand Show all by category . Under Collaboration , select Fabric Administrator . Click Save changes . You need your Cloud Application Administrator or Application Administrator to complete these steps, you may not have access yourself. The following permissions are only required for delegated user authentication. If using service principal authentication, you don't need to configure any delegated permissions for a service principal it's recommended that you avoid adding these permissions. These are never used and can cause errors that may be hard to troubleshoot. To add permissions for the registered application : In your app registration, click API permissions under the Manage section. Click Add a permission . Click Delegated permissions and select: Capacity.Read.All Dataflow.Read.All Dataset.Read.All Report.Read.All Tenant.Read.All Workspace.Read.All Click Grant Admin consent (If you only see the Add permissions button, you aren't an administrator). You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks, you may not have access yourself. To enable the Microsoft Power BI admin API: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Admin API settings : Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply . Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply . Add your security group Click Apply . Before you begin Configure authentication options"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/business-intelligence\/microsoft-power-bi\/how-tos\/set-up-microsoft-power-bi#fabric-administrator-role-assignment","title":"Set up Microsoft Power BI | Atlan Documentation","text":"Depending on the authentication method you choose, you may need a combination of your Cloud Application Administrator or Application Administrator for Microsoft Entra ID, Microsoft 365 administrator for Microsoft 365, and Fabric Administrator ( formerly known as Power BI Administrator ) for Microsoft Power BI to complete these tasks -> you may not have access yourself. This guide outlines how to set up Microsoft Power BI so it can connect with Atlan for metadata extraction and lineage tracking. Before you begin Register application in Microsoft Entra ID You need your Cloud Application Administrator or Application Administrator to complete these steps > you may not have access yourself. This is required if the creation of registered applications isn't enabled for the entire organization. To register a new application in Microsoft Entra ID: Log in to the Azure portal . Click App registrations from the left menu. Click + New registration . Enter a name for your client application and click Register . Application (client) ID Directory (tenant) ID Click Certificates & secrets from the left menu. Under Client secrets , click + New client secret . Enter a description, select an expiry time, and click Add . Copy and securely store the client secret Value . Create security group in Microsoft Entra ID You need your Cloud Application Administrator or Application Administrator to complete these steps - you may not have access yourself. To create a security group for your application: Log in to the Azure portal . Click Groups under the Manage section. Click New group . Set the Group type to Security . Enter a Group name and optional description. Click No members selected . Click Select and then Create . By the end of these steps, you have registered an application with Microsoft Entra ID and created a Security Group with the appropriate member. Configure authentication options Atlan supports two authentication methods for fetching metadata from Microsoft Power BI: Service principal authentication (recommended) When using Service Principal authentication, you must decide how the connector shall access metadata to catalog assets and build lineage. There are two supported options: This option grants permissions that let the service principal to access only admin-level Power BI APIs. In this mode, Atlan extracts metadata exclusively using administrative endpoints. This option is recommended for stricter access control environments. You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks - you may not have access yourself. To configure admin API access: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Admin API settings : Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group under Specific security groups Click Apply Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group under Specific security groups Click Apply Add your security group under Specific security groups Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Add your security group This option grants permissions that let the service principal to access both admin and non-admin Power BI APIs. This enables Atlan to extract richer metadata and build detailed lineage across Power BI assets. You need to be at least a member of the Microsoft Power BI workspace to which you want to add the security group to complete these steps - you may not have access yourself. Make sure that you add the security group from the homepage and not the admin portal. To assign a Microsoft Power BI workspace role to the security group: Open the Microsoft Power BI homepage . Open Workspaces and select the workspace you want to access from Atlan. Click Access . In the panel: Enter the name of your security group where it says Enter email addresses Choose one of the following roles: Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Click Add . Enter the name of your security group where it says Enter email addresses Choose one of the following roles: Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Click Add . You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks - you may not have access yourself. To enable both admin and non-admin API access: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Developer settings : Expand Service principals can use Fabric APIs and set to Enabled Add your security group under Specific security groups Click Apply Expand Service principals can use Fabric APIs and set to Enabled Add your security group under Specific security groups Click Apply Add your security group under Specific security groups Under Admin API settings : Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Add your security group After making these changes, you typically need to wait 15-30 minutes for the settings to take effect across Microsoft's services. Delegated user authentication Atlan doesn't recommend using delegated user authentication as it's also not recommended by Microsoft. You need your Microsoft 365 administrator to complete these steps - you may not have access yourself. To assign the delegated user to the Fabric Administrator role: Open the Microsoft 365 admin portal . Click Users and then Active users from the left menu. Select the delegated user. Under Roles , click Manage roles . Expand Show all by category . Under Collaboration , select Fabric Administrator . Click Save changes . You need your Cloud Application Administrator or Application Administrator to complete these steps, you may not have access yourself. The following permissions are only required for delegated user authentication. If using service principal authentication, you don't need to configure any delegated permissions for a service principal it's recommended that you avoid adding these permissions. These are never used and can cause errors that may be hard to troubleshoot. To add permissions for the registered application : In your app registration, click API permissions under the Manage section. Click Add a permission . Click Delegated permissions and select: Capacity.Read.All Dataflow.Read.All Dataset.Read.All Report.Read.All Tenant.Read.All Workspace.Read.All Click Grant Admin consent (If you only see the Add permissions button, you aren't an administrator). You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks, you may not have access yourself. To enable the Microsoft Power BI admin API: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Admin API settings : Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply . Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply . Add your security group Click Apply . Before you begin Configure authentication options"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/business-intelligence\/microsoft-power-bi\/how-tos\/set-up-microsoft-power-bi#api-permissions","title":"Set up Microsoft Power BI | Atlan Documentation","text":"Depending on the authentication method you choose, you may need a combination of your Cloud Application Administrator or Application Administrator for Microsoft Entra ID, Microsoft 365 administrator for Microsoft 365, and Fabric Administrator ( formerly known as Power BI Administrator ) for Microsoft Power BI to complete these tasks -> you may not have access yourself. This guide outlines how to set up Microsoft Power BI so it can connect with Atlan for metadata extraction and lineage tracking. Before you begin Register application in Microsoft Entra ID You need your Cloud Application Administrator or Application Administrator to complete these steps > you may not have access yourself. This is required if the creation of registered applications isn't enabled for the entire organization. To register a new application in Microsoft Entra ID: Log in to the Azure portal . Click App registrations from the left menu. Click + New registration . Enter a name for your client application and click Register . Application (client) ID Directory (tenant) ID Click Certificates & secrets from the left menu. Under Client secrets , click + New client secret . Enter a description, select an expiry time, and click Add . Copy and securely store the client secret Value . Create security group in Microsoft Entra ID You need your Cloud Application Administrator or Application Administrator to complete these steps - you may not have access yourself. To create a security group for your application: Log in to the Azure portal . Click Groups under the Manage section. Click New group . Set the Group type to Security . Enter a Group name and optional description. Click No members selected . Click Select and then Create . By the end of these steps, you have registered an application with Microsoft Entra ID and created a Security Group with the appropriate member. Configure authentication options Atlan supports two authentication methods for fetching metadata from Microsoft Power BI: Service principal authentication (recommended) When using Service Principal authentication, you must decide how the connector shall access metadata to catalog assets and build lineage. There are two supported options: This option grants permissions that let the service principal to access only admin-level Power BI APIs. In this mode, Atlan extracts metadata exclusively using administrative endpoints. This option is recommended for stricter access control environments. You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks - you may not have access yourself. To configure admin API access: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Admin API settings : Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group under Specific security groups Click Apply Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group under Specific security groups Click Apply Add your security group under Specific security groups Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Add your security group This option grants permissions that let the service principal to access both admin and non-admin Power BI APIs. This enables Atlan to extract richer metadata and build detailed lineage across Power BI assets. You need to be at least a member of the Microsoft Power BI workspace to which you want to add the security group to complete these steps - you may not have access yourself. Make sure that you add the security group from the homepage and not the admin portal. To assign a Microsoft Power BI workspace role to the security group: Open the Microsoft Power BI homepage . Open Workspaces and select the workspace you want to access from Atlan. Click Access . In the panel: Enter the name of your security group where it says Enter email addresses Choose one of the following roles: Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Click Add . Enter the name of your security group where it says Enter email addresses Choose one of the following roles: Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Click Add . You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks - you may not have access yourself. To enable both admin and non-admin API access: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Developer settings : Expand Service principals can use Fabric APIs and set to Enabled Add your security group under Specific security groups Click Apply Expand Service principals can use Fabric APIs and set to Enabled Add your security group under Specific security groups Click Apply Add your security group under Specific security groups Under Admin API settings : Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Add your security group After making these changes, you typically need to wait 15-30 minutes for the settings to take effect across Microsoft's services. Delegated user authentication Atlan doesn't recommend using delegated user authentication as it's also not recommended by Microsoft. You need your Microsoft 365 administrator to complete these steps - you may not have access yourself. To assign the delegated user to the Fabric Administrator role: Open the Microsoft 365 admin portal . Click Users and then Active users from the left menu. Select the delegated user. Under Roles , click Manage roles . Expand Show all by category . Under Collaboration , select Fabric Administrator . Click Save changes . You need your Cloud Application Administrator or Application Administrator to complete these steps, you may not have access yourself. The following permissions are only required for delegated user authentication. If using service principal authentication, you don't need to configure any delegated permissions for a service principal it's recommended that you avoid adding these permissions. These are never used and can cause errors that may be hard to troubleshoot. To add permissions for the registered application : In your app registration, click API permissions under the Manage section. Click Add a permission . Click Delegated permissions and select: Capacity.Read.All Dataflow.Read.All Dataset.Read.All Report.Read.All Tenant.Read.All Workspace.Read.All Click Grant Admin consent (If you only see the Add permissions button, you aren't an administrator). You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks, you may not have access yourself. To enable the Microsoft Power BI admin API: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Admin API settings : Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply . Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply . Add your security group Click Apply . Before you begin Configure authentication options"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/business-intelligence\/microsoft-power-bi\/how-tos\/set-up-microsoft-power-bi#admin-api-settings-configuration","title":"Set up Microsoft Power BI | Atlan Documentation","text":"Depending on the authentication method you choose, you may need a combination of your Cloud Application Administrator or Application Administrator for Microsoft Entra ID, Microsoft 365 administrator for Microsoft 365, and Fabric Administrator ( formerly known as Power BI Administrator ) for Microsoft Power BI to complete these tasks -> you may not have access yourself. This guide outlines how to set up Microsoft Power BI so it can connect with Atlan for metadata extraction and lineage tracking. Before you begin Register application in Microsoft Entra ID You need your Cloud Application Administrator or Application Administrator to complete these steps > you may not have access yourself. This is required if the creation of registered applications isn't enabled for the entire organization. To register a new application in Microsoft Entra ID: Log in to the Azure portal . Click App registrations from the left menu. Click + New registration . Enter a name for your client application and click Register . Application (client) ID Directory (tenant) ID Click Certificates & secrets from the left menu. Under Client secrets , click + New client secret . Enter a description, select an expiry time, and click Add . Copy and securely store the client secret Value . Create security group in Microsoft Entra ID You need your Cloud Application Administrator or Application Administrator to complete these steps - you may not have access yourself. To create a security group for your application: Log in to the Azure portal . Click Groups under the Manage section. Click New group . Set the Group type to Security . Enter a Group name and optional description. Click No members selected . Click Select and then Create . By the end of these steps, you have registered an application with Microsoft Entra ID and created a Security Group with the appropriate member. Configure authentication options Atlan supports two authentication methods for fetching metadata from Microsoft Power BI: Service principal authentication (recommended) When using Service Principal authentication, you must decide how the connector shall access metadata to catalog assets and build lineage. There are two supported options: This option grants permissions that let the service principal to access only admin-level Power BI APIs. In this mode, Atlan extracts metadata exclusively using administrative endpoints. This option is recommended for stricter access control environments. You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks - you may not have access yourself. To configure admin API access: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Admin API settings : Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group under Specific security groups Click Apply Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group under Specific security groups Click Apply Add your security group under Specific security groups Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Add your security group This option grants permissions that let the service principal to access both admin and non-admin Power BI APIs. This enables Atlan to extract richer metadata and build detailed lineage across Power BI assets. You need to be at least a member of the Microsoft Power BI workspace to which you want to add the security group to complete these steps - you may not have access yourself. Make sure that you add the security group from the homepage and not the admin portal. To assign a Microsoft Power BI workspace role to the security group: Open the Microsoft Power BI homepage . Open Workspaces and select the workspace you want to access from Atlan. Click Access . In the panel: Enter the name of your security group where it says Enter email addresses Choose one of the following roles: Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Click Add . Enter the name of your security group where it says Enter email addresses Choose one of the following roles: Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Click Add . You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks - you may not have access yourself. To enable both admin and non-admin API access: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Developer settings : Expand Service principals can use Fabric APIs and set to Enabled Add your security group under Specific security groups Click Apply Expand Service principals can use Fabric APIs and set to Enabled Add your security group under Specific security groups Click Apply Add your security group under Specific security groups Under Admin API settings : Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Add your security group After making these changes, you typically need to wait 15-30 minutes for the settings to take effect across Microsoft's services. Delegated user authentication Atlan doesn't recommend using delegated user authentication as it's also not recommended by Microsoft. You need your Microsoft 365 administrator to complete these steps - you may not have access yourself. To assign the delegated user to the Fabric Administrator role: Open the Microsoft 365 admin portal . Click Users and then Active users from the left menu. Select the delegated user. Under Roles , click Manage roles . Expand Show all by category . Under Collaboration , select Fabric Administrator . Click Save changes . You need your Cloud Application Administrator or Application Administrator to complete these steps, you may not have access yourself. The following permissions are only required for delegated user authentication. If using service principal authentication, you don't need to configure any delegated permissions for a service principal it's recommended that you avoid adding these permissions. These are never used and can cause errors that may be hard to troubleshoot. To add permissions for the registered application : In your app registration, click API permissions under the Manage section. Click Add a permission . Click Delegated permissions and select: Capacity.Read.All Dataflow.Read.All Dataset.Read.All Report.Read.All Tenant.Read.All Workspace.Read.All Click Grant Admin consent (If you only see the Add permissions button, you aren't an administrator). You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks, you may not have access yourself. To enable the Microsoft Power BI admin API: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Admin API settings : Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply . Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Add your security group Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply . Add your security group Click Apply . Before you begin Configure authentication options"}
{"url":"https:\/\/docs.atlan.com\/platform\/references\/atlan-architecture#__docusaurus_skipToContent_fallback","title":"Atlan architecture | Atlan Documentation","text":"Atlan is a cloud-first solution. Single-tenant SaaS is the recommended deployment model. Atlan currently supports hosting tenants on the following cloud platforms: Amazon Web Services (AWS) Microsoft Azure Google Cloud Platform (GCP) The components of Atlan are isolated, across both compute and data. For more details, see How are resources isolated? Platform components Kong is an API gateway. It handles rate limiting and token verification on all incoming API requests. Apache Keycloak is an identity and access management component. It manages everything to do with users, login, SSO and so on. Heracles is Atlan's API service. It houses the business logic used by the frontend and APIs to interact with other platform components. PostgreSQL is a SQL database. Many services on the platform use it for storage. HashiCorp Vault is a secret manager. It stores sensitive credentials provided by the user. Apache Ranger is the policy engine. It provides fine-grained access control over data in the metastore. Argo Workflows is a workflow orchestrator for k8s. It runs and manages long-running jobs in a container and k8s-native fashion. Admission Controller is a k8s admission controller. It performs certain actions when Argo Workflows are updated such as workflow alerts. Apache Zookeeper manages consensus and coordination for the metastore services. Apache Cassandra is an object-oriented database used to store the metastore's data. Apache Kafka is an event stream. It enables event-driven use cases across the platform. Heka is Atlan's SQL component. It parses, rewrites and optimizes SQL queries and is powered by Apache Calcite . Redis is a cache layer used by Heracles. Platform management components Velero performs cluster backups. Fluent Bit is a logging and metrics processor. It parses and pushes logs from pods to various destinations. Central components Zenduty is used for incident response. Alerts are sent when something goes wrong in one of the clusters. Argo CD is used for continuous deployment. Changes in git repositories lead to upgrades in the clusters. Github Actions update the Docker container images as part of the development process. Sendgrid is used to send emails. The frontend is a Vue.js web application that's hosted on S3 and delivered via Amazon CloudFront content delivery network (CDN) service. Alertmanager sends alerts generated by metrics stored in Prometheus. Grafana provides observability dashboards. VictoriaMetrics is a fast, cost-effective, and scalable monitoring solution and time series database. It processes high volumes of data and enables long-term storing. Atlan marketplace (not pictured) The marketplace offers packages (workflows) that perform long-running tasks on the Atlan platform. The ecosystem enables the creation of metadata and lineage connectors. See security.atlan.com for the latest policies and standards, reports and certifications, architecture, diagrams and more. Platform management components Atlan marketplace (not pictured)"}
{"url":"https:\/\/docs.atlan.com\/platform\/references\/atlan-architecture#amazon-web-services-aws","title":"Atlan architecture | Atlan Documentation","text":"Atlan is a cloud-first solution. Single-tenant SaaS is the recommended deployment model. Atlan currently supports hosting tenants on the following cloud platforms: Amazon Web Services (AWS) Microsoft Azure Google Cloud Platform (GCP) The components of Atlan are isolated, across both compute and data. For more details, see How are resources isolated? Platform components Kong is an API gateway. It handles rate limiting and token verification on all incoming API requests. Apache Keycloak is an identity and access management component. It manages everything to do with users, login, SSO and so on. Heracles is Atlan's API service. It houses the business logic used by the frontend and APIs to interact with other platform components. PostgreSQL is a SQL database. Many services on the platform use it for storage. HashiCorp Vault is a secret manager. It stores sensitive credentials provided by the user. Apache Ranger is the policy engine. It provides fine-grained access control over data in the metastore. Argo Workflows is a workflow orchestrator for k8s. It runs and manages long-running jobs in a container and k8s-native fashion. Admission Controller is a k8s admission controller. It performs certain actions when Argo Workflows are updated such as workflow alerts. Apache Zookeeper manages consensus and coordination for the metastore services. Apache Cassandra is an object-oriented database used to store the metastore's data. Apache Kafka is an event stream. It enables event-driven use cases across the platform. Heka is Atlan's SQL component. It parses, rewrites and optimizes SQL queries and is powered by Apache Calcite . Redis is a cache layer used by Heracles. Platform management components Velero performs cluster backups. Fluent Bit is a logging and metrics processor. It parses and pushes logs from pods to various destinations. Central components Zenduty is used for incident response. Alerts are sent when something goes wrong in one of the clusters. Argo CD is used for continuous deployment. Changes in git repositories lead to upgrades in the clusters. Github Actions update the Docker container images as part of the development process. Sendgrid is used to send emails. The frontend is a Vue.js web application that's hosted on S3 and delivered via Amazon CloudFront content delivery network (CDN) service. Alertmanager sends alerts generated by metrics stored in Prometheus. Grafana provides observability dashboards. VictoriaMetrics is a fast, cost-effective, and scalable monitoring solution and time series database. It processes high volumes of data and enables long-term storing. Atlan marketplace (not pictured) The marketplace offers packages (workflows) that perform long-running tasks on the Atlan platform. The ecosystem enables the creation of metadata and lineage connectors. See security.atlan.com for the latest policies and standards, reports and certifications, architecture, diagrams and more. Platform management components Atlan marketplace (not pictured)"}
{"url":"https:\/\/docs.atlan.com\/platform\/references\/atlan-architecture#microsoft-azure","title":"Atlan architecture | Atlan Documentation","text":"Atlan is a cloud-first solution. Single-tenant SaaS is the recommended deployment model. Atlan currently supports hosting tenants on the following cloud platforms: Amazon Web Services (AWS) Microsoft Azure Google Cloud Platform (GCP) The components of Atlan are isolated, across both compute and data. For more details, see How are resources isolated? Platform components Kong is an API gateway. It handles rate limiting and token verification on all incoming API requests. Apache Keycloak is an identity and access management component. It manages everything to do with users, login, SSO and so on. Heracles is Atlan's API service. It houses the business logic used by the frontend and APIs to interact with other platform components. PostgreSQL is a SQL database. Many services on the platform use it for storage. HashiCorp Vault is a secret manager. It stores sensitive credentials provided by the user. Apache Ranger is the policy engine. It provides fine-grained access control over data in the metastore. Argo Workflows is a workflow orchestrator for k8s. It runs and manages long-running jobs in a container and k8s-native fashion. Admission Controller is a k8s admission controller. It performs certain actions when Argo Workflows are updated such as workflow alerts. Apache Zookeeper manages consensus and coordination for the metastore services. Apache Cassandra is an object-oriented database used to store the metastore's data. Apache Kafka is an event stream. It enables event-driven use cases across the platform. Heka is Atlan's SQL component. It parses, rewrites and optimizes SQL queries and is powered by Apache Calcite . Redis is a cache layer used by Heracles. Platform management components Velero performs cluster backups. Fluent Bit is a logging and metrics processor. It parses and pushes logs from pods to various destinations. Central components Zenduty is used for incident response. Alerts are sent when something goes wrong in one of the clusters. Argo CD is used for continuous deployment. Changes in git repositories lead to upgrades in the clusters. Github Actions update the Docker container images as part of the development process. Sendgrid is used to send emails. The frontend is a Vue.js web application that's hosted on S3 and delivered via Amazon CloudFront content delivery network (CDN) service. Alertmanager sends alerts generated by metrics stored in Prometheus. Grafana provides observability dashboards. VictoriaMetrics is a fast, cost-effective, and scalable monitoring solution and time series database. It processes high volumes of data and enables long-term storing. Atlan marketplace (not pictured) The marketplace offers packages (workflows) that perform long-running tasks on the Atlan platform. The ecosystem enables the creation of metadata and lineage connectors. See security.atlan.com for the latest policies and standards, reports and certifications, architecture, diagrams and more. Platform management components Atlan marketplace (not pictured)"}
{"url":"https:\/\/docs.atlan.com\/platform\/references\/atlan-architecture#google-cloud-platform-gcp","title":"Atlan architecture | Atlan Documentation","text":"Atlan is a cloud-first solution. Single-tenant SaaS is the recommended deployment model. Atlan currently supports hosting tenants on the following cloud platforms: Amazon Web Services (AWS) Microsoft Azure Google Cloud Platform (GCP) The components of Atlan are isolated, across both compute and data. For more details, see How are resources isolated? Platform components Kong is an API gateway. It handles rate limiting and token verification on all incoming API requests. Apache Keycloak is an identity and access management component. It manages everything to do with users, login, SSO and so on. Heracles is Atlan's API service. It houses the business logic used by the frontend and APIs to interact with other platform components. PostgreSQL is a SQL database. Many services on the platform use it for storage. HashiCorp Vault is a secret manager. It stores sensitive credentials provided by the user. Apache Ranger is the policy engine. It provides fine-grained access control over data in the metastore. Argo Workflows is a workflow orchestrator for k8s. It runs and manages long-running jobs in a container and k8s-native fashion. Admission Controller is a k8s admission controller. It performs certain actions when Argo Workflows are updated such as workflow alerts. Apache Zookeeper manages consensus and coordination for the metastore services. Apache Cassandra is an object-oriented database used to store the metastore's data. Apache Kafka is an event stream. It enables event-driven use cases across the platform. Heka is Atlan's SQL component. It parses, rewrites and optimizes SQL queries and is powered by Apache Calcite . Redis is a cache layer used by Heracles. Platform management components Velero performs cluster backups. Fluent Bit is a logging and metrics processor. It parses and pushes logs from pods to various destinations. Central components Zenduty is used for incident response. Alerts are sent when something goes wrong in one of the clusters. Argo CD is used for continuous deployment. Changes in git repositories lead to upgrades in the clusters. Github Actions update the Docker container images as part of the development process. Sendgrid is used to send emails. The frontend is a Vue.js web application that's hosted on S3 and delivered via Amazon CloudFront content delivery network (CDN) service. Alertmanager sends alerts generated by metrics stored in Prometheus. Grafana provides observability dashboards. VictoriaMetrics is a fast, cost-effective, and scalable monitoring solution and time series database. It processes high volumes of data and enables long-term storing. Atlan marketplace (not pictured) The marketplace offers packages (workflows) that perform long-running tasks on the Atlan platform. The ecosystem enables the creation of metadata and lineage connectors. See security.atlan.com for the latest policies and standards, reports and certifications, architecture, diagrams and more. Platform management components Atlan marketplace (not pictured)"}
{"url":"https:\/\/docs.atlan.com\/platform\/references\/how-are-resources-isolated","title":"How are resources isolated? | Atlan Documentation","text":"Each Atlan customer has their own isolated set of nodes within Kubernetes. The underlying Kubernetes control plane and networking layer (coredns) are shared between tenants. To achieve logical isolation, Loft s virtual clusters are implemented. The underlying Kubernetes control plane and networking layer (coredns) are shared between tenants. To achieve logical isolation, Loft s virtual clusters are implemented. The compute resources (nodes, nodegroups) and storage are physically isolated between tenants. The compute resources (nodes, nodegroups) and storage are physically isolated between tenants. Only Atlan's cloud team is able to manage the AWS , Azure , and GCP resources across these levels of isolation. Only Atlan's cloud team is able to manage the AWS , Azure , and GCP resources across these levels of isolation. See security.atlan.com for the latest policies and standards, reports and certifications, architecture, diagrams and more."}
{"url":"https:\/\/docs.atlan.com\/platform\/references\/atlan-architecture#platform-components","title":"Atlan architecture | Atlan Documentation","text":"Atlan is a cloud-first solution. Single-tenant SaaS is the recommended deployment model. Atlan currently supports hosting tenants on the following cloud platforms: Amazon Web Services (AWS) Microsoft Azure Google Cloud Platform (GCP) The components of Atlan are isolated, across both compute and data. For more details, see How are resources isolated? Platform components Kong is an API gateway. It handles rate limiting and token verification on all incoming API requests. Apache Keycloak is an identity and access management component. It manages everything to do with users, login, SSO and so on. Heracles is Atlan's API service. It houses the business logic used by the frontend and APIs to interact with other platform components. PostgreSQL is a SQL database. Many services on the platform use it for storage. HashiCorp Vault is a secret manager. It stores sensitive credentials provided by the user. Apache Ranger is the policy engine. It provides fine-grained access control over data in the metastore. Argo Workflows is a workflow orchestrator for k8s. It runs and manages long-running jobs in a container and k8s-native fashion. Admission Controller is a k8s admission controller. It performs certain actions when Argo Workflows are updated such as workflow alerts. Apache Zookeeper manages consensus and coordination for the metastore services. Apache Cassandra is an object-oriented database used to store the metastore's data. Apache Kafka is an event stream. It enables event-driven use cases across the platform. Heka is Atlan's SQL component. It parses, rewrites and optimizes SQL queries and is powered by Apache Calcite . Redis is a cache layer used by Heracles. Platform management components Velero performs cluster backups. Fluent Bit is a logging and metrics processor. It parses and pushes logs from pods to various destinations. Central components Zenduty is used for incident response. Alerts are sent when something goes wrong in one of the clusters. Argo CD is used for continuous deployment. Changes in git repositories lead to upgrades in the clusters. Github Actions update the Docker container images as part of the development process. Sendgrid is used to send emails. The frontend is a Vue.js web application that's hosted on S3 and delivered via Amazon CloudFront content delivery network (CDN) service. Alertmanager sends alerts generated by metrics stored in Prometheus. Grafana provides observability dashboards. VictoriaMetrics is a fast, cost-effective, and scalable monitoring solution and time series database. It processes high volumes of data and enables long-term storing. Atlan marketplace (not pictured) The marketplace offers packages (workflows) that perform long-running tasks on the Atlan platform. The ecosystem enables the creation of metadata and lineage connectors. See security.atlan.com for the latest policies and standards, reports and certifications, architecture, diagrams and more. Platform management components Atlan marketplace (not pictured)"}
{"url":"https:\/\/docs.atlan.com\/platform\/references\/atlan-architecture#platform-management-components","title":"Atlan architecture | Atlan Documentation","text":"Atlan is a cloud-first solution. Single-tenant SaaS is the recommended deployment model. Atlan currently supports hosting tenants on the following cloud platforms: Amazon Web Services (AWS) Microsoft Azure Google Cloud Platform (GCP) The components of Atlan are isolated, across both compute and data. For more details, see How are resources isolated? Platform components Kong is an API gateway. It handles rate limiting and token verification on all incoming API requests. Apache Keycloak is an identity and access management component. It manages everything to do with users, login, SSO and so on. Heracles is Atlan's API service. It houses the business logic used by the frontend and APIs to interact with other platform components. PostgreSQL is a SQL database. Many services on the platform use it for storage. HashiCorp Vault is a secret manager. It stores sensitive credentials provided by the user. Apache Ranger is the policy engine. It provides fine-grained access control over data in the metastore. Argo Workflows is a workflow orchestrator for k8s. It runs and manages long-running jobs in a container and k8s-native fashion. Admission Controller is a k8s admission controller. It performs certain actions when Argo Workflows are updated such as workflow alerts. Apache Zookeeper manages consensus and coordination for the metastore services. Apache Cassandra is an object-oriented database used to store the metastore's data. Apache Kafka is an event stream. It enables event-driven use cases across the platform. Heka is Atlan's SQL component. It parses, rewrites and optimizes SQL queries and is powered by Apache Calcite . Redis is a cache layer used by Heracles. Platform management components Velero performs cluster backups. Fluent Bit is a logging and metrics processor. It parses and pushes logs from pods to various destinations. Central components Zenduty is used for incident response. Alerts are sent when something goes wrong in one of the clusters. Argo CD is used for continuous deployment. Changes in git repositories lead to upgrades in the clusters. Github Actions update the Docker container images as part of the development process. Sendgrid is used to send emails. The frontend is a Vue.js web application that's hosted on S3 and delivered via Amazon CloudFront content delivery network (CDN) service. Alertmanager sends alerts generated by metrics stored in Prometheus. Grafana provides observability dashboards. VictoriaMetrics is a fast, cost-effective, and scalable monitoring solution and time series database. It processes high volumes of data and enables long-term storing. Atlan marketplace (not pictured) The marketplace offers packages (workflows) that perform long-running tasks on the Atlan platform. The ecosystem enables the creation of metadata and lineage connectors. See security.atlan.com for the latest policies and standards, reports and certifications, architecture, diagrams and more. Platform management components Atlan marketplace (not pictured)"}
{"url":"https:\/\/docs.atlan.com\/platform\/references\/atlan-architecture#central-components","title":"Atlan architecture | Atlan Documentation","text":"Atlan is a cloud-first solution. Single-tenant SaaS is the recommended deployment model. Atlan currently supports hosting tenants on the following cloud platforms: Amazon Web Services (AWS) Microsoft Azure Google Cloud Platform (GCP) The components of Atlan are isolated, across both compute and data. For more details, see How are resources isolated? Platform components Kong is an API gateway. It handles rate limiting and token verification on all incoming API requests. Apache Keycloak is an identity and access management component. It manages everything to do with users, login, SSO and so on. Heracles is Atlan's API service. It houses the business logic used by the frontend and APIs to interact with other platform components. PostgreSQL is a SQL database. Many services on the platform use it for storage. HashiCorp Vault is a secret manager. It stores sensitive credentials provided by the user. Apache Ranger is the policy engine. It provides fine-grained access control over data in the metastore. Argo Workflows is a workflow orchestrator for k8s. It runs and manages long-running jobs in a container and k8s-native fashion. Admission Controller is a k8s admission controller. It performs certain actions when Argo Workflows are updated such as workflow alerts. Apache Zookeeper manages consensus and coordination for the metastore services. Apache Cassandra is an object-oriented database used to store the metastore's data. Apache Kafka is an event stream. It enables event-driven use cases across the platform. Heka is Atlan's SQL component. It parses, rewrites and optimizes SQL queries and is powered by Apache Calcite . Redis is a cache layer used by Heracles. Platform management components Velero performs cluster backups. Fluent Bit is a logging and metrics processor. It parses and pushes logs from pods to various destinations. Central components Zenduty is used for incident response. Alerts are sent when something goes wrong in one of the clusters. Argo CD is used for continuous deployment. Changes in git repositories lead to upgrades in the clusters. Github Actions update the Docker container images as part of the development process. Sendgrid is used to send emails. The frontend is a Vue.js web application that's hosted on S3 and delivered via Amazon CloudFront content delivery network (CDN) service. Alertmanager sends alerts generated by metrics stored in Prometheus. Grafana provides observability dashboards. VictoriaMetrics is a fast, cost-effective, and scalable monitoring solution and time series database. It processes high volumes of data and enables long-term storing. Atlan marketplace (not pictured) The marketplace offers packages (workflows) that perform long-running tasks on the Atlan platform. The ecosystem enables the creation of metadata and lineage connectors. See security.atlan.com for the latest policies and standards, reports and certifications, architecture, diagrams and more. Platform management components Atlan marketplace (not pictured)"}
{"url":"https:\/\/docs.atlan.com\/platform\/references\/atlan-architecture#atlan-marketplace-not-pictured","title":"Atlan architecture | Atlan Documentation","text":"Atlan is a cloud-first solution. Single-tenant SaaS is the recommended deployment model. Atlan currently supports hosting tenants on the following cloud platforms: Amazon Web Services (AWS) Microsoft Azure Google Cloud Platform (GCP) The components of Atlan are isolated, across both compute and data. For more details, see How are resources isolated? Platform components Kong is an API gateway. It handles rate limiting and token verification on all incoming API requests. Apache Keycloak is an identity and access management component. It manages everything to do with users, login, SSO and so on. Heracles is Atlan's API service. It houses the business logic used by the frontend and APIs to interact with other platform components. PostgreSQL is a SQL database. Many services on the platform use it for storage. HashiCorp Vault is a secret manager. It stores sensitive credentials provided by the user. Apache Ranger is the policy engine. It provides fine-grained access control over data in the metastore. Argo Workflows is a workflow orchestrator for k8s. It runs and manages long-running jobs in a container and k8s-native fashion. Admission Controller is a k8s admission controller. It performs certain actions when Argo Workflows are updated such as workflow alerts. Apache Zookeeper manages consensus and coordination for the metastore services. Apache Cassandra is an object-oriented database used to store the metastore's data. Apache Kafka is an event stream. It enables event-driven use cases across the platform. Heka is Atlan's SQL component. It parses, rewrites and optimizes SQL queries and is powered by Apache Calcite . Redis is a cache layer used by Heracles. Platform management components Velero performs cluster backups. Fluent Bit is a logging and metrics processor. It parses and pushes logs from pods to various destinations. Central components Zenduty is used for incident response. Alerts are sent when something goes wrong in one of the clusters. Argo CD is used for continuous deployment. Changes in git repositories lead to upgrades in the clusters. Github Actions update the Docker container images as part of the development process. Sendgrid is used to send emails. The frontend is a Vue.js web application that's hosted on S3 and delivered via Amazon CloudFront content delivery network (CDN) service. Alertmanager sends alerts generated by metrics stored in Prometheus. Grafana provides observability dashboards. VictoriaMetrics is a fast, cost-effective, and scalable monitoring solution and time series database. It processes high volumes of data and enables long-term storing. Atlan marketplace (not pictured) The marketplace offers packages (workflows) that perform long-running tasks on the Atlan platform. The ecosystem enables the creation of metadata and lineage connectors. See security.atlan.com for the latest policies and standards, reports and certifications, architecture, diagrams and more. Platform management components Atlan marketplace (not pictured)"}
{"url":"https:\/\/docs.atlan.com\/product\/integrations\/automation\/browser-extension\/how-tos\/use-the-atlan-browser-extension#__docusaurus_skipToContent_fallback","title":"Use the Atlan browser extension | Atlan Documentation","text":"The Atlan browser extension provides metadata context directly in your supported data tools . You can use the extension in the following Chromium-based browsers: Google Chrome and Microsoft Edge. Install the extension To install Atlan's browser extension: You can either: Find the extension in the Chrome Web Store: https:\/\/chrome.google.com\/webstore\/detail\/atlan\/fipjfjlalpnbejlmmpfnmlkadjgaaheg From the upper right of any screen in Atlan, navigate to your name and then click Profile . Click the four dots icon in the resulting dialog to get to integrations. Under Apps , for Browser extension , click Install . Find the extension in the Chrome Web Store: https:\/\/chrome.google.com\/webstore\/detail\/atlan\/fipjfjlalpnbejlmmpfnmlkadjgaaheg From the upper right of any screen in Atlan, navigate to your name and then click Profile . Click the four dots icon in the resulting dialog to get to integrations. Under Apps , for Browser extension , click Install . Click the four dots icon in the resulting dialog to get to integrations. Under Apps , for Browser extension , click Install . To install the Atlan browser extension: For Google Chrome, in the upper right of your screen, click Add to Chrome . When prompted for confirmation, click the Add extension button. For Microsoft Edge, follow the steps in Add an extension to Microsoft Edge from the Chrome Web Store . For Google Chrome, in the upper right of your screen, click Add to Chrome . When prompted for confirmation, click the Add extension button. For Microsoft Edge, follow the steps in Add an extension to Microsoft Edge from the Chrome Web Store . Currently, you can't install the browser extension on mobile devices or tablets. You can also install Atlan's browser extension at the workspace level . To set this up, you need to be an administrator or have access to the admin console of your organization's Google account. If your organization uses managed browsers, you can configure the extension for managed browsers . Configure the extension Once installed, configure the Atlan browser extension to get started. Optionally, Atlan admins can preconfigure custom domains for data sources , if any. Configure the extension as a user To configure the browser extension, once installed: If you are logged into your Atlan instance, skip to the next step. If you haven't logged into Atlan, log in to your Atlan instance when prompted. In the Options page, to enter the URL of your Atlan instance: If your organization uses an Atlan domain (for example, _mycompany_.atlan.com ), the Atlan instance URL appears preselected. Click Get started . (Optional) Switch to a different Atlan domain, if required. If your organization uses a custom domain (for example, _atlan_.mycompany.com ), enter the URL of your Atlan instance and then click Get started . If your organization uses an Atlan domain (for example, _mycompany_.atlan.com ), the Atlan instance URL appears preselected. Click Get started . (Optional) Switch to a different Atlan domain, if required. If your organization uses a custom domain (for example, _atlan_.mycompany.com ), enter the URL of your Atlan instance and then click Get started . After a successful login, the message Updated successfully appears. (Optional) If your data tools are hosted on custom domains, rather than the standard SaaS domain of each tool: Click the Configure custom domain link at the bottom. In the dropdown on the left, select your data tool. In the text box on the right, enter the custom domain you use for that tool. Repeat these steps for each tool hosted on a custom domain. Click the Save button when finished. If your Atlan admin has preconfigured custom domains for data sources , you won't be able to update or remove these selections. Click + Add to configure custom domains for additional data sources as required. Click the Configure custom domain link at the bottom. In the dropdown on the left, select your data tool. In the text box on the right, enter the custom domain you use for that tool. Repeat these steps for each tool hosted on a custom domain. Click the Save button when finished. In the dropdown on the left, select your data tool. In the text box on the right, enter the custom domain you use for that tool. Repeat these steps for each tool hosted on a custom domain. Click the Save button when finished. If your Atlan admin has preconfigured custom domains for data sources , you won't be able to update or remove these selections. Click + Add to configure custom domains for additional data sources as required. You can now close the Options tab. The extension is now ready to use! Ã° (Optional) Configure custom domains as an admin You need to be an admin user in Atlan to configure custom domains for data sources from the admin center. To configure custom domains, from within Atlan: From the left menu of any screen, click Admin . Under Workspace , click Integrations . Under Apps , expand the Browser extension tile. In the Browser extension tile, for Set up your custom data source. , if your data tools are hosted on custom domains rather than the standard SaaS domain of each tool, click the Configure link to configure them for users in your organization. For Connector , select a supported tool for the browser extension. In the adjacent field, enter the URL of the custom domain for your data source. (Optional) Click + Add to add more. Click Save to save your configuration. info Ã° Âª Did you know? For any supported tools that you have configured, your users won't be able to update or remove these selections. They can, however, add additional custom domains for data sources. For Connector , select a supported tool for the browser extension. In the adjacent field, enter the URL of the custom domain for your data source. (Optional) Click + Add to add more. Click Save to save your configuration. Ã° Âª Did you know? For any supported tools that you have configured, your users won't be able to update or remove these selections. They can, however, add additional custom domains for data sources. (Optional) For Download Atlan extension or share with your team , you can either install the Atlan browser extension for your own use or share the link with your users. Anyone with access to Atlan any admin, member, or guest user and a supported tool can use the browser extension. First, log into Atlan. When using Atlan's browser extension in a supported tool , the extension only reads the URL of your browser tab no other data is accessed. If using Atlan's browser extension on any website , it only reads the favicon, page title, and URL of your browser tab. Learn more about Atlan browser extension security . Access and enrich context in-flow To access context for an asset, from within a supported tool: Log into the supported tool. Open any supported asset. In the lower-right corner of the page, click the small Atlan icon. danger The icon to activate Atlan is not the extension icon that appears at the top of your Chrome browser. This small Atlan icon in the lower right corner of the page is the only way to access the metadata for the asset you are viewing in another tool. The icon to activate Atlan is not the extension icon that appears at the top of your Chrome browser. This small Atlan icon in the lower right corner of the page is the only way to access the metadata for the asset you are viewing in another tool. In the sidebar that appears: Click the tabs and links to view all context about the asset. Make changes to any of the metadata you'd like. Click the tabs and links to view all context about the asset. Make changes to any of the metadata you'd like. Now you can understand and enrich assets without leaving your data tools themselves! Ã° The Atlan sidebar automatically reloads as you browse your assets in a supported tool to show details about the asset you're currently viewing. Your permissions in Atlan control what metadata you can see and change in the extension. The extension opens a new browser tab on Atlan's discovery page, with the results for that text! Ã° Add a resource You can link any web page as a resource to your assets in Atlan using the browser extension. To add a web page as a resource to an asset: In the top right of the web page you're viewing, click the Atlan Chrome extension . In the resource clipper menu, under Link this page to an asset , select the asset to which you'd like to add the web page as a resource. Click Save to confirm your selection. (Optional) Once the resource has been linked successfully, click the Open in Atlan button to view the linked asset directly in Atlan. You can now add resources to your assets in Atlan from any website! Ã° The Tableau extension offers native embeddings directly in your dashboards. See Enable embedded metadata in Tableau for more information. Supported tools Currently, the Atlan browser extension supports assets in the following tools: Amazon QuickSight : analyses, dashboards, and datasets Databricks : databases, schemas, views, and tables dbt Cloud : models and sources in the model editor and dbt docs Google BigQuery : datasets, schemas, views, and tables IBM Cognos Analytics : folders, dashboards, packages, explorations, reports, files, data sources, and modules Looker : dashboards, explores, and folders Microsoft Power BI : dashboards, reports, dataflows, and datasets Mode : collections, reports, queries, and charts Qlik Sense Cloud : apps, datasets, sheets, and spaces Redash : queries, dashboards, and visualizations Salesforce : objects Sigma : datasets, pages, and data elements Snowflake (via Snowsight schema explorer): databases, schemas, tables, views, dynamic tables, streams, and pipes Tableau : dashboards, data sources, workbooks, and metrics. Additionally, you can choose to switch the Tableau extension to offer native embeddings directly in your dashboards. See Enable embedded metadata in Tableau for more information. ThoughtSpot : liveboards, answers, visualizations, and tables MicroStrategy : dossiers, reports, documents Install the extension Configure the extension"}
{"url":"https:\/\/docs.atlan.com\/product\/integrations\/automation\/browser-extension\/how-tos\/use-the-atlan-browser-extension#supported-tools","title":"Use the Atlan browser extension | Atlan Documentation","text":"The Atlan browser extension provides metadata context directly in your supported data tools . You can use the extension in the following Chromium-based browsers: Google Chrome and Microsoft Edge. Install the extension To install Atlan's browser extension: You can either: Find the extension in the Chrome Web Store: https:\/\/chrome.google.com\/webstore\/detail\/atlan\/fipjfjlalpnbejlmmpfnmlkadjgaaheg From the upper right of any screen in Atlan, navigate to your name and then click Profile . Click the four dots icon in the resulting dialog to get to integrations. Under Apps , for Browser extension , click Install . Find the extension in the Chrome Web Store: https:\/\/chrome.google.com\/webstore\/detail\/atlan\/fipjfjlalpnbejlmmpfnmlkadjgaaheg From the upper right of any screen in Atlan, navigate to your name and then click Profile . Click the four dots icon in the resulting dialog to get to integrations. Under Apps , for Browser extension , click Install . Click the four dots icon in the resulting dialog to get to integrations. Under Apps , for Browser extension , click Install . To install the Atlan browser extension: For Google Chrome, in the upper right of your screen, click Add to Chrome . When prompted for confirmation, click the Add extension button. For Microsoft Edge, follow the steps in Add an extension to Microsoft Edge from the Chrome Web Store . For Google Chrome, in the upper right of your screen, click Add to Chrome . When prompted for confirmation, click the Add extension button. For Microsoft Edge, follow the steps in Add an extension to Microsoft Edge from the Chrome Web Store . Currently, you can't install the browser extension on mobile devices or tablets. You can also install Atlan's browser extension at the workspace level . To set this up, you need to be an administrator or have access to the admin console of your organization's Google account. If your organization uses managed browsers, you can configure the extension for managed browsers . Configure the extension Once installed, configure the Atlan browser extension to get started. Optionally, Atlan admins can preconfigure custom domains for data sources , if any. Configure the extension as a user To configure the browser extension, once installed: If you are logged into your Atlan instance, skip to the next step. If you haven't logged into Atlan, log in to your Atlan instance when prompted. In the Options page, to enter the URL of your Atlan instance: If your organization uses an Atlan domain (for example, _mycompany_.atlan.com ), the Atlan instance URL appears preselected. Click Get started . (Optional) Switch to a different Atlan domain, if required. If your organization uses a custom domain (for example, _atlan_.mycompany.com ), enter the URL of your Atlan instance and then click Get started . If your organization uses an Atlan domain (for example, _mycompany_.atlan.com ), the Atlan instance URL appears preselected. Click Get started . (Optional) Switch to a different Atlan domain, if required. If your organization uses a custom domain (for example, _atlan_.mycompany.com ), enter the URL of your Atlan instance and then click Get started . After a successful login, the message Updated successfully appears. (Optional) If your data tools are hosted on custom domains, rather than the standard SaaS domain of each tool: Click the Configure custom domain link at the bottom. In the dropdown on the left, select your data tool. In the text box on the right, enter the custom domain you use for that tool. Repeat these steps for each tool hosted on a custom domain. Click the Save button when finished. If your Atlan admin has preconfigured custom domains for data sources , you won't be able to update or remove these selections. Click + Add to configure custom domains for additional data sources as required. Click the Configure custom domain link at the bottom. In the dropdown on the left, select your data tool. In the text box on the right, enter the custom domain you use for that tool. Repeat these steps for each tool hosted on a custom domain. Click the Save button when finished. In the dropdown on the left, select your data tool. In the text box on the right, enter the custom domain you use for that tool. Repeat these steps for each tool hosted on a custom domain. Click the Save button when finished. If your Atlan admin has preconfigured custom domains for data sources , you won't be able to update or remove these selections. Click + Add to configure custom domains for additional data sources as required. You can now close the Options tab. The extension is now ready to use! Ã° (Optional) Configure custom domains as an admin You need to be an admin user in Atlan to configure custom domains for data sources from the admin center. To configure custom domains, from within Atlan: From the left menu of any screen, click Admin . Under Workspace , click Integrations . Under Apps , expand the Browser extension tile. In the Browser extension tile, for Set up your custom data source. , if your data tools are hosted on custom domains rather than the standard SaaS domain of each tool, click the Configure link to configure them for users in your organization. For Connector , select a supported tool for the browser extension. In the adjacent field, enter the URL of the custom domain for your data source. (Optional) Click + Add to add more. Click Save to save your configuration. info Ã° Âª Did you know? For any supported tools that you have configured, your users won't be able to update or remove these selections. They can, however, add additional custom domains for data sources. For Connector , select a supported tool for the browser extension. In the adjacent field, enter the URL of the custom domain for your data source. (Optional) Click + Add to add more. Click Save to save your configuration. Ã° Âª Did you know? For any supported tools that you have configured, your users won't be able to update or remove these selections. They can, however, add additional custom domains for data sources. (Optional) For Download Atlan extension or share with your team , you can either install the Atlan browser extension for your own use or share the link with your users. Anyone with access to Atlan any admin, member, or guest user and a supported tool can use the browser extension. First, log into Atlan. When using Atlan's browser extension in a supported tool , the extension only reads the URL of your browser tab no other data is accessed. If using Atlan's browser extension on any website , it only reads the favicon, page title, and URL of your browser tab. Learn more about Atlan browser extension security . Access and enrich context in-flow To access context for an asset, from within a supported tool: Log into the supported tool. Open any supported asset. In the lower-right corner of the page, click the small Atlan icon. danger The icon to activate Atlan is not the extension icon that appears at the top of your Chrome browser. This small Atlan icon in the lower right corner of the page is the only way to access the metadata for the asset you are viewing in another tool. The icon to activate Atlan is not the extension icon that appears at the top of your Chrome browser. This small Atlan icon in the lower right corner of the page is the only way to access the metadata for the asset you are viewing in another tool. In the sidebar that appears: Click the tabs and links to view all context about the asset. Make changes to any of the metadata you'd like. Click the tabs and links to view all context about the asset. Make changes to any of the metadata you'd like. Now you can understand and enrich assets without leaving your data tools themselves! Ã° The Atlan sidebar automatically reloads as you browse your assets in a supported tool to show details about the asset you're currently viewing. Your permissions in Atlan control what metadata you can see and change in the extension. The extension opens a new browser tab on Atlan's discovery page, with the results for that text! Ã° Add a resource You can link any web page as a resource to your assets in Atlan using the browser extension. To add a web page as a resource to an asset: In the top right of the web page you're viewing, click the Atlan Chrome extension . In the resource clipper menu, under Link this page to an asset , select the asset to which you'd like to add the web page as a resource. Click Save to confirm your selection. (Optional) Once the resource has been linked successfully, click the Open in Atlan button to view the linked asset directly in Atlan. You can now add resources to your assets in Atlan from any website! Ã° The Tableau extension offers native embeddings directly in your dashboards. See Enable embedded metadata in Tableau for more information. Supported tools Currently, the Atlan browser extension supports assets in the following tools: Amazon QuickSight : analyses, dashboards, and datasets Databricks : databases, schemas, views, and tables dbt Cloud : models and sources in the model editor and dbt docs Google BigQuery : datasets, schemas, views, and tables IBM Cognos Analytics : folders, dashboards, packages, explorations, reports, files, data sources, and modules Looker : dashboards, explores, and folders Microsoft Power BI : dashboards, reports, dataflows, and datasets Mode : collections, reports, queries, and charts Qlik Sense Cloud : apps, datasets, sheets, and spaces Redash : queries, dashboards, and visualizations Salesforce : objects Sigma : datasets, pages, and data elements Snowflake (via Snowsight schema explorer): databases, schemas, tables, views, dynamic tables, streams, and pipes Tableau : dashboards, data sources, workbooks, and metrics. Additionally, you can choose to switch the Tableau extension to offer native embeddings directly in your dashboards. See Enable embedded metadata in Tableau for more information. ThoughtSpot : liveboards, answers, visualizations, and tables MicroStrategy : dossiers, reports, documents Install the extension Configure the extension"}
{"url":"https:\/\/docs.atlan.com\/product\/integrations\/automation\/browser-extension\/how-tos\/use-the-atlan-browser-extension#install-the-extension","title":"Use the Atlan browser extension | Atlan Documentation","text":"The Atlan browser extension provides metadata context directly in your supported data tools . You can use the extension in the following Chromium-based browsers: Google Chrome and Microsoft Edge. Install the extension To install Atlan's browser extension: You can either: Find the extension in the Chrome Web Store: https:\/\/chrome.google.com\/webstore\/detail\/atlan\/fipjfjlalpnbejlmmpfnmlkadjgaaheg From the upper right of any screen in Atlan, navigate to your name and then click Profile . Click the four dots icon in the resulting dialog to get to integrations. Under Apps , for Browser extension , click Install . Find the extension in the Chrome Web Store: https:\/\/chrome.google.com\/webstore\/detail\/atlan\/fipjfjlalpnbejlmmpfnmlkadjgaaheg From the upper right of any screen in Atlan, navigate to your name and then click Profile . Click the four dots icon in the resulting dialog to get to integrations. Under Apps , for Browser extension , click Install . Click the four dots icon in the resulting dialog to get to integrations. Under Apps , for Browser extension , click Install . To install the Atlan browser extension: For Google Chrome, in the upper right of your screen, click Add to Chrome . When prompted for confirmation, click the Add extension button. For Microsoft Edge, follow the steps in Add an extension to Microsoft Edge from the Chrome Web Store . For Google Chrome, in the upper right of your screen, click Add to Chrome . When prompted for confirmation, click the Add extension button. For Microsoft Edge, follow the steps in Add an extension to Microsoft Edge from the Chrome Web Store . Currently, you can't install the browser extension on mobile devices or tablets. You can also install Atlan's browser extension at the workspace level . To set this up, you need to be an administrator or have access to the admin console of your organization's Google account. If your organization uses managed browsers, you can configure the extension for managed browsers . Configure the extension Once installed, configure the Atlan browser extension to get started. Optionally, Atlan admins can preconfigure custom domains for data sources , if any. Configure the extension as a user To configure the browser extension, once installed: If you are logged into your Atlan instance, skip to the next step. If you haven't logged into Atlan, log in to your Atlan instance when prompted. In the Options page, to enter the URL of your Atlan instance: If your organization uses an Atlan domain (for example, _mycompany_.atlan.com ), the Atlan instance URL appears preselected. Click Get started . (Optional) Switch to a different Atlan domain, if required. If your organization uses a custom domain (for example, _atlan_.mycompany.com ), enter the URL of your Atlan instance and then click Get started . If your organization uses an Atlan domain (for example, _mycompany_.atlan.com ), the Atlan instance URL appears preselected. Click Get started . (Optional) Switch to a different Atlan domain, if required. If your organization uses a custom domain (for example, _atlan_.mycompany.com ), enter the URL of your Atlan instance and then click Get started . After a successful login, the message Updated successfully appears. (Optional) If your data tools are hosted on custom domains, rather than the standard SaaS domain of each tool: Click the Configure custom domain link at the bottom. In the dropdown on the left, select your data tool. In the text box on the right, enter the custom domain you use for that tool. Repeat these steps for each tool hosted on a custom domain. Click the Save button when finished. If your Atlan admin has preconfigured custom domains for data sources , you won't be able to update or remove these selections. Click + Add to configure custom domains for additional data sources as required. Click the Configure custom domain link at the bottom. In the dropdown on the left, select your data tool. In the text box on the right, enter the custom domain you use for that tool. Repeat these steps for each tool hosted on a custom domain. Click the Save button when finished. In the dropdown on the left, select your data tool. In the text box on the right, enter the custom domain you use for that tool. Repeat these steps for each tool hosted on a custom domain. Click the Save button when finished. If your Atlan admin has preconfigured custom domains for data sources , you won't be able to update or remove these selections. Click + Add to configure custom domains for additional data sources as required. You can now close the Options tab. The extension is now ready to use! Ã° (Optional) Configure custom domains as an admin You need to be an admin user in Atlan to configure custom domains for data sources from the admin center. To configure custom domains, from within Atlan: From the left menu of any screen, click Admin . Under Workspace , click Integrations . Under Apps , expand the Browser extension tile. In the Browser extension tile, for Set up your custom data source. , if your data tools are hosted on custom domains rather than the standard SaaS domain of each tool, click the Configure link to configure them for users in your organization. For Connector , select a supported tool for the browser extension. In the adjacent field, enter the URL of the custom domain for your data source. (Optional) Click + Add to add more. Click Save to save your configuration. info Ã° Âª Did you know? For any supported tools that you have configured, your users won't be able to update or remove these selections. They can, however, add additional custom domains for data sources. For Connector , select a supported tool for the browser extension. In the adjacent field, enter the URL of the custom domain for your data source. (Optional) Click + Add to add more. Click Save to save your configuration. Ã° Âª Did you know? For any supported tools that you have configured, your users won't be able to update or remove these selections. They can, however, add additional custom domains for data sources. (Optional) For Download Atlan extension or share with your team , you can either install the Atlan browser extension for your own use or share the link with your users. Anyone with access to Atlan any admin, member, or guest user and a supported tool can use the browser extension. First, log into Atlan. When using Atlan's browser extension in a supported tool , the extension only reads the URL of your browser tab no other data is accessed. If using Atlan's browser extension on any website , it only reads the favicon, page title, and URL of your browser tab. Learn more about Atlan browser extension security . Access and enrich context in-flow To access context for an asset, from within a supported tool: Log into the supported tool. Open any supported asset. In the lower-right corner of the page, click the small Atlan icon. danger The icon to activate Atlan is not the extension icon that appears at the top of your Chrome browser. This small Atlan icon in the lower right corner of the page is the only way to access the metadata for the asset you are viewing in another tool. The icon to activate Atlan is not the extension icon that appears at the top of your Chrome browser. This small Atlan icon in the lower right corner of the page is the only way to access the metadata for the asset you are viewing in another tool. In the sidebar that appears: Click the tabs and links to view all context about the asset. Make changes to any of the metadata you'd like. Click the tabs and links to view all context about the asset. Make changes to any of the metadata you'd like. Now you can understand and enrich assets without leaving your data tools themselves! Ã° The Atlan sidebar automatically reloads as you browse your assets in a supported tool to show details about the asset you're currently viewing. Your permissions in Atlan control what metadata you can see and change in the extension. The extension opens a new browser tab on Atlan's discovery page, with the results for that text! Ã° Add a resource You can link any web page as a resource to your assets in Atlan using the browser extension. To add a web page as a resource to an asset: In the top right of the web page you're viewing, click the Atlan Chrome extension . In the resource clipper menu, under Link this page to an asset , select the asset to which you'd like to add the web page as a resource. Click Save to confirm your selection. (Optional) Once the resource has been linked successfully, click the Open in Atlan button to view the linked asset directly in Atlan. You can now add resources to your assets in Atlan from any website! Ã° The Tableau extension offers native embeddings directly in your dashboards. See Enable embedded metadata in Tableau for more information. Supported tools Currently, the Atlan browser extension supports assets in the following tools: Amazon QuickSight : analyses, dashboards, and datasets Databricks : databases, schemas, views, and tables dbt Cloud : models and sources in the model editor and dbt docs Google BigQuery : datasets, schemas, views, and tables IBM Cognos Analytics : folders, dashboards, packages, explorations, reports, files, data sources, and modules Looker : dashboards, explores, and folders Microsoft Power BI : dashboards, reports, dataflows, and datasets Mode : collections, reports, queries, and charts Qlik Sense Cloud : apps, datasets, sheets, and spaces Redash : queries, dashboards, and visualizations Salesforce : objects Sigma : datasets, pages, and data elements Snowflake (via Snowsight schema explorer): databases, schemas, tables, views, dynamic tables, streams, and pipes Tableau : dashboards, data sources, workbooks, and metrics. Additionally, you can choose to switch the Tableau extension to offer native embeddings directly in your dashboards. See Enable embedded metadata in Tableau for more information. ThoughtSpot : liveboards, answers, visualizations, and tables MicroStrategy : dossiers, reports, documents Install the extension Configure the extension"}
{"url":"https:\/\/docs.atlan.com\/product\/integrations\/automation\/browser-extension\/how-tos\/configure-the-extension-for-managed-browsers","title":"Configure the extension for managed browsers | Atlan Documentation","text":"If you're using managed browsers, you can install and configure the Atlan browser extension for all users in your organization. To do so, you will need to bulk install the extension and deploy a configuration script. Atlan supports managing the Atlan browser extension for the following: Operating systems: macOS and Microsoft Windows Browsers: Google Chrome and Microsoft Edge The deployment scripts - .mobileconfig file for macOS and PowerShell script for Microsoft Windows - are designed to make only the most necessary modifications required for the Atlan browser extension to function properly. Both deployment methods adhere to the principle of least privilege: The .mobileconfig file for macOS only includes the configuration settings required to install and operate the Atlan browser extension. The PowerShell script creates essential registry keys required for the Atlan browser extension to operate on Microsoft Windows systems. To configure the Atlan browser extension for a managed browser, you must complete these steps in the following order: Configure the browser extension Bulk install the browser extension Deploy the configuration script (Optional) Verify and monitor the installation Configure the browser extension You will need to be an admin in Atlan to configure the browser extension for users in your organization. You will also need inputs and approval from the IT administrator of your organization. You can configure the browser extension and then download a configuration script to bulk install and deploy it for everyone in your organization. To configure the browser extension, from within Atlan: From the left menu on any screen, click Admin . Under Workspace , click Integrations . Under Apps , expand the Browser extension tile. In the Browser extension tile, for Bulk install the browser extension , click the Set up now button. In the Set up browser extension form, enter the following details: For Choose browser , the browser and operating system values will be prefilled based on what you're currently using - you can modify the fields, if required. For Your Atlan domain , enter the URL of your Atlan instance - for example, https:\/\/(instance_name).atlan.com . info Ã° Âª Did you know? If you enable multiple Atlan domains, your users will be able to select the most relevant Atlan domain from a dropdown menu while using the browser extension. The default value in the dropdown will be the Atlan instance entered as Your Atlan domain . If your organization does not have multiple Atlan domains, only the default selection will be displayed. (Optional) For Advanced settings , you can configure the following: If you have multiple Atlan instances, toggle on Multiple Atlan domains and then enter the URLs of your additional Atlan instances. Click + Add to add more Atlan domains. If your data tools are hosted on custom domains rather than the standard SaaS domain of each tool, toggle on Custom data source domain . Click + Add to add more custom domains for data sources. For Connector , select a supported tool for the browser extension. In the adjacent field, enter the URL of your custom data source domain. info Ã° Âª Did you know? For any supported tools configured while setting up the managed browser extension, your users will not be able to update or remove these selections. They can, however, add additional custom domains for data sources. Click the Download Script button to download the corresponding configuration script. The IT administrator(s) in your organization will need to install this configuration file in your organization's devices using a mobile device management (MDM) software. Administrative permissions to the MDM platform are required to complete the setup. Based on your operating system, the downloaded file can be one of the following two types: .mobileconfig - use this file to configure profiles with specific settings in macOS devices . .ps1 - use this PowerShell script to create registry keys in Microsoft Windows devices . For Choose browser , the browser and operating system values will be prefilled based on what you're currently using - you can modify the fields, if required. For Choose browser , the browser and operating system values will be prefilled based on what you're currently using - you can modify the fields, if required. For Your Atlan domain , enter the URL of your Atlan instance - for example, https:\/\/(instance_name).atlan.com . info Ã° Âª Did you know? If you enable multiple Atlan domains, your users will be able to select the most relevant Atlan domain from a dropdown menu while using the browser extension. The default value in the dropdown will be the Atlan instance entered as Your Atlan domain . If your organization does not have multiple Atlan domains, only the default selection will be displayed. For Your Atlan domain , enter the URL of your Atlan instance - for example, https:\/\/(instance_name).atlan.com . Ã° Âª Did you know? If you enable multiple Atlan domains, your users will be able to select the most relevant Atlan domain from a dropdown menu while using the browser extension. The default value in the dropdown will be the Atlan instance entered as Your Atlan domain . If your organization does not have multiple Atlan domains, only the default selection will be displayed. (Optional) For Advanced settings , you can configure the following: If you have multiple Atlan instances, toggle on Multiple Atlan domains and then enter the URLs of your additional Atlan instances. Click + Add to add more Atlan domains. If your data tools are hosted on custom domains rather than the standard SaaS domain of each tool, toggle on Custom data source domain . Click + Add to add more custom domains for data sources. For Connector , select a supported tool for the browser extension. In the adjacent field, enter the URL of your custom data source domain. info Ã° Âª Did you know? For any supported tools configured while setting up the managed browser extension, your users will not be able to update or remove these selections. They can, however, add additional custom domains for data sources. (Optional) For Advanced settings , you can configure the following: If you have multiple Atlan instances, toggle on Multiple Atlan domains and then enter the URLs of your additional Atlan instances. Click + Add to add more Atlan domains. If you have multiple Atlan instances, toggle on Multiple Atlan domains and then enter the URLs of your additional Atlan instances. Click + Add to add more Atlan domains. If your data tools are hosted on custom domains rather than the standard SaaS domain of each tool, toggle on Custom data source domain . Click + Add to add more custom domains for data sources. For Connector , select a supported tool for the browser extension. In the adjacent field, enter the URL of your custom data source domain. info Ã° Âª Did you know? For any supported tools configured while setting up the managed browser extension, your users will not be able to update or remove these selections. They can, however, add additional custom domains for data sources. If your data tools are hosted on custom domains rather than the standard SaaS domain of each tool, toggle on Custom data source domain . Click + Add to add more custom domains for data sources. For Connector , select a supported tool for the browser extension. In the adjacent field, enter the URL of your custom data source domain. Ã° Âª Did you know? For any supported tools configured while setting up the managed browser extension, your users will not be able to update or remove these selections. They can, however, add additional custom domains for data sources. Click the Download Script button to download the corresponding configuration script. The IT administrator(s) in your organization will need to install this configuration file in your organization's devices using a mobile device management (MDM) software. Administrative permissions to the MDM platform are required to complete the setup. Based on your operating system, the downloaded file can be one of the following two types: .mobileconfig - use this file to configure profiles with specific settings in macOS devices . .ps1 - use this PowerShell script to create registry keys in Microsoft Windows devices . Click the Download Script button to download the corresponding configuration script. The IT administrator(s) in your organization will need to install this configuration file in your organization's devices using a mobile device management (MDM) software. Administrative permissions to the MDM platform are required to complete the setup. Based on your operating system, the downloaded file can be one of the following two types: .mobileconfig - use this file to configure profiles with specific settings in macOS devices . .ps1 - use this PowerShell script to create registry keys in Microsoft Windows devices . Bulk install the browser extension You will need to have administrator access to your organization's mobile device management (MDM) software with the permission to add and deploy new policies to all users. You will also need inputs and approval from your Atlan admin. You will need to configure the ExtensionInstallForcelist browser policy for either Google Chrome or Microsoft Edge to force-install the extension for everyone in your organization. The ExtensionInstallForcelist browser policy: Governs extensions that can be silently installed and automatically enabled for all users. Provides extension IDs that the browser will automatically install and enable when a user logs in. Google Chrome To bulk install the Atlan browser extension in Google Chrome, follow the steps in Google documentation: Force install apps and extensions . Microsoft Edge To bulk install the Atlan browser extension in Microsoft Edge, follow the steps in Microsoft documentation: Force-install an extension . For the Extension\/App IDs and update URLs to be silently installed (Device) field, copy and paste the following value: fipjfjlalpnbejlmmpfnmlkadjgaaheg is the extension-id for the Atlan browser extension. Deploy the configuration script You will need to have administrator access to your organization's mobile device management (MDM) software with the permission to add and deploy new policies to all users. You will also need inputs and approval from your Atlan admin. The browser extension relies on managed storage for configuring domains in the Atlan extension. The values for managed storage can be configured through: A configuration profile in macOS Registry keys in Microsoft Windows Although Atlan's solution is platform-agnostic, the following example pertains to Microsoft Intune . You will need to create a custom managed profile to configure the domains for the Atlan browser extension. To deploy the .mobileconfig file for your organization, you can use any MDM platform. Microsoft Intune - follow the steps in Custom configuration profile settings . Microsoft Windows You will need to create registry keys to deploy the extension. You can create the required registry keys with a PowerShell script, which can then be deployed to your users devices using an MDM software. To deploy the PowerShell configuration script for your organization, you can use any MDM platform. Microsoft Intune - follow the steps in Create a script policy and assign it . For Script settings , enter the following details: Script location - upload the .ps1 configuration script downloaded from Atlan. Run this script using the logged on credentials - change to No . Enforce script signature check - change to No . Run script in 64 bit PowerShell Host - change to Yes . Script location - upload the .ps1 configuration script downloaded from Atlan. Run this script using the logged on credentials - change to No . Enforce script signature check - change to No . Run script in 64 bit PowerShell Host - change to Yes . Verify and monitor the installation To ensure that the Atlan browser extension has been successfully deployed across all selected devices in your organization, you can: Verify the installation - after you have deployed the policies, check a few target devices to ensure that the extension was installed and configured correctly. Monitor compliance - monitor the compliance status of the policy and troubleshoot any issues. Your users will now be able to use the Atlan browser extension in a managed browser! Ã° Once the managed browser has synced with the latest configuration changes for your organization, the Atlan browser extension will be automatically installed and a new tab will open to indicate that the Atlan browser extension is now active. Configure the browser extension Bulk install the browser extension Deploy the configuration script Verify and monitor the installation"}
{"url":"https:\/\/docs.atlan.com\/product\/integrations\/automation\/browser-extension\/how-tos\/use-the-atlan-browser-extension#configure-the-extension","title":"Use the Atlan browser extension | Atlan Documentation","text":"The Atlan browser extension provides metadata context directly in your supported data tools . You can use the extension in the following Chromium-based browsers: Google Chrome and Microsoft Edge. Install the extension To install Atlan's browser extension: You can either: Find the extension in the Chrome Web Store: https:\/\/chrome.google.com\/webstore\/detail\/atlan\/fipjfjlalpnbejlmmpfnmlkadjgaaheg From the upper right of any screen in Atlan, navigate to your name and then click Profile . Click the four dots icon in the resulting dialog to get to integrations. Under Apps , for Browser extension , click Install . Find the extension in the Chrome Web Store: https:\/\/chrome.google.com\/webstore\/detail\/atlan\/fipjfjlalpnbejlmmpfnmlkadjgaaheg From the upper right of any screen in Atlan, navigate to your name and then click Profile . Click the four dots icon in the resulting dialog to get to integrations. Under Apps , for Browser extension , click Install . Click the four dots icon in the resulting dialog to get to integrations. Under Apps , for Browser extension , click Install . To install the Atlan browser extension: For Google Chrome, in the upper right of your screen, click Add to Chrome . When prompted for confirmation, click the Add extension button. For Microsoft Edge, follow the steps in Add an extension to Microsoft Edge from the Chrome Web Store . For Google Chrome, in the upper right of your screen, click Add to Chrome . When prompted for confirmation, click the Add extension button. For Microsoft Edge, follow the steps in Add an extension to Microsoft Edge from the Chrome Web Store . Currently, you can't install the browser extension on mobile devices or tablets. You can also install Atlan's browser extension at the workspace level . To set this up, you need to be an administrator or have access to the admin console of your organization's Google account. If your organization uses managed browsers, you can configure the extension for managed browsers . Configure the extension Once installed, configure the Atlan browser extension to get started. Optionally, Atlan admins can preconfigure custom domains for data sources , if any. Configure the extension as a user To configure the browser extension, once installed: If you are logged into your Atlan instance, skip to the next step. If you haven't logged into Atlan, log in to your Atlan instance when prompted. In the Options page, to enter the URL of your Atlan instance: If your organization uses an Atlan domain (for example, _mycompany_.atlan.com ), the Atlan instance URL appears preselected. Click Get started . (Optional) Switch to a different Atlan domain, if required. If your organization uses a custom domain (for example, _atlan_.mycompany.com ), enter the URL of your Atlan instance and then click Get started . If your organization uses an Atlan domain (for example, _mycompany_.atlan.com ), the Atlan instance URL appears preselected. Click Get started . (Optional) Switch to a different Atlan domain, if required. If your organization uses a custom domain (for example, _atlan_.mycompany.com ), enter the URL of your Atlan instance and then click Get started . After a successful login, the message Updated successfully appears. (Optional) If your data tools are hosted on custom domains, rather than the standard SaaS domain of each tool: Click the Configure custom domain link at the bottom. In the dropdown on the left, select your data tool. In the text box on the right, enter the custom domain you use for that tool. Repeat these steps for each tool hosted on a custom domain. Click the Save button when finished. If your Atlan admin has preconfigured custom domains for data sources , you won't be able to update or remove these selections. Click + Add to configure custom domains for additional data sources as required. Click the Configure custom domain link at the bottom. In the dropdown on the left, select your data tool. In the text box on the right, enter the custom domain you use for that tool. Repeat these steps for each tool hosted on a custom domain. Click the Save button when finished. In the dropdown on the left, select your data tool. In the text box on the right, enter the custom domain you use for that tool. Repeat these steps for each tool hosted on a custom domain. Click the Save button when finished. If your Atlan admin has preconfigured custom domains for data sources , you won't be able to update or remove these selections. Click + Add to configure custom domains for additional data sources as required. You can now close the Options tab. The extension is now ready to use! Ã° (Optional) Configure custom domains as an admin You need to be an admin user in Atlan to configure custom domains for data sources from the admin center. To configure custom domains, from within Atlan: From the left menu of any screen, click Admin . Under Workspace , click Integrations . Under Apps , expand the Browser extension tile. In the Browser extension tile, for Set up your custom data source. , if your data tools are hosted on custom domains rather than the standard SaaS domain of each tool, click the Configure link to configure them for users in your organization. For Connector , select a supported tool for the browser extension. In the adjacent field, enter the URL of the custom domain for your data source. (Optional) Click + Add to add more. Click Save to save your configuration. info Ã° Âª Did you know? For any supported tools that you have configured, your users won't be able to update or remove these selections. They can, however, add additional custom domains for data sources. For Connector , select a supported tool for the browser extension. In the adjacent field, enter the URL of the custom domain for your data source. (Optional) Click + Add to add more. Click Save to save your configuration. Ã° Âª Did you know? For any supported tools that you have configured, your users won't be able to update or remove these selections. They can, however, add additional custom domains for data sources. (Optional) For Download Atlan extension or share with your team , you can either install the Atlan browser extension for your own use or share the link with your users. Anyone with access to Atlan any admin, member, or guest user and a supported tool can use the browser extension. First, log into Atlan. When using Atlan's browser extension in a supported tool , the extension only reads the URL of your browser tab no other data is accessed. If using Atlan's browser extension on any website , it only reads the favicon, page title, and URL of your browser tab. Learn more about Atlan browser extension security . Access and enrich context in-flow To access context for an asset, from within a supported tool: Log into the supported tool. Open any supported asset. In the lower-right corner of the page, click the small Atlan icon. danger The icon to activate Atlan is not the extension icon that appears at the top of your Chrome browser. This small Atlan icon in the lower right corner of the page is the only way to access the metadata for the asset you are viewing in another tool. The icon to activate Atlan is not the extension icon that appears at the top of your Chrome browser. This small Atlan icon in the lower right corner of the page is the only way to access the metadata for the asset you are viewing in another tool. In the sidebar that appears: Click the tabs and links to view all context about the asset. Make changes to any of the metadata you'd like. Click the tabs and links to view all context about the asset. Make changes to any of the metadata you'd like. Now you can understand and enrich assets without leaving your data tools themselves! Ã° The Atlan sidebar automatically reloads as you browse your assets in a supported tool to show details about the asset you're currently viewing. Your permissions in Atlan control what metadata you can see and change in the extension. The extension opens a new browser tab on Atlan's discovery page, with the results for that text! Ã° Add a resource You can link any web page as a resource to your assets in Atlan using the browser extension. To add a web page as a resource to an asset: In the top right of the web page you're viewing, click the Atlan Chrome extension . In the resource clipper menu, under Link this page to an asset , select the asset to which you'd like to add the web page as a resource. Click Save to confirm your selection. (Optional) Once the resource has been linked successfully, click the Open in Atlan button to view the linked asset directly in Atlan. You can now add resources to your assets in Atlan from any website! Ã° The Tableau extension offers native embeddings directly in your dashboards. See Enable embedded metadata in Tableau for more information. Supported tools Currently, the Atlan browser extension supports assets in the following tools: Amazon QuickSight : analyses, dashboards, and datasets Databricks : databases, schemas, views, and tables dbt Cloud : models and sources in the model editor and dbt docs Google BigQuery : datasets, schemas, views, and tables IBM Cognos Analytics : folders, dashboards, packages, explorations, reports, files, data sources, and modules Looker : dashboards, explores, and folders Microsoft Power BI : dashboards, reports, dataflows, and datasets Mode : collections, reports, queries, and charts Qlik Sense Cloud : apps, datasets, sheets, and spaces Redash : queries, dashboards, and visualizations Salesforce : objects Sigma : datasets, pages, and data elements Snowflake (via Snowsight schema explorer): databases, schemas, tables, views, dynamic tables, streams, and pipes Tableau : dashboards, data sources, workbooks, and metrics. Additionally, you can choose to switch the Tableau extension to offer native embeddings directly in your dashboards. See Enable embedded metadata in Tableau for more information. ThoughtSpot : liveboards, answers, visualizations, and tables MicroStrategy : dossiers, reports, documents Install the extension Configure the extension"}
{"url":"https:\/\/docs.atlan.com\/product\/integrations\/automation\/browser-extension\/how-tos\/use-the-atlan-browser-extension#optional-configure-custom-domains-as-an-admin","title":"Use the Atlan browser extension | Atlan Documentation","text":"The Atlan browser extension provides metadata context directly in your supported data tools . You can use the extension in the following Chromium-based browsers: Google Chrome and Microsoft Edge. Install the extension To install Atlan's browser extension: You can either: Find the extension in the Chrome Web Store: https:\/\/chrome.google.com\/webstore\/detail\/atlan\/fipjfjlalpnbejlmmpfnmlkadjgaaheg From the upper right of any screen in Atlan, navigate to your name and then click Profile . Click the four dots icon in the resulting dialog to get to integrations. Under Apps , for Browser extension , click Install . Find the extension in the Chrome Web Store: https:\/\/chrome.google.com\/webstore\/detail\/atlan\/fipjfjlalpnbejlmmpfnmlkadjgaaheg From the upper right of any screen in Atlan, navigate to your name and then click Profile . Click the four dots icon in the resulting dialog to get to integrations. Under Apps , for Browser extension , click Install . Click the four dots icon in the resulting dialog to get to integrations. Under Apps , for Browser extension , click Install . To install the Atlan browser extension: For Google Chrome, in the upper right of your screen, click Add to Chrome . When prompted for confirmation, click the Add extension button. For Microsoft Edge, follow the steps in Add an extension to Microsoft Edge from the Chrome Web Store . For Google Chrome, in the upper right of your screen, click Add to Chrome . When prompted for confirmation, click the Add extension button. For Microsoft Edge, follow the steps in Add an extension to Microsoft Edge from the Chrome Web Store . Currently, you can't install the browser extension on mobile devices or tablets. You can also install Atlan's browser extension at the workspace level . To set this up, you need to be an administrator or have access to the admin console of your organization's Google account. If your organization uses managed browsers, you can configure the extension for managed browsers . Configure the extension Once installed, configure the Atlan browser extension to get started. Optionally, Atlan admins can preconfigure custom domains for data sources , if any. Configure the extension as a user To configure the browser extension, once installed: If you are logged into your Atlan instance, skip to the next step. If you haven't logged into Atlan, log in to your Atlan instance when prompted. In the Options page, to enter the URL of your Atlan instance: If your organization uses an Atlan domain (for example, _mycompany_.atlan.com ), the Atlan instance URL appears preselected. Click Get started . (Optional) Switch to a different Atlan domain, if required. If your organization uses a custom domain (for example, _atlan_.mycompany.com ), enter the URL of your Atlan instance and then click Get started . If your organization uses an Atlan domain (for example, _mycompany_.atlan.com ), the Atlan instance URL appears preselected. Click Get started . (Optional) Switch to a different Atlan domain, if required. If your organization uses a custom domain (for example, _atlan_.mycompany.com ), enter the URL of your Atlan instance and then click Get started . After a successful login, the message Updated successfully appears. (Optional) If your data tools are hosted on custom domains, rather than the standard SaaS domain of each tool: Click the Configure custom domain link at the bottom. In the dropdown on the left, select your data tool. In the text box on the right, enter the custom domain you use for that tool. Repeat these steps for each tool hosted on a custom domain. Click the Save button when finished. If your Atlan admin has preconfigured custom domains for data sources , you won't be able to update or remove these selections. Click + Add to configure custom domains for additional data sources as required. Click the Configure custom domain link at the bottom. In the dropdown on the left, select your data tool. In the text box on the right, enter the custom domain you use for that tool. Repeat these steps for each tool hosted on a custom domain. Click the Save button when finished. In the dropdown on the left, select your data tool. In the text box on the right, enter the custom domain you use for that tool. Repeat these steps for each tool hosted on a custom domain. Click the Save button when finished. If your Atlan admin has preconfigured custom domains for data sources , you won't be able to update or remove these selections. Click + Add to configure custom domains for additional data sources as required. You can now close the Options tab. The extension is now ready to use! Ã° (Optional) Configure custom domains as an admin You need to be an admin user in Atlan to configure custom domains for data sources from the admin center. To configure custom domains, from within Atlan: From the left menu of any screen, click Admin . Under Workspace , click Integrations . Under Apps , expand the Browser extension tile. In the Browser extension tile, for Set up your custom data source. , if your data tools are hosted on custom domains rather than the standard SaaS domain of each tool, click the Configure link to configure them for users in your organization. For Connector , select a supported tool for the browser extension. In the adjacent field, enter the URL of the custom domain for your data source. (Optional) Click + Add to add more. Click Save to save your configuration. info Ã° Âª Did you know? For any supported tools that you have configured, your users won't be able to update or remove these selections. They can, however, add additional custom domains for data sources. For Connector , select a supported tool for the browser extension. In the adjacent field, enter the URL of the custom domain for your data source. (Optional) Click + Add to add more. Click Save to save your configuration. Ã° Âª Did you know? For any supported tools that you have configured, your users won't be able to update or remove these selections. They can, however, add additional custom domains for data sources. (Optional) For Download Atlan extension or share with your team , you can either install the Atlan browser extension for your own use or share the link with your users. Anyone with access to Atlan any admin, member, or guest user and a supported tool can use the browser extension. First, log into Atlan. When using Atlan's browser extension in a supported tool , the extension only reads the URL of your browser tab no other data is accessed. If using Atlan's browser extension on any website , it only reads the favicon, page title, and URL of your browser tab. Learn more about Atlan browser extension security . Access and enrich context in-flow To access context for an asset, from within a supported tool: Log into the supported tool. Open any supported asset. In the lower-right corner of the page, click the small Atlan icon. danger The icon to activate Atlan is not the extension icon that appears at the top of your Chrome browser. This small Atlan icon in the lower right corner of the page is the only way to access the metadata for the asset you are viewing in another tool. The icon to activate Atlan is not the extension icon that appears at the top of your Chrome browser. This small Atlan icon in the lower right corner of the page is the only way to access the metadata for the asset you are viewing in another tool. In the sidebar that appears: Click the tabs and links to view all context about the asset. Make changes to any of the metadata you'd like. Click the tabs and links to view all context about the asset. Make changes to any of the metadata you'd like. Now you can understand and enrich assets without leaving your data tools themselves! Ã° The Atlan sidebar automatically reloads as you browse your assets in a supported tool to show details about the asset you're currently viewing. Your permissions in Atlan control what metadata you can see and change in the extension. The extension opens a new browser tab on Atlan's discovery page, with the results for that text! Ã° Add a resource You can link any web page as a resource to your assets in Atlan using the browser extension. To add a web page as a resource to an asset: In the top right of the web page you're viewing, click the Atlan Chrome extension . In the resource clipper menu, under Link this page to an asset , select the asset to which you'd like to add the web page as a resource. Click Save to confirm your selection. (Optional) Once the resource has been linked successfully, click the Open in Atlan button to view the linked asset directly in Atlan. You can now add resources to your assets in Atlan from any website! Ã° The Tableau extension offers native embeddings directly in your dashboards. See Enable embedded metadata in Tableau for more information. Supported tools Currently, the Atlan browser extension supports assets in the following tools: Amazon QuickSight : analyses, dashboards, and datasets Databricks : databases, schemas, views, and tables dbt Cloud : models and sources in the model editor and dbt docs Google BigQuery : datasets, schemas, views, and tables IBM Cognos Analytics : folders, dashboards, packages, explorations, reports, files, data sources, and modules Looker : dashboards, explores, and folders Microsoft Power BI : dashboards, reports, dataflows, and datasets Mode : collections, reports, queries, and charts Qlik Sense Cloud : apps, datasets, sheets, and spaces Redash : queries, dashboards, and visualizations Salesforce : objects Sigma : datasets, pages, and data elements Snowflake (via Snowsight schema explorer): databases, schemas, tables, views, dynamic tables, streams, and pipes Tableau : dashboards, data sources, workbooks, and metrics. Additionally, you can choose to switch the Tableau extension to offer native embeddings directly in your dashboards. See Enable embedded metadata in Tableau for more information. ThoughtSpot : liveboards, answers, visualizations, and tables MicroStrategy : dossiers, reports, documents Install the extension Configure the extension"}
{"url":"https:\/\/docs.atlan.com\/product\/integrations\/automation\/browser-extension\/how-tos\/use-the-atlan-browser-extension#configure-the-extension-as-a-user","title":"Use the Atlan browser extension | Atlan Documentation","text":"The Atlan browser extension provides metadata context directly in your supported data tools . You can use the extension in the following Chromium-based browsers: Google Chrome and Microsoft Edge. Install the extension To install Atlan's browser extension: You can either: Find the extension in the Chrome Web Store: https:\/\/chrome.google.com\/webstore\/detail\/atlan\/fipjfjlalpnbejlmmpfnmlkadjgaaheg From the upper right of any screen in Atlan, navigate to your name and then click Profile . Click the four dots icon in the resulting dialog to get to integrations. Under Apps , for Browser extension , click Install . Find the extension in the Chrome Web Store: https:\/\/chrome.google.com\/webstore\/detail\/atlan\/fipjfjlalpnbejlmmpfnmlkadjgaaheg From the upper right of any screen in Atlan, navigate to your name and then click Profile . Click the four dots icon in the resulting dialog to get to integrations. Under Apps , for Browser extension , click Install . Click the four dots icon in the resulting dialog to get to integrations. Under Apps , for Browser extension , click Install . To install the Atlan browser extension: For Google Chrome, in the upper right of your screen, click Add to Chrome . When prompted for confirmation, click the Add extension button. For Microsoft Edge, follow the steps in Add an extension to Microsoft Edge from the Chrome Web Store . For Google Chrome, in the upper right of your screen, click Add to Chrome . When prompted for confirmation, click the Add extension button. For Microsoft Edge, follow the steps in Add an extension to Microsoft Edge from the Chrome Web Store . Currently, you can't install the browser extension on mobile devices or tablets. You can also install Atlan's browser extension at the workspace level . To set this up, you need to be an administrator or have access to the admin console of your organization's Google account. If your organization uses managed browsers, you can configure the extension for managed browsers . Configure the extension Once installed, configure the Atlan browser extension to get started. Optionally, Atlan admins can preconfigure custom domains for data sources , if any. Configure the extension as a user To configure the browser extension, once installed: If you are logged into your Atlan instance, skip to the next step. If you haven't logged into Atlan, log in to your Atlan instance when prompted. In the Options page, to enter the URL of your Atlan instance: If your organization uses an Atlan domain (for example, _mycompany_.atlan.com ), the Atlan instance URL appears preselected. Click Get started . (Optional) Switch to a different Atlan domain, if required. If your organization uses a custom domain (for example, _atlan_.mycompany.com ), enter the URL of your Atlan instance and then click Get started . If your organization uses an Atlan domain (for example, _mycompany_.atlan.com ), the Atlan instance URL appears preselected. Click Get started . (Optional) Switch to a different Atlan domain, if required. If your organization uses a custom domain (for example, _atlan_.mycompany.com ), enter the URL of your Atlan instance and then click Get started . After a successful login, the message Updated successfully appears. (Optional) If your data tools are hosted on custom domains, rather than the standard SaaS domain of each tool: Click the Configure custom domain link at the bottom. In the dropdown on the left, select your data tool. In the text box on the right, enter the custom domain you use for that tool. Repeat these steps for each tool hosted on a custom domain. Click the Save button when finished. If your Atlan admin has preconfigured custom domains for data sources , you won't be able to update or remove these selections. Click + Add to configure custom domains for additional data sources as required. Click the Configure custom domain link at the bottom. In the dropdown on the left, select your data tool. In the text box on the right, enter the custom domain you use for that tool. Repeat these steps for each tool hosted on a custom domain. Click the Save button when finished. In the dropdown on the left, select your data tool. In the text box on the right, enter the custom domain you use for that tool. Repeat these steps for each tool hosted on a custom domain. Click the Save button when finished. If your Atlan admin has preconfigured custom domains for data sources , you won't be able to update or remove these selections. Click + Add to configure custom domains for additional data sources as required. You can now close the Options tab. The extension is now ready to use! Ã° (Optional) Configure custom domains as an admin You need to be an admin user in Atlan to configure custom domains for data sources from the admin center. To configure custom domains, from within Atlan: From the left menu of any screen, click Admin . Under Workspace , click Integrations . Under Apps , expand the Browser extension tile. In the Browser extension tile, for Set up your custom data source. , if your data tools are hosted on custom domains rather than the standard SaaS domain of each tool, click the Configure link to configure them for users in your organization. For Connector , select a supported tool for the browser extension. In the adjacent field, enter the URL of the custom domain for your data source. (Optional) Click + Add to add more. Click Save to save your configuration. info Ã° Âª Did you know? For any supported tools that you have configured, your users won't be able to update or remove these selections. They can, however, add additional custom domains for data sources. For Connector , select a supported tool for the browser extension. In the adjacent field, enter the URL of the custom domain for your data source. (Optional) Click + Add to add more. Click Save to save your configuration. Ã° Âª Did you know? For any supported tools that you have configured, your users won't be able to update or remove these selections. They can, however, add additional custom domains for data sources. (Optional) For Download Atlan extension or share with your team , you can either install the Atlan browser extension for your own use or share the link with your users. Anyone with access to Atlan any admin, member, or guest user and a supported tool can use the browser extension. First, log into Atlan. When using Atlan's browser extension in a supported tool , the extension only reads the URL of your browser tab no other data is accessed. If using Atlan's browser extension on any website , it only reads the favicon, page title, and URL of your browser tab. Learn more about Atlan browser extension security . Access and enrich context in-flow To access context for an asset, from within a supported tool: Log into the supported tool. Open any supported asset. In the lower-right corner of the page, click the small Atlan icon. danger The icon to activate Atlan is not the extension icon that appears at the top of your Chrome browser. This small Atlan icon in the lower right corner of the page is the only way to access the metadata for the asset you are viewing in another tool. The icon to activate Atlan is not the extension icon that appears at the top of your Chrome browser. This small Atlan icon in the lower right corner of the page is the only way to access the metadata for the asset you are viewing in another tool. In the sidebar that appears: Click the tabs and links to view all context about the asset. Make changes to any of the metadata you'd like. Click the tabs and links to view all context about the asset. Make changes to any of the metadata you'd like. Now you can understand and enrich assets without leaving your data tools themselves! Ã° The Atlan sidebar automatically reloads as you browse your assets in a supported tool to show details about the asset you're currently viewing. Your permissions in Atlan control what metadata you can see and change in the extension. The extension opens a new browser tab on Atlan's discovery page, with the results for that text! Ã° Add a resource You can link any web page as a resource to your assets in Atlan using the browser extension. To add a web page as a resource to an asset: In the top right of the web page you're viewing, click the Atlan Chrome extension . In the resource clipper menu, under Link this page to an asset , select the asset to which you'd like to add the web page as a resource. Click Save to confirm your selection. (Optional) Once the resource has been linked successfully, click the Open in Atlan button to view the linked asset directly in Atlan. You can now add resources to your assets in Atlan from any website! Ã° The Tableau extension offers native embeddings directly in your dashboards. See Enable embedded metadata in Tableau for more information. Supported tools Currently, the Atlan browser extension supports assets in the following tools: Amazon QuickSight : analyses, dashboards, and datasets Databricks : databases, schemas, views, and tables dbt Cloud : models and sources in the model editor and dbt docs Google BigQuery : datasets, schemas, views, and tables IBM Cognos Analytics : folders, dashboards, packages, explorations, reports, files, data sources, and modules Looker : dashboards, explores, and folders Microsoft Power BI : dashboards, reports, dataflows, and datasets Mode : collections, reports, queries, and charts Qlik Sense Cloud : apps, datasets, sheets, and spaces Redash : queries, dashboards, and visualizations Salesforce : objects Sigma : datasets, pages, and data elements Snowflake (via Snowsight schema explorer): databases, schemas, tables, views, dynamic tables, streams, and pipes Tableau : dashboards, data sources, workbooks, and metrics. Additionally, you can choose to switch the Tableau extension to offer native embeddings directly in your dashboards. See Enable embedded metadata in Tableau for more information. ThoughtSpot : liveboards, answers, visualizations, and tables MicroStrategy : dossiers, reports, documents Install the extension Configure the extension"}
{"url":"https:\/\/docs.atlan.com\/product\/integrations\/automation\/browser-extension\/how-tos\/use-the-atlan-browser-extension#usage","title":"Use the Atlan browser extension | Atlan Documentation","text":"The Atlan browser extension provides metadata context directly in your supported data tools . You can use the extension in the following Chromium-based browsers: Google Chrome and Microsoft Edge. Install the extension To install Atlan's browser extension: You can either: Find the extension in the Chrome Web Store: https:\/\/chrome.google.com\/webstore\/detail\/atlan\/fipjfjlalpnbejlmmpfnmlkadjgaaheg From the upper right of any screen in Atlan, navigate to your name and then click Profile . Click the four dots icon in the resulting dialog to get to integrations. Under Apps , for Browser extension , click Install . Find the extension in the Chrome Web Store: https:\/\/chrome.google.com\/webstore\/detail\/atlan\/fipjfjlalpnbejlmmpfnmlkadjgaaheg From the upper right of any screen in Atlan, navigate to your name and then click Profile . Click the four dots icon in the resulting dialog to get to integrations. Under Apps , for Browser extension , click Install . Click the four dots icon in the resulting dialog to get to integrations. Under Apps , for Browser extension , click Install . To install the Atlan browser extension: For Google Chrome, in the upper right of your screen, click Add to Chrome . When prompted for confirmation, click the Add extension button. For Microsoft Edge, follow the steps in Add an extension to Microsoft Edge from the Chrome Web Store . For Google Chrome, in the upper right of your screen, click Add to Chrome . When prompted for confirmation, click the Add extension button. For Microsoft Edge, follow the steps in Add an extension to Microsoft Edge from the Chrome Web Store . Currently, you can't install the browser extension on mobile devices or tablets. You can also install Atlan's browser extension at the workspace level . To set this up, you need to be an administrator or have access to the admin console of your organization's Google account. If your organization uses managed browsers, you can configure the extension for managed browsers . Configure the extension Once installed, configure the Atlan browser extension to get started. Optionally, Atlan admins can preconfigure custom domains for data sources , if any. Configure the extension as a user To configure the browser extension, once installed: If you are logged into your Atlan instance, skip to the next step. If you haven't logged into Atlan, log in to your Atlan instance when prompted. In the Options page, to enter the URL of your Atlan instance: If your organization uses an Atlan domain (for example, _mycompany_.atlan.com ), the Atlan instance URL appears preselected. Click Get started . (Optional) Switch to a different Atlan domain, if required. If your organization uses a custom domain (for example, _atlan_.mycompany.com ), enter the URL of your Atlan instance and then click Get started . If your organization uses an Atlan domain (for example, _mycompany_.atlan.com ), the Atlan instance URL appears preselected. Click Get started . (Optional) Switch to a different Atlan domain, if required. If your organization uses a custom domain (for example, _atlan_.mycompany.com ), enter the URL of your Atlan instance and then click Get started . After a successful login, the message Updated successfully appears. (Optional) If your data tools are hosted on custom domains, rather than the standard SaaS domain of each tool: Click the Configure custom domain link at the bottom. In the dropdown on the left, select your data tool. In the text box on the right, enter the custom domain you use for that tool. Repeat these steps for each tool hosted on a custom domain. Click the Save button when finished. If your Atlan admin has preconfigured custom domains for data sources , you won't be able to update or remove these selections. Click + Add to configure custom domains for additional data sources as required. Click the Configure custom domain link at the bottom. In the dropdown on the left, select your data tool. In the text box on the right, enter the custom domain you use for that tool. Repeat these steps for each tool hosted on a custom domain. Click the Save button when finished. In the dropdown on the left, select your data tool. In the text box on the right, enter the custom domain you use for that tool. Repeat these steps for each tool hosted on a custom domain. Click the Save button when finished. If your Atlan admin has preconfigured custom domains for data sources , you won't be able to update or remove these selections. Click + Add to configure custom domains for additional data sources as required. You can now close the Options tab. The extension is now ready to use! Ã° (Optional) Configure custom domains as an admin You need to be an admin user in Atlan to configure custom domains for data sources from the admin center. To configure custom domains, from within Atlan: From the left menu of any screen, click Admin . Under Workspace , click Integrations . Under Apps , expand the Browser extension tile. In the Browser extension tile, for Set up your custom data source. , if your data tools are hosted on custom domains rather than the standard SaaS domain of each tool, click the Configure link to configure them for users in your organization. For Connector , select a supported tool for the browser extension. In the adjacent field, enter the URL of the custom domain for your data source. (Optional) Click + Add to add more. Click Save to save your configuration. info Ã° Âª Did you know? For any supported tools that you have configured, your users won't be able to update or remove these selections. They can, however, add additional custom domains for data sources. For Connector , select a supported tool for the browser extension. In the adjacent field, enter the URL of the custom domain for your data source. (Optional) Click + Add to add more. Click Save to save your configuration. Ã° Âª Did you know? For any supported tools that you have configured, your users won't be able to update or remove these selections. They can, however, add additional custom domains for data sources. (Optional) For Download Atlan extension or share with your team , you can either install the Atlan browser extension for your own use or share the link with your users. Anyone with access to Atlan any admin, member, or guest user and a supported tool can use the browser extension. First, log into Atlan. When using Atlan's browser extension in a supported tool , the extension only reads the URL of your browser tab no other data is accessed. If using Atlan's browser extension on any website , it only reads the favicon, page title, and URL of your browser tab. Learn more about Atlan browser extension security . Access and enrich context in-flow To access context for an asset, from within a supported tool: Log into the supported tool. Open any supported asset. In the lower-right corner of the page, click the small Atlan icon. danger The icon to activate Atlan is not the extension icon that appears at the top of your Chrome browser. This small Atlan icon in the lower right corner of the page is the only way to access the metadata for the asset you are viewing in another tool. The icon to activate Atlan is not the extension icon that appears at the top of your Chrome browser. This small Atlan icon in the lower right corner of the page is the only way to access the metadata for the asset you are viewing in another tool. In the sidebar that appears: Click the tabs and links to view all context about the asset. Make changes to any of the metadata you'd like. Click the tabs and links to view all context about the asset. Make changes to any of the metadata you'd like. Now you can understand and enrich assets without leaving your data tools themselves! Ã° The Atlan sidebar automatically reloads as you browse your assets in a supported tool to show details about the asset you're currently viewing. Your permissions in Atlan control what metadata you can see and change in the extension. The extension opens a new browser tab on Atlan's discovery page, with the results for that text! Ã° Add a resource You can link any web page as a resource to your assets in Atlan using the browser extension. To add a web page as a resource to an asset: In the top right of the web page you're viewing, click the Atlan Chrome extension . In the resource clipper menu, under Link this page to an asset , select the asset to which you'd like to add the web page as a resource. Click Save to confirm your selection. (Optional) Once the resource has been linked successfully, click the Open in Atlan button to view the linked asset directly in Atlan. You can now add resources to your assets in Atlan from any website! Ã° The Tableau extension offers native embeddings directly in your dashboards. See Enable embedded metadata in Tableau for more information. Supported tools Currently, the Atlan browser extension supports assets in the following tools: Amazon QuickSight : analyses, dashboards, and datasets Databricks : databases, schemas, views, and tables dbt Cloud : models and sources in the model editor and dbt docs Google BigQuery : datasets, schemas, views, and tables IBM Cognos Analytics : folders, dashboards, packages, explorations, reports, files, data sources, and modules Looker : dashboards, explores, and folders Microsoft Power BI : dashboards, reports, dataflows, and datasets Mode : collections, reports, queries, and charts Qlik Sense Cloud : apps, datasets, sheets, and spaces Redash : queries, dashboards, and visualizations Salesforce : objects Sigma : datasets, pages, and data elements Snowflake (via Snowsight schema explorer): databases, schemas, tables, views, dynamic tables, streams, and pipes Tableau : dashboards, data sources, workbooks, and metrics. Additionally, you can choose to switch the Tableau extension to offer native embeddings directly in your dashboards. See Enable embedded metadata in Tableau for more information. ThoughtSpot : liveboards, answers, visualizations, and tables MicroStrategy : dossiers, reports, documents Install the extension Configure the extension"}
{"url":"https:\/\/docs.atlan.com\/product\/integrations\/automation\/browser-extension\/concepts\/atlan-browser-extension-security","title":"Atlan browser extension security | Atlan Documentation","text":"Atlan adheres to strict security standards for the browser extension . Atlan mandates security throughout the extension coding lifecycle: Hardening configurations through content security policies, Validating all inputs, Requiring least privileges, Employing defense-in-depth techniques like code obfuscation to frustrate reverse engineering, Accessing customer resources over secure HTTPS channels after SSL certificate verification to prevent tampering. Atlan follows proven CI\/CD methodologies used for our SaaS application, enabling rapid and frequent updates to Atlan's Chrome extension. This allows: Patching identified vulnerabilities faster through new releases while simultaneously upholding stability. Mandatory code reviews specifically focused on analyzing security to help with identifying issues before these can impact customers. Once ready, both static and dynamic scanning tools rigorously test the extension codebase for any weaknesses before distribution. Atlan is committed to transparency. If any post-deployment points of concern arise, Atlan will notify impacted customers promptly and address their concerns responsibly. By incorporating security into each phase - secure architecture, peer reviews, robust testing, and responsible disclosure - Atlan strives to build browser extensions with both user needs and enterprise risks top of mind. Reach out to Atlan support for any questions. When using Atlan's browser extension in a supported tool , Atlan will read: the URL of your browser tab Document Object Model (DOM) elements such as asset title, hierarchy information, text, data-test-id attributes, and more to locate an asset in a supported source tool . This list may vary depending on the source tool. If you're using Atlan's browser extension on any website , it will only read the favicon, page title, and URL of your browser tab. Atlan uses the following permissions for the browser extension to work in a supported tool: activeTab - the activeTab permission allows the browser extension to temporarily access the content of the active tab as you interact with the extension. This enables Atlan to display the Atlan badge and read the URL and DOM elements to locate the asset for displaying asset metadata in the sidebar. storage - the storage permission allows Atlan to store information about the locally domains configured. This enables the browser extension to remember the sites that you want to use it on, even when you close and reopen your browser. host_permissions - the host permissions allow the browser extension to work specifically with Atlan tenants, which is the host in this case. For example, https:\/\/atlan.com\/* , https:\/\/atlan.dev\/* . \"content_scripts\": [ { \"matches\": [\"http:\/\/*\/*\", \"https:\/\/*\/*\"] - the content_scripts key allows Atlan to inject Atlan's content script to any website you visit. Although this content script will be injected into all webpages, it will neither be executed nor any DOM elements captured if the webpage is not a supported tool."}
{"url":"https:\/\/docs.atlan.com\/product\/integrations\/automation\/browser-extension\/how-tos\/use-the-atlan-browser-extension#access-and-enrich-context-in-flow","title":"Use the Atlan browser extension | Atlan Documentation","text":"The Atlan browser extension provides metadata context directly in your supported data tools . You can use the extension in the following Chromium-based browsers: Google Chrome and Microsoft Edge. Install the extension To install Atlan's browser extension: You can either: Find the extension in the Chrome Web Store: https:\/\/chrome.google.com\/webstore\/detail\/atlan\/fipjfjlalpnbejlmmpfnmlkadjgaaheg From the upper right of any screen in Atlan, navigate to your name and then click Profile . Click the four dots icon in the resulting dialog to get to integrations. Under Apps , for Browser extension , click Install . Find the extension in the Chrome Web Store: https:\/\/chrome.google.com\/webstore\/detail\/atlan\/fipjfjlalpnbejlmmpfnmlkadjgaaheg From the upper right of any screen in Atlan, navigate to your name and then click Profile . Click the four dots icon in the resulting dialog to get to integrations. Under Apps , for Browser extension , click Install . Click the four dots icon in the resulting dialog to get to integrations. Under Apps , for Browser extension , click Install . To install the Atlan browser extension: For Google Chrome, in the upper right of your screen, click Add to Chrome . When prompted for confirmation, click the Add extension button. For Microsoft Edge, follow the steps in Add an extension to Microsoft Edge from the Chrome Web Store . For Google Chrome, in the upper right of your screen, click Add to Chrome . When prompted for confirmation, click the Add extension button. For Microsoft Edge, follow the steps in Add an extension to Microsoft Edge from the Chrome Web Store . Currently, you can't install the browser extension on mobile devices or tablets. You can also install Atlan's browser extension at the workspace level . To set this up, you need to be an administrator or have access to the admin console of your organization's Google account. If your organization uses managed browsers, you can configure the extension for managed browsers . Configure the extension Once installed, configure the Atlan browser extension to get started. Optionally, Atlan admins can preconfigure custom domains for data sources , if any. Configure the extension as a user To configure the browser extension, once installed: If you are logged into your Atlan instance, skip to the next step. If you haven't logged into Atlan, log in to your Atlan instance when prompted. In the Options page, to enter the URL of your Atlan instance: If your organization uses an Atlan domain (for example, _mycompany_.atlan.com ), the Atlan instance URL appears preselected. Click Get started . (Optional) Switch to a different Atlan domain, if required. If your organization uses a custom domain (for example, _atlan_.mycompany.com ), enter the URL of your Atlan instance and then click Get started . If your organization uses an Atlan domain (for example, _mycompany_.atlan.com ), the Atlan instance URL appears preselected. Click Get started . (Optional) Switch to a different Atlan domain, if required. If your organization uses a custom domain (for example, _atlan_.mycompany.com ), enter the URL of your Atlan instance and then click Get started . After a successful login, the message Updated successfully appears. (Optional) If your data tools are hosted on custom domains, rather than the standard SaaS domain of each tool: Click the Configure custom domain link at the bottom. In the dropdown on the left, select your data tool. In the text box on the right, enter the custom domain you use for that tool. Repeat these steps for each tool hosted on a custom domain. Click the Save button when finished. If your Atlan admin has preconfigured custom domains for data sources , you won't be able to update or remove these selections. Click + Add to configure custom domains for additional data sources as required. Click the Configure custom domain link at the bottom. In the dropdown on the left, select your data tool. In the text box on the right, enter the custom domain you use for that tool. Repeat these steps for each tool hosted on a custom domain. Click the Save button when finished. In the dropdown on the left, select your data tool. In the text box on the right, enter the custom domain you use for that tool. Repeat these steps for each tool hosted on a custom domain. Click the Save button when finished. If your Atlan admin has preconfigured custom domains for data sources , you won't be able to update or remove these selections. Click + Add to configure custom domains for additional data sources as required. You can now close the Options tab. The extension is now ready to use! Ã° (Optional) Configure custom domains as an admin You need to be an admin user in Atlan to configure custom domains for data sources from the admin center. To configure custom domains, from within Atlan: From the left menu of any screen, click Admin . Under Workspace , click Integrations . Under Apps , expand the Browser extension tile. In the Browser extension tile, for Set up your custom data source. , if your data tools are hosted on custom domains rather than the standard SaaS domain of each tool, click the Configure link to configure them for users in your organization. For Connector , select a supported tool for the browser extension. In the adjacent field, enter the URL of the custom domain for your data source. (Optional) Click + Add to add more. Click Save to save your configuration. info Ã° Âª Did you know? For any supported tools that you have configured, your users won't be able to update or remove these selections. They can, however, add additional custom domains for data sources. For Connector , select a supported tool for the browser extension. In the adjacent field, enter the URL of the custom domain for your data source. (Optional) Click + Add to add more. Click Save to save your configuration. Ã° Âª Did you know? For any supported tools that you have configured, your users won't be able to update or remove these selections. They can, however, add additional custom domains for data sources. (Optional) For Download Atlan extension or share with your team , you can either install the Atlan browser extension for your own use or share the link with your users. Anyone with access to Atlan any admin, member, or guest user and a supported tool can use the browser extension. First, log into Atlan. When using Atlan's browser extension in a supported tool , the extension only reads the URL of your browser tab no other data is accessed. If using Atlan's browser extension on any website , it only reads the favicon, page title, and URL of your browser tab. Learn more about Atlan browser extension security . Access and enrich context in-flow To access context for an asset, from within a supported tool: Log into the supported tool. Open any supported asset. In the lower-right corner of the page, click the small Atlan icon. danger The icon to activate Atlan is not the extension icon that appears at the top of your Chrome browser. This small Atlan icon in the lower right corner of the page is the only way to access the metadata for the asset you are viewing in another tool. The icon to activate Atlan is not the extension icon that appears at the top of your Chrome browser. This small Atlan icon in the lower right corner of the page is the only way to access the metadata for the asset you are viewing in another tool. In the sidebar that appears: Click the tabs and links to view all context about the asset. Make changes to any of the metadata you'd like. Click the tabs and links to view all context about the asset. Make changes to any of the metadata you'd like. Now you can understand and enrich assets without leaving your data tools themselves! Ã° The Atlan sidebar automatically reloads as you browse your assets in a supported tool to show details about the asset you're currently viewing. Your permissions in Atlan control what metadata you can see and change in the extension. The extension opens a new browser tab on Atlan's discovery page, with the results for that text! Ã° Add a resource You can link any web page as a resource to your assets in Atlan using the browser extension. To add a web page as a resource to an asset: In the top right of the web page you're viewing, click the Atlan Chrome extension . In the resource clipper menu, under Link this page to an asset , select the asset to which you'd like to add the web page as a resource. Click Save to confirm your selection. (Optional) Once the resource has been linked successfully, click the Open in Atlan button to view the linked asset directly in Atlan. You can now add resources to your assets in Atlan from any website! Ã° The Tableau extension offers native embeddings directly in your dashboards. See Enable embedded metadata in Tableau for more information. Supported tools Currently, the Atlan browser extension supports assets in the following tools: Amazon QuickSight : analyses, dashboards, and datasets Databricks : databases, schemas, views, and tables dbt Cloud : models and sources in the model editor and dbt docs Google BigQuery : datasets, schemas, views, and tables IBM Cognos Analytics : folders, dashboards, packages, explorations, reports, files, data sources, and modules Looker : dashboards, explores, and folders Microsoft Power BI : dashboards, reports, dataflows, and datasets Mode : collections, reports, queries, and charts Qlik Sense Cloud : apps, datasets, sheets, and spaces Redash : queries, dashboards, and visualizations Salesforce : objects Sigma : datasets, pages, and data elements Snowflake (via Snowsight schema explorer): databases, schemas, tables, views, dynamic tables, streams, and pipes Tableau : dashboards, data sources, workbooks, and metrics. Additionally, you can choose to switch the Tableau extension to offer native embeddings directly in your dashboards. See Enable embedded metadata in Tableau for more information. ThoughtSpot : liveboards, answers, visualizations, and tables MicroStrategy : dossiers, reports, documents Install the extension Configure the extension"}
{"url":"https:\/\/docs.atlan.com\/product\/integrations\/automation\/browser-extension\/how-tos\/use-the-atlan-browser-extension#search-for-metadata","title":"Use the Atlan browser extension | Atlan Documentation","text":"The Atlan browser extension provides metadata context directly in your supported data tools . You can use the extension in the following Chromium-based browsers: Google Chrome and Microsoft Edge. Install the extension To install Atlan's browser extension: You can either: Find the extension in the Chrome Web Store: https:\/\/chrome.google.com\/webstore\/detail\/atlan\/fipjfjlalpnbejlmmpfnmlkadjgaaheg From the upper right of any screen in Atlan, navigate to your name and then click Profile . Click the four dots icon in the resulting dialog to get to integrations. Under Apps , for Browser extension , click Install . Find the extension in the Chrome Web Store: https:\/\/chrome.google.com\/webstore\/detail\/atlan\/fipjfjlalpnbejlmmpfnmlkadjgaaheg From the upper right of any screen in Atlan, navigate to your name and then click Profile . Click the four dots icon in the resulting dialog to get to integrations. Under Apps , for Browser extension , click Install . Click the four dots icon in the resulting dialog to get to integrations. Under Apps , for Browser extension , click Install . To install the Atlan browser extension: For Google Chrome, in the upper right of your screen, click Add to Chrome . When prompted for confirmation, click the Add extension button. For Microsoft Edge, follow the steps in Add an extension to Microsoft Edge from the Chrome Web Store . For Google Chrome, in the upper right of your screen, click Add to Chrome . When prompted for confirmation, click the Add extension button. For Microsoft Edge, follow the steps in Add an extension to Microsoft Edge from the Chrome Web Store . Currently, you can't install the browser extension on mobile devices or tablets. You can also install Atlan's browser extension at the workspace level . To set this up, you need to be an administrator or have access to the admin console of your organization's Google account. If your organization uses managed browsers, you can configure the extension for managed browsers . Configure the extension Once installed, configure the Atlan browser extension to get started. Optionally, Atlan admins can preconfigure custom domains for data sources , if any. Configure the extension as a user To configure the browser extension, once installed: If you are logged into your Atlan instance, skip to the next step. If you haven't logged into Atlan, log in to your Atlan instance when prompted. In the Options page, to enter the URL of your Atlan instance: If your organization uses an Atlan domain (for example, _mycompany_.atlan.com ), the Atlan instance URL appears preselected. Click Get started . (Optional) Switch to a different Atlan domain, if required. If your organization uses a custom domain (for example, _atlan_.mycompany.com ), enter the URL of your Atlan instance and then click Get started . If your organization uses an Atlan domain (for example, _mycompany_.atlan.com ), the Atlan instance URL appears preselected. Click Get started . (Optional) Switch to a different Atlan domain, if required. If your organization uses a custom domain (for example, _atlan_.mycompany.com ), enter the URL of your Atlan instance and then click Get started . After a successful login, the message Updated successfully appears. (Optional) If your data tools are hosted on custom domains, rather than the standard SaaS domain of each tool: Click the Configure custom domain link at the bottom. In the dropdown on the left, select your data tool. In the text box on the right, enter the custom domain you use for that tool. Repeat these steps for each tool hosted on a custom domain. Click the Save button when finished. If your Atlan admin has preconfigured custom domains for data sources , you won't be able to update or remove these selections. Click + Add to configure custom domains for additional data sources as required. Click the Configure custom domain link at the bottom. In the dropdown on the left, select your data tool. In the text box on the right, enter the custom domain you use for that tool. Repeat these steps for each tool hosted on a custom domain. Click the Save button when finished. In the dropdown on the left, select your data tool. In the text box on the right, enter the custom domain you use for that tool. Repeat these steps for each tool hosted on a custom domain. Click the Save button when finished. If your Atlan admin has preconfigured custom domains for data sources , you won't be able to update or remove these selections. Click + Add to configure custom domains for additional data sources as required. You can now close the Options tab. The extension is now ready to use! Ã° (Optional) Configure custom domains as an admin You need to be an admin user in Atlan to configure custom domains for data sources from the admin center. To configure custom domains, from within Atlan: From the left menu of any screen, click Admin . Under Workspace , click Integrations . Under Apps , expand the Browser extension tile. In the Browser extension tile, for Set up your custom data source. , if your data tools are hosted on custom domains rather than the standard SaaS domain of each tool, click the Configure link to configure them for users in your organization. For Connector , select a supported tool for the browser extension. In the adjacent field, enter the URL of the custom domain for your data source. (Optional) Click + Add to add more. Click Save to save your configuration. info Ã° Âª Did you know? For any supported tools that you have configured, your users won't be able to update or remove these selections. They can, however, add additional custom domains for data sources. For Connector , select a supported tool for the browser extension. In the adjacent field, enter the URL of the custom domain for your data source. (Optional) Click + Add to add more. Click Save to save your configuration. Ã° Âª Did you know? For any supported tools that you have configured, your users won't be able to update or remove these selections. They can, however, add additional custom domains for data sources. (Optional) For Download Atlan extension or share with your team , you can either install the Atlan browser extension for your own use or share the link with your users. Anyone with access to Atlan any admin, member, or guest user and a supported tool can use the browser extension. First, log into Atlan. When using Atlan's browser extension in a supported tool , the extension only reads the URL of your browser tab no other data is accessed. If using Atlan's browser extension on any website , it only reads the favicon, page title, and URL of your browser tab. Learn more about Atlan browser extension security . Access and enrich context in-flow To access context for an asset, from within a supported tool: Log into the supported tool. Open any supported asset. In the lower-right corner of the page, click the small Atlan icon. danger The icon to activate Atlan is not the extension icon that appears at the top of your Chrome browser. This small Atlan icon in the lower right corner of the page is the only way to access the metadata for the asset you are viewing in another tool. The icon to activate Atlan is not the extension icon that appears at the top of your Chrome browser. This small Atlan icon in the lower right corner of the page is the only way to access the metadata for the asset you are viewing in another tool. In the sidebar that appears: Click the tabs and links to view all context about the asset. Make changes to any of the metadata you'd like. Click the tabs and links to view all context about the asset. Make changes to any of the metadata you'd like. Now you can understand and enrich assets without leaving your data tools themselves! Ã° The Atlan sidebar automatically reloads as you browse your assets in a supported tool to show details about the asset you're currently viewing. Your permissions in Atlan control what metadata you can see and change in the extension. The extension opens a new browser tab on Atlan's discovery page, with the results for that text! Ã° Add a resource You can link any web page as a resource to your assets in Atlan using the browser extension. To add a web page as a resource to an asset: In the top right of the web page you're viewing, click the Atlan Chrome extension . In the resource clipper menu, under Link this page to an asset , select the asset to which you'd like to add the web page as a resource. Click Save to confirm your selection. (Optional) Once the resource has been linked successfully, click the Open in Atlan button to view the linked asset directly in Atlan. You can now add resources to your assets in Atlan from any website! Ã° The Tableau extension offers native embeddings directly in your dashboards. See Enable embedded metadata in Tableau for more information. Supported tools Currently, the Atlan browser extension supports assets in the following tools: Amazon QuickSight : analyses, dashboards, and datasets Databricks : databases, schemas, views, and tables dbt Cloud : models and sources in the model editor and dbt docs Google BigQuery : datasets, schemas, views, and tables IBM Cognos Analytics : folders, dashboards, packages, explorations, reports, files, data sources, and modules Looker : dashboards, explores, and folders Microsoft Power BI : dashboards, reports, dataflows, and datasets Mode : collections, reports, queries, and charts Qlik Sense Cloud : apps, datasets, sheets, and spaces Redash : queries, dashboards, and visualizations Salesforce : objects Sigma : datasets, pages, and data elements Snowflake (via Snowsight schema explorer): databases, schemas, tables, views, dynamic tables, streams, and pipes Tableau : dashboards, data sources, workbooks, and metrics. Additionally, you can choose to switch the Tableau extension to offer native embeddings directly in your dashboards. See Enable embedded metadata in Tableau for more information. ThoughtSpot : liveboards, answers, visualizations, and tables MicroStrategy : dossiers, reports, documents Install the extension Configure the extension"}
{"url":"https:\/\/docs.atlan.com\/product\/integrations\/automation\/browser-extension\/how-tos\/use-the-atlan-browser-extension#add-a-resource","title":"Use the Atlan browser extension | Atlan Documentation","text":"The Atlan browser extension provides metadata context directly in your supported data tools . You can use the extension in the following Chromium-based browsers: Google Chrome and Microsoft Edge. Install the extension To install Atlan's browser extension: You can either: Find the extension in the Chrome Web Store: https:\/\/chrome.google.com\/webstore\/detail\/atlan\/fipjfjlalpnbejlmmpfnmlkadjgaaheg From the upper right of any screen in Atlan, navigate to your name and then click Profile . Click the four dots icon in the resulting dialog to get to integrations. Under Apps , for Browser extension , click Install . Find the extension in the Chrome Web Store: https:\/\/chrome.google.com\/webstore\/detail\/atlan\/fipjfjlalpnbejlmmpfnmlkadjgaaheg From the upper right of any screen in Atlan, navigate to your name and then click Profile . Click the four dots icon in the resulting dialog to get to integrations. Under Apps , for Browser extension , click Install . Click the four dots icon in the resulting dialog to get to integrations. Under Apps , for Browser extension , click Install . To install the Atlan browser extension: For Google Chrome, in the upper right of your screen, click Add to Chrome . When prompted for confirmation, click the Add extension button. For Microsoft Edge, follow the steps in Add an extension to Microsoft Edge from the Chrome Web Store . For Google Chrome, in the upper right of your screen, click Add to Chrome . When prompted for confirmation, click the Add extension button. For Microsoft Edge, follow the steps in Add an extension to Microsoft Edge from the Chrome Web Store . Currently, you can't install the browser extension on mobile devices or tablets. You can also install Atlan's browser extension at the workspace level . To set this up, you need to be an administrator or have access to the admin console of your organization's Google account. If your organization uses managed browsers, you can configure the extension for managed browsers . Configure the extension Once installed, configure the Atlan browser extension to get started. Optionally, Atlan admins can preconfigure custom domains for data sources , if any. Configure the extension as a user To configure the browser extension, once installed: If you are logged into your Atlan instance, skip to the next step. If you haven't logged into Atlan, log in to your Atlan instance when prompted. In the Options page, to enter the URL of your Atlan instance: If your organization uses an Atlan domain (for example, _mycompany_.atlan.com ), the Atlan instance URL appears preselected. Click Get started . (Optional) Switch to a different Atlan domain, if required. If your organization uses a custom domain (for example, _atlan_.mycompany.com ), enter the URL of your Atlan instance and then click Get started . If your organization uses an Atlan domain (for example, _mycompany_.atlan.com ), the Atlan instance URL appears preselected. Click Get started . (Optional) Switch to a different Atlan domain, if required. If your organization uses a custom domain (for example, _atlan_.mycompany.com ), enter the URL of your Atlan instance and then click Get started . After a successful login, the message Updated successfully appears. (Optional) If your data tools are hosted on custom domains, rather than the standard SaaS domain of each tool: Click the Configure custom domain link at the bottom. In the dropdown on the left, select your data tool. In the text box on the right, enter the custom domain you use for that tool. Repeat these steps for each tool hosted on a custom domain. Click the Save button when finished. If your Atlan admin has preconfigured custom domains for data sources , you won't be able to update or remove these selections. Click + Add to configure custom domains for additional data sources as required. Click the Configure custom domain link at the bottom. In the dropdown on the left, select your data tool. In the text box on the right, enter the custom domain you use for that tool. Repeat these steps for each tool hosted on a custom domain. Click the Save button when finished. In the dropdown on the left, select your data tool. In the text box on the right, enter the custom domain you use for that tool. Repeat these steps for each tool hosted on a custom domain. Click the Save button when finished. If your Atlan admin has preconfigured custom domains for data sources , you won't be able to update or remove these selections. Click + Add to configure custom domains for additional data sources as required. You can now close the Options tab. The extension is now ready to use! Ã° (Optional) Configure custom domains as an admin You need to be an admin user in Atlan to configure custom domains for data sources from the admin center. To configure custom domains, from within Atlan: From the left menu of any screen, click Admin . Under Workspace , click Integrations . Under Apps , expand the Browser extension tile. In the Browser extension tile, for Set up your custom data source. , if your data tools are hosted on custom domains rather than the standard SaaS domain of each tool, click the Configure link to configure them for users in your organization. For Connector , select a supported tool for the browser extension. In the adjacent field, enter the URL of the custom domain for your data source. (Optional) Click + Add to add more. Click Save to save your configuration. info Ã° Âª Did you know? For any supported tools that you have configured, your users won't be able to update or remove these selections. They can, however, add additional custom domains for data sources. For Connector , select a supported tool for the browser extension. In the adjacent field, enter the URL of the custom domain for your data source. (Optional) Click + Add to add more. Click Save to save your configuration. Ã° Âª Did you know? For any supported tools that you have configured, your users won't be able to update or remove these selections. They can, however, add additional custom domains for data sources. (Optional) For Download Atlan extension or share with your team , you can either install the Atlan browser extension for your own use or share the link with your users. Anyone with access to Atlan any admin, member, or guest user and a supported tool can use the browser extension. First, log into Atlan. When using Atlan's browser extension in a supported tool , the extension only reads the URL of your browser tab no other data is accessed. If using Atlan's browser extension on any website , it only reads the favicon, page title, and URL of your browser tab. Learn more about Atlan browser extension security . Access and enrich context in-flow To access context for an asset, from within a supported tool: Log into the supported tool. Open any supported asset. In the lower-right corner of the page, click the small Atlan icon. danger The icon to activate Atlan is not the extension icon that appears at the top of your Chrome browser. This small Atlan icon in the lower right corner of the page is the only way to access the metadata for the asset you are viewing in another tool. The icon to activate Atlan is not the extension icon that appears at the top of your Chrome browser. This small Atlan icon in the lower right corner of the page is the only way to access the metadata for the asset you are viewing in another tool. In the sidebar that appears: Click the tabs and links to view all context about the asset. Make changes to any of the metadata you'd like. Click the tabs and links to view all context about the asset. Make changes to any of the metadata you'd like. Now you can understand and enrich assets without leaving your data tools themselves! Ã° The Atlan sidebar automatically reloads as you browse your assets in a supported tool to show details about the asset you're currently viewing. Your permissions in Atlan control what metadata you can see and change in the extension. The extension opens a new browser tab on Atlan's discovery page, with the results for that text! Ã° Add a resource You can link any web page as a resource to your assets in Atlan using the browser extension. To add a web page as a resource to an asset: In the top right of the web page you're viewing, click the Atlan Chrome extension . In the resource clipper menu, under Link this page to an asset , select the asset to which you'd like to add the web page as a resource. Click Save to confirm your selection. (Optional) Once the resource has been linked successfully, click the Open in Atlan button to view the linked asset directly in Atlan. You can now add resources to your assets in Atlan from any website! Ã° The Tableau extension offers native embeddings directly in your dashboards. See Enable embedded metadata in Tableau for more information. Supported tools Currently, the Atlan browser extension supports assets in the following tools: Amazon QuickSight : analyses, dashboards, and datasets Databricks : databases, schemas, views, and tables dbt Cloud : models and sources in the model editor and dbt docs Google BigQuery : datasets, schemas, views, and tables IBM Cognos Analytics : folders, dashboards, packages, explorations, reports, files, data sources, and modules Looker : dashboards, explores, and folders Microsoft Power BI : dashboards, reports, dataflows, and datasets Mode : collections, reports, queries, and charts Qlik Sense Cloud : apps, datasets, sheets, and spaces Redash : queries, dashboards, and visualizations Salesforce : objects Sigma : datasets, pages, and data elements Snowflake (via Snowsight schema explorer): databases, schemas, tables, views, dynamic tables, streams, and pipes Tableau : dashboards, data sources, workbooks, and metrics. Additionally, you can choose to switch the Tableau extension to offer native embeddings directly in your dashboards. See Enable embedded metadata in Tableau for more information. ThoughtSpot : liveboards, answers, visualizations, and tables MicroStrategy : dossiers, reports, documents Install the extension Configure the extension"}
{"url":"https:\/\/docs.atlan.com\/product\/capabilities\/discovery\/how-tos\/add-a-resource","title":"Add a resource | Atlan Documentation","text":"Need to redirect users to important information that's outside Atlan? You can add links to internal or external URLs wit hin an asset profile. These can help your team better understand the contextual information for your asset. To add resources to your assets, follow these steps: On the Atlan homepage, click Assets in the left menu. Click on an asset to open the asset profile. In the navigation bar to the right of the asset profile, click Resources . Click +Add Resource and paste your URL. For Title , type a title for your resource. Your resource is now ready to use! Ã° Once you've added a resource, click +Add in the Resource menu to add more resources for your asset. You can also edit or delete them to curate your list of resources. You can also add any web page as a resource to your assets using Atlan's Chrome extension ."}
{"url":"https:\/\/docs.atlan.com\/product\/integrations\/automation\/browser-extension\/how-tos\/enable-embedded-metadata-in-tableau","title":"Enable embedded metadata in Tableau | Atlan Documentation","text":"Atlan metadata layers Atlan context directly onto the source application, rather than in a sidebar. This embedded experience provides users with immediate access to data lineage, quality metrics, and governance information without leaving their Tableau workflow. Before enabling embedded metadata in Tableau: You must have the Atlan browser extension installed. If not, see the How to use the Atlan browser extension guide for instructions. Tableau dashboards must be available through the browser Permissions required To enable this feature, you need: Admin role in Atlan Access to the Labs settings Enable embedded metadata in Tableau To enable the embedded Atlan experience in Tableau for your users: From the left menu in Atlan, click Admin . From the left menu in Atlan, click Admin . Under Workspace , click Labs . Under Workspace , click Labs . Under the Access Control heading of the Labs page, turn on View embedded metadata in Tableau Under the Access Control heading of the Labs page, turn on View embedded metadata in Tableau Once enabled, users with the extension installed can view Atlan metadata in Tableau dashboards. Once enabled, users with the extension installed can view Atlan metadata in Tableau dashboards. See also Troubleshoot the Atlan browser extension Use the Atlan browser extension Tableau connector documentation Enable embedded metadata in Tableau"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/business-intelligence\/amazon-quicksight\/references\/what-does-atlan-crawl-from-amazon-quicksight","title":"What does Atlan crawl from Amazon QuickSight? | Atlan Documentation","text":"Atlan currently supports lineage for the Amazon QuickSight connector to the following data sources: Microsoft SQL Server Atlan crawls and maps the following assets and properties from Amazon QuickSight. Atlan maps analyses from Amazon QuickSight to its QuickSightAnalysis asset type. Atlan maps dashboards from Amazon QuickSight to its QuickSightDashboard asset type. Atlan maps datasets from Amazon QuickSight to its QuickSightDataset asset type. Atlan maps folders from Amazon QuickSight to its QuickSightFolder asset type. Dataset fields Atlan maps dataset fields from Amazon QuickSight to its QuickSightDatasetField asset type. Analysis visuals Atlan maps analysis visuals from Amazon QuickSight to its QuickSightAnalysisVisual asset type. Dashboard visuals Atlan maps dashboard visuals from Amazon QuickSight to its QuickSightDashboardVisual asset type."}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/databricks\/references\/what-does-atlan-crawl-from-databricks","title":"What does Atlan crawl from Databricks? | Atlan Documentation","text":"Atlan crawls and maps the following assets and properties from Databricks. The following properties aren't crawled by the System tables extraction method: Table properties : partitionList , partitionCount Column properties : maxLength , precision Atlan maps databases from Databricks to its Database asset type. Atlan maps schemas from Databricks to its Schema asset type. Atlan maps tables from Databricks to its Table asset type. Atlan maps views from Databricks to its View asset type. Materialized views Atlan maps materialized views from Databricks to its MaterialisedView asset type. To help you work seamlessly with STRUCT data types, Atlan supports nested columns up to level 30 in Databricks. You can view these columns in the Tree view or the asset sidebar of your table assets, and also explore child columns of STRUCTs nested within MAPs or ARRAYs. However, lineage for nested columns isn't supported. Atlan maps columns from Databricks to its Column asset type."}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/etl-tools\/dbt\/references\/what-does-atlan-crawl-from-dbt-cloud","title":"What does Atlan crawl from dbt Cloud? | Atlan Documentation","text":"Atlan crawls and maps the following assets and properties from dbt Cloud. Atlan also supports lineage between the following: SQL tables and views materialized by dbt models, dbt seeds, dbt sources Column-level lineage for these entities Atlan only crawls dbt assets that are in the applied (built) state in dbt Cloud. Models must be part of a successful run to be picked up during crawling; models that are only defined in your project files but haven t been executed won t be included. For more information about project state, see Project states in dbt Cloud. Once you've crawled dbt , you can use dbt-specific filters for quick asset discovery: Test status - filter dbt tests that passed, failed, or have a warning or error Alias - filter by the name of a dbt model's identifier in the dbt project Unique id - filter by the unique node identifier of a dbt model Project name - filter by dbt project name, only supported for dbt Core version 1.6+ Environment name - filter by dbt environment name Job status - filter by dbt job status Last job run - filter by the last run of the dbt job Atlan's dbt connectivity also populates custom metadata to further enrich the assets in Atlan. The Atlan dbt-specific property column in the tables below gives the name of the mapped custom metadata property in Atlan. Atlan enables you to sync your dbt tags and update your dbt assets with the synced tags. It's also possible to map other metadata on Atlan's assets through your dbt models . Atlan maps tables from dbt Cloud to its Table asset type. Atlan maps columns from dbt Cloud to its Column asset type. Atlan maps models from dbt Cloud to its Model asset type. Atlan maps sources from dbt Cloud to its DbtSource asset type. Atlan maps tests from dbt Cloud to its Test asset type. Atlan maps models from dbt Core to its Seed asset type."}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/google-bigquery\/references\/what-does-atlan-crawl-from-google-bigquery","title":"What does Atlan crawl from Google BigQuery? | Atlan Documentation","text":"Once you have crawled Google BigQuery , you can use connector-specific filters for quick asset discovery. The following filters are currently supported for these assets: Tables - BigQuery labels and Is sharded filters Atlan doesn't run any table scans. Atlan leverages the table preview options from Google BigQuery that enable you to view data for free and without affecting any quotas using the tabledata.list API. Hence, table asset previews in Atlan are already cost-optimized. However, this doesn't apply to views and materialized views . For Google BigQuery views and materialized views , Atlan sends you a cost nudge before viewing a sample data preview. This informs you about the precise bytes that are spent during the execution of the query, helping you decide if you still want to run the preview. You also receive a cost nudge before querying your Google BigQuery assets . Atlan crawls and maps the following assets and properties from Google BigQuery. Atlan maps projects from Google BigQuery to its Database asset type. Atlan maps datasets from Google BigQuery to its Schema asset type. Table asset previews are already cost-optimized. Google BigQuery enables you to use the table preview options to view data for free and without affecting any quotas. Note that this isn't currently supported for Google BigQuery views and materialized views in Atlan. Atlan maps tables from Google BigQuery to its Table asset type. Atlan maps views from Google BigQuery to its View asset type. Materialized views Atlan maps materialized views from Google BigQuery to its MaterialisedView asset type. Atlan supports nested columns up to level 1 for Google BigQuery to help you enrich your semi-structured data types. You can view nested columns in the asset sidebar for your table assets. Atlan maps columns from Google BigQuery to its Column asset type. Atlan doesn't crawl primary key (PK) and foreign key (FK) information from Google BigQuery. Stored procedures Atlan maps stored procedures from Google BigQuery to its Procedure asset type."}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/business-intelligence\/ibm-cognos-analytics\/references\/what-does-atlan-crawl-from-ibm-cognos-analytics","title":"What does Atlan crawl from IBM Cognos Analytics? | Atlan Documentation","text":"Atlan crawls and maps the following assets and properties from IBM Cognos Analytics. Atlan also supports lineage: For packages, files, reports, and modules. To upstream sources - Microsoft SQL Server and Snowflake. Field-level lineage is currently not supported. Atlan generates the sourceURL property for IBM Cognos Analytics assets using a combination of the host, port, and id of the asset. This allows Atlan to help you view your assets directly in IBM Cognos Analytics from the asset profile. Direct extraction - in addition to id , Atlan obtains the host and port values from the credentials you provided while setting up a crawler workflow. Offline extraction - in addition to id , Atlan obtains the host and port values from the parameters with which the offline extractor is executed. Assets marked with Ã° includes lineage and column-level lineage. Assets marked with Ã° display column information. Atlan maps folders from IBM Cognos Analytics to its CognosFolder asset type. Dashboards Ã° Atlan maps dashboards from IBM Cognos Analytics to its CognosDashboard asset type. Packages Ã° Atlan maps packages from IBM Cognos Analytics to its CognosPackage asset type. Explorations Ã° Atlan maps explorations from IBM Cognos Analytics to its CognosExploration asset type. Reports Ã° Atlan maps reports from IBM Cognos Analytics to its CognosReport asset type. Files Ã° Atlan maps files from IBM Cognos Analytics to its CognosFile asset type. Data sources Atlan maps data sources from IBM Cognos Analytics to its CognosDatasource asset type. Modules Ã° Atlan maps modules from IBM Cognos Analytics to its CognosModule asset type. Atlan maps fields from IBM Cognos Analytics to its CognosColumns asset type. Based on the asset type, some attributes may not be extracted: cognosColumnRegularAggregate appears only for reports and datasets. cognosColumnDatatype appears only for modules."}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/business-intelligence\/looker\/references\/what-does-atlan-crawl-from-looker","title":"What does Atlan crawl from Looker? | Atlan Documentation","text":"Atlan crawls and maps the following assets and properties from Looker. Atlan also supports the following lineage: Asset-level lineage for views, models, looks, dashboards, tiles, and explores. Field-level lineage for views, explores, looks, tiles, and dashboards. Lineage between explore fields and dashboards. This allows you to view all the fields used in a given dashboard and trace their upstream lineage to SQL columns. Cross-project lineage for Looker assets. For example, if an explore includes a view from an imported project, Atlan will parse project manifest files to generate lineage. Looker refinements for views and explores. Atlan will parse project manifest files to generate lineage. Refined fields for views and explores are displayed with a Refinement label in Atlan. Currently Atlan only represents the assets marked with Ã° in lineage. Atlan maps connections from Looker to its Connection asset type. Atlan maps projects from Looker to its LookerProject asset type. Views Ã° Atlan maps views from Looker to its LookerView asset type. To trace the upstream lineage of these views, Atlan currently supports SQL-based derived tables . Persistent derived tables (PDTs) and Liquid parameterized tables are currently not supported. However, Atlan will always catalog the associated views. Atlan also supports view refinements . Atlan includes the fields from refinements in the parent view asset, and marks the fields with a Refinement label. You can hover over the label to view the file path and line number where the refinement is defined. Models Ã° Atlan maps models from Looker to its LookerModel asset type. Atlan maps folders from Looker to its LookerFolder asset type. Fields Ã° For explores Atlan maps fields for explores from Looker to its LookerField asset type. For views Atlan maps fields for views from Looker to its LookerField asset type. For looks Atlan maps fields for looks from Looker to its LookerField asset type. For tiles Atlan maps fields for tiles from Looker to its LookerField asset type. For dashboards Atlan maps fields for dashboards from Looker to its LookerField asset type. Looks Ã° Atlan maps looks from Looker to its LookerLook asset type. Dashboards Ã° Atlan maps dashboards from Looker to its LookerDashboard asset type. Tiles Ã° Atlan maps tiles from Looker to its LookerTile asset type. Explores Ã° Atlan maps explores from Looker to its LookerExplore asset type. Atlan also supports explore refinements : For explores defined in the same model, Atlan includes the fields from refinements in the parent explore asset. For explores with the same name that are defined in a different model, Atlan will create a new explore asset. In both cases, Atlan marks the fields with a Refinement label. You can hover over the label to view the file path and line number where the refinement is defined."}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/business-intelligence\/microsoft-power-bi\/references\/what-does-atlan-crawl-from-microsoft-power-bi","title":"What does Atlan crawl from Microsoft Power BI? | Atlan Documentation","text":"Atlan crawls and maps the following assets and properties from Microsoft Power BI. Once you've crawled Microsoft Power BI , you can use connector-specific filters for quick asset discovery. The following filters are currently supported for these assets: Measures - External measure filter Currently Atlan only represents the assets marked with Ã° in lineage. For your Microsoft Power BI reports , Atlan also provides asset previews to help with quick discovery and give you the context you need. Atlan maps Apps from Microsoft Power BI to its PowerBIApp asset type. Atlan maps workspaces from Microsoft Power BI to its PowerBIWorkspace asset type. Dashboards Ã° Atlan maps dashboards from Microsoft Power BI to its PowerBIDashboard asset type. Data sources Atlan maps data sources from Microsoft Power BI to its PowerBIDatasource asset type. Datasets Ã° Atlan maps datasets from Microsoft Power BI to its PowerBIDataset asset type. Dataflows Ã° Atlan maps dataflows from Microsoft Power BI to its PowerBIDataflow asset type. Atlan currently only supports dataflow lineage for the following SQL sources: Microsoft SQL Server Dataflow entity columns Ã° Atlan maps attributes of dataflow entities from Microsoft Power BI to its PowerBIDataflowEntityColumn asset type. Reports Ã° Atlan maps reports from Microsoft Power BI to its PowerBIReport asset type. Pages Ã° Atlan maps pages from Microsoft Power BI to its PowerBIPage asset type. Tiles Ã° Atlan maps tiles from Microsoft Power BI to its PowerBITile asset type. Tables Ã° Atlan maps tables from Microsoft Power BI to its PowerBITable asset type. Columns Ã° Atlan maps columns from Microsoft Power BI to its PowerBIColumn asset type. Measures Ã° Atlan maps measures from Microsoft Power BI to its PowerBIMeasure asset type. Atlan supports PowerBI Measures for downstream lineage to a PowerBI Page. Dataflow entity columns Ã°"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/business-intelligence\/mode\/references\/what-does-atlan-crawl-from-mode","title":"What does Atlan crawl from Mode? | Atlan Documentation","text":"Atlan crawls and maps the following assets and properties from Mode. Atlan maps workspaces from Mode to its ModeWorkspace asset type. Atlan maps collections from Mode to its ModeCollection asset type. Atlan maps reports from Mode to its ModeReport asset type. Atlan maps queries from Mode to its ModeQuery asset type. Atlan maps charts from Mode to its ModeChart asset type."}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/business-intelligence\/qlik-sense-cloud\/references\/what-does-atlan-crawl-from-qlik-sense-cloud","title":"What does Atlan crawl from Qlik Sense Cloud? | Atlan Documentation","text":"Atlan crawls and maps the following assets and properties from Qlik Sense Cloud. Once you've crawled Qlik Sense Cloud , you can use connector-specific filters for quick asset discovery. The following filters are currently supported for these assets: Atlan only supports asset-level lineage for the following asset types: Datasets --> Charts --> Sheets --> Apps Atlan maps spaces from Qlik Sense Cloud to its QlikSpace asset type. Atlan maps apps from Qlik Sense Cloud to its QlikApp asset type. Only the app resource type is retrieved. Other types, such as qvapp or qlikview , are not crawled. Atlan maps sheets from Qlik Sense Cloud to its QlikSheet asset type. Atlan maps charts from Qlik Sense Cloud to the QlikChart asset type and catalogs only those linked to dataset fields. For example, table charts are crawled because their columns represent dataset dimensions or measures. UI elements that do not reference dataset fields - such as filters, buttons, and text elements - are ignored. These elements are not considered charts and are not crawled: filterpane , qlik-button-for-navigation , VizlibAdvancedTextObject , listbox , action-button , VizlibFilter , variable , text-image , VizlibLineObject . Atlan maps datasets from Qlik Sense Cloud to the QlikDataset asset type. Datasets loaded through the Data Load Editor are called implicit datasets in Atlan and appear under this type."}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/business-intelligence\/redash\/references\/what-does-atlan-crawl-from-redash","title":"What does Atlan crawl from Redash? | Atlan Documentation","text":"Atlan crawls and maps the following assets and properties from Redash. Once you've crawled Redash , you can use connector-specific filters for quick asset discovery. The following filters are currently supported for these assets: Queries - Schedule, Is Published, and Redash tags filters Visualizations - Type filter Dashboards - Redash tags filter Currently, Atlan only represents the assets marked with Ã° in lineage. Queries Ã° Atlan maps queries from Redash to its RedashQuery asset type. Dashboards Ã° Atlan maps dashboards from Redash to its RedashDashboard asset type. Visualizations Ã° Atlan maps visualization elements from Redash to its RedashVisualization asset type."}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/crm\/salesforce\/references\/what-does-atlan-crawl-from-salesforce","title":"What does Atlan crawl from Salesforce? | Atlan Documentation","text":"Atlan only performs GET requests on these five endpoints: sObject Basic Information Each endpoint will be set in its own OAuth client session. For every API request, it will hit the Salesforce login endpoint, which means there will be at least five (same as the number of endpoints above) login entries in your Salesforce account's login history within the duration of the scheduled workflow run. Atlan crawls and maps the following assets and properties from Salesforce. Once you've crawled Salesforce , you can use connector-specific filters for quick asset discovery. The following filters are currently supported for these assets: Fields - Is encrypted and Is required filters Atlan maps organizations from Salesforce to its SalesforceOrganization asset type. Atlan maps objects from Salesforce to its SalesforceObject asset type. Atlan maps fields from Salesforce to its SalesforceField asset type. Atlan maps reports from Salesforce to its SalesforceReport asset type. Atlan maps dashboards from Salesforce to its SalesforceDashboard asset type."}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/business-intelligence\/sigma\/references\/what-does-atlan-crawl-from-sigma","title":"What does Atlan crawl from Sigma? | Atlan Documentation","text":"Atlan crawls and maps the following assets and properties from Sigma. Currently, Atlan only represents the assets marked with Ã° in lineage. For your Sigma workbooks , Atlan also provides asset previews to help with quick discovery and give you the context you need. Workbooks Ã° Atlan maps workbooks from Sigma to its SigmaWorkbook asset type. Pages Ã° Atlan maps pages from Sigma to its SigmaPage asset type. Data elements Ã° Atlan maps table, pivot table, and visualization elements from Sigma to its SigmaDataElement asset type. Data element fields Ã° Atlan maps table, pivot table, and visualization element fields from Sigma to its SigmaDataElementField asset type. Atlan maps datasets from Sigma to its SigmaDataset asset type. Data elements Ã° Data element fields Ã°"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/data-warehouses\/snowflake\/references\/what-does-atlan-crawl-from-snowflake","title":"What does Atlan crawl from Snowflake? | Atlan Documentation","text":"Atlan crawls and maps the following assets and properties from Snowflake. Once you've crawled Snowflake , you can use connector-specific filters for quick asset discovery. The following filters are currently supported for Snowflake assets: Streams - Source type and Stale filters Functions - Language, Function type, Is secure, and Is external filters Snowflake tags and tag values Atlan supports lineage for the following asset types: External Named Stages Pipe Table Internal Named Stages Pipe Table (auto-ingest not recommended) Not supported for External or Iceberg Tables Atlan maps databases from Snowflake to its Database asset type. Atlan maps schemas from Snowflake to its Schema asset type. Atlan maps tables from Snowflake to its Table asset type. For Iceberg tables In addition to the table metadata above, Atlan supports additional metadata for Iceberg tables crawled from Snowflake. Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method. Atlan maps views from Snowflake to its View asset type. Materialized views Atlan maps materialized views from Snowflake to its MaterialisedView asset type. External tables Atlan maps external tables from Snowflake to its Table asset type. Atlan maps columns from Snowflake to its Column asset type. Atlan maps stages from Snowflake to its Stage asset type. Atlan maps streams from Snowflake to its Stream asset type. Atlan maps pipes from Snowflake to its Pipe asset type. User-defined functions Atlan maps user-defined functions (UDFs) from Snowflake to its Function asset type. Atlan currently does not support lineage for user-defined functions from Snowflake. Dynamic tables Atlan maps dynamic tables from Snowflake to its DynamicTable asset type."}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/business-intelligence\/tableau\/references\/what-does-atlan-crawl-from-tableau","title":"What does Atlan crawl from Tableau? | Atlan Documentation","text":"Atlan crawls and maps the following assets and properties from Tableau. Once you've crawled Tableau , you can use connector-specific filters for quick asset discovery. The following filters are currently supported for these assets: Projects - filter Tableau assets by projects, including nested projects Data sources - Is published filter For your Tableau worksheets and dashboards , Atlan also provides asset previews to help with quick discovery and give you the context you need. You may need to disable clickjack protection for Tableau asset previews to load. Did you know? Lineage to dashboards may appear incomplete or missing if worksheets are not crawled. Additionally, Tableau assets that haven't been refreshed since May 27, 2025, won't display the new column-level lineage (CLL) or updated lineage paths. Atlan supports lineage for the following: Asset Lineage - Datasource to Dashboard, Datasource to Worksheet, Datasource to Workbook Column Level Lineage - Supported for Datasource to Worksheet and Worksheet to Dashboard Atlan maps sites from Tableau to its TableauSite asset type. Atlan maps projects from Tableau to its TableauProject asset type. Due to limitations at source, Atlan won't be able to crawl Tableau flows if you use the JWT bearer authentication method. Atlan maps flows from Tableau to its TableauFlow asset type. Tableau has retired metrics methods in API 3.22 for Tableau Cloud and Tableau Server version 2024.2. If you're using Tableau API version 3.22 or higher, metadata for metrics is unavailable in Atlan. Atlan maps metrics from Tableau to its TableauMetric asset type. Atlan maps workbooks from Tableau to its TableauWorkbook asset type. Atlan maps worksheets from Tableau to its TableauWorksheet asset type. Atlan maps dashboards from Tableau to its TableauDashboard asset type. Data sources Atlan maps data sources (embedded and published) from Tableau to its TableauDatasource asset type. Data source fields Atlan maps data source fields and column fields from Tableau to its TableauDatasourceField asset type. Custom SQL Atlan parses custom SQL queries used in Tableau data sources to extract lineage information. This process identifies the relationships between data assets based on the SQL logic defined within Tableau. Calculated fields Atlan maps calculated fields from Tableau to its TableauCalculatedField asset type. Atlan calculates lineage for Tableau as follows: Lineage is currently not supported for Tableau flows and metrics . Data source fields"}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/business-intelligence\/thoughtspot\/references\/what-does-atlan-crawl-from-thoughtspot","title":"What does Atlan crawl from ThoughtSpot? | Atlan Documentation","text":"Once you've crawled ThoughtSpot , you can use connector-specific filters for quick asset discovery. The following filters are currently supported for all ThoughtSpot assets: Tags and chart type filters Atlan supports lineage for the following ThoughtSpot assets: Answers - upstream lineage to tables, views, or worksheets from multiple sources (if applicable), no downstream lineage Visualizations - upstream lineage to tables, views, or worksheets from multiple sources (if applicable) Liveboards - upstream lineage to visualizations Tables - upstream lineage to source tables, and column-level lineage between ThoughtSpot tables and worksheets Views - upstream lineage to ThoughtSpot tables or worksheets, and column-level lineage between ThoughtSpot views and worksheets Worksheets - upstream lineage to ThoughtSpot tables or views from multiple sources (if applicable) Atlan crawls and maps the following assets and properties from ThoughtSpot. Currently, Atlan only represents the assets marked with Ã° in lineage. Answers Ã° Atlan maps answers from ThoughtSpot to its ThoughtSpotAnswer asset type. Visualizations Ã° Atlan maps visualizations from ThoughtSpot to its ThoughtspotDashlet asset type. Liveboards Ã° Atlan maps Liveboards from ThoughtSpot to its ThoughtspotLiveboard asset type. Tables Ã° Atlan maps tables from ThoughtSpot to its ThoughtspotTable asset type. Views Ã° Atlan maps views from ThoughtSpot to its ThoughtspotView asset type. Worksheets Ã° Atlan maps worksheets from ThoughtSpot to its ThoughtspotWorksheet asset type. Columns Ã° Atlan maps columns from ThoughtSpot to its ThoughtspotColumn asset type."}
{"url":"https:\/\/docs.atlan.com\/apps\/connectors\/business-intelligence\/microstrategy\/references\/what-does-atlan-crawl-from-microstrategy","title":"What does Atlan crawl from MicroStrategy? | Atlan Documentation","text":"Atlan crawls and maps the following assets and properties from MicroStrategy. Currently Atlan only represents the assets marked with Ã° in lineage. Once you've crawled MicroStrategy , you can use connector-specific filters for quick asset discovery. The following filters are currently supported for these assets: Projects , attributes , facts , metrics , cubes , reports , documents , dossiers , and visualizations - Is Certified filter Cubes , reports , and visualizations - Type filter Atlan maps projects from MicroStrategy to its MicroStrategyProject asset type. Atlan maps attributes from MicroStrategy to its MicroStrategyAttribute asset type. Atlan maps facts from MicroStrategy to its MicroStrategyFact asset type. Atlan maps metrics from MicroStrategy to its MicroStrategyMetric asset type. Cubes Ã° Atlan maps cubes from MicroStrategy to its MicroStrategyCube asset type. Reports Ã° Atlan maps reports from MicroStrategy to its MicroStrategyReport asset type. Documents Ã° Atlan maps documents from MicroStrategy to its MicroStrategyDocument asset type. Dossiers Ã° Atlan maps dossiers from MicroStrategy to its MicroStrategyDossier asset type. Atlan maps dossier visualizations from MicroStrategy to its MicroStrategyVisualization asset type."}
{"url":"https:\/\/docs.atlan.com\/get-started\/how-tos\/quick-start-for-admins#__docusaurus_skipToContent_fallback","title":"Administrators | Atlan Documentation","text":"User management User management is a critical part of data governance. Atlan's user management capabilities should be a mainstay of how you organize and control access for people in your organization. Add and manage users from the admin center It's super simple to invite and remove users from Atlan from the Admin center . You can also manage existing users by adding them to groups, changing their roles, or set up SSO , SCIM , and SMTP configurations. Manage access control from the governance center The Governance center is where you can build access control mechanisms to manage user access . Personas allow you to group users into teams, such as Financial Analysts or Cloud Engineers , and set policies based on the access those personas should have. Purposes are where you can build policies based on the actions or access that a user might need. For example, you can use Atlan's policy-based access controls to manage access to PII and other sensitive data. This is a best practice for data governance. Once you set these policies, Atlan will enforce them throughout your users' experience. This means that users who don't have access to a particular type of data will not be able to see it. Governance workflows help you set up robust controls on data access management, metadata enrichment, new entity creation, and more, with out-of-the-box workflow templates and automated execution. Asset profile The asset profile in Atlan gives you a quick and clear understanding of what a data asset contains. You can think of the asset profile as the TL;DR about your data. The glossary provides key intel on your data assets so you can quickly understand important attributes of your data, such as: Owners of your data, so you know who to ask for clarification. Certificate status, to easily understand if metadata enrichment is still in progress or the asset is ready to be used. Linked assets that are relevant to the term, so you can explore other helpful material. Here are a few of the things that make Atlan's discovery awesome: Intelligent keyword recognition sees through your typos to show exactly what you wanted, no matter what you actually typed. Sort by popularity to quickly discover what assets your teammates are using every day."}
{"url":"https:\/\/docs.atlan.com\/get-started\/how-tos\/quick-start-for-admins#user-management","title":"Administrators | Atlan Documentation","text":"User management User management is a critical part of data governance. Atlan's user management capabilities should be a mainstay of how you organize and control access for people in your organization. Add and manage users from the admin center It's super simple to invite and remove users from Atlan from the Admin center . You can also manage existing users by adding them to groups, changing their roles, or set up SSO , SCIM , and SMTP configurations. Manage access control from the governance center The Governance center is where you can build access control mechanisms to manage user access . Personas allow you to group users into teams, such as Financial Analysts or Cloud Engineers , and set policies based on the access those personas should have. Purposes are where you can build policies based on the actions or access that a user might need. For example, you can use Atlan's policy-based access controls to manage access to PII and other sensitive data. This is a best practice for data governance. Once you set these policies, Atlan will enforce them throughout your users' experience. This means that users who don't have access to a particular type of data will not be able to see it. Governance workflows help you set up robust controls on data access management, metadata enrichment, new entity creation, and more, with out-of-the-box workflow templates and automated execution. Asset profile The asset profile in Atlan gives you a quick and clear understanding of what a data asset contains. You can think of the asset profile as the TL;DR about your data. The glossary provides key intel on your data assets so you can quickly understand important attributes of your data, such as: Owners of your data, so you know who to ask for clarification. Certificate status, to easily understand if metadata enrichment is still in progress or the asset is ready to be used. Linked assets that are relevant to the term, so you can explore other helpful material. Here are a few of the things that make Atlan's discovery awesome: Intelligent keyword recognition sees through your typos to show exactly what you wanted, no matter what you actually typed. Sort by popularity to quickly discover what assets your teammates are using every day."}
{"url":"https:\/\/docs.atlan.com\/get-started\/how-tos\/quick-start-for-admins#add-and-manage-users-from-the-admin-center","title":"Administrators | Atlan Documentation","text":"User management User management is a critical part of data governance. Atlan's user management capabilities should be a mainstay of how you organize and control access for people in your organization. Add and manage users from the admin center It's super simple to invite and remove users from Atlan from the Admin center . You can also manage existing users by adding them to groups, changing their roles, or set up SSO , SCIM , and SMTP configurations. Manage access control from the governance center The Governance center is where you can build access control mechanisms to manage user access . Personas allow you to group users into teams, such as Financial Analysts or Cloud Engineers , and set policies based on the access those personas should have. Purposes are where you can build policies based on the actions or access that a user might need. For example, you can use Atlan's policy-based access controls to manage access to PII and other sensitive data. This is a best practice for data governance. Once you set these policies, Atlan will enforce them throughout your users' experience. This means that users who don't have access to a particular type of data will not be able to see it. Governance workflows help you set up robust controls on data access management, metadata enrichment, new entity creation, and more, with out-of-the-box workflow templates and automated execution. Asset profile The asset profile in Atlan gives you a quick and clear understanding of what a data asset contains. You can think of the asset profile as the TL;DR about your data. The glossary provides key intel on your data assets so you can quickly understand important attributes of your data, such as: Owners of your data, so you know who to ask for clarification. Certificate status, to easily understand if metadata enrichment is still in progress or the asset is ready to be used. Linked assets that are relevant to the term, so you can explore other helpful material. Here are a few of the things that make Atlan's discovery awesome: Intelligent keyword recognition sees through your typos to show exactly what you wanted, no matter what you actually typed. Sort by popularity to quickly discover what assets your teammates are using every day."}
{"url":"https:\/\/docs.atlan.com\/product\/capabilities\/governance\/users-and-groups\/how-tos\/invite-new-users","title":"Invite new users | Atlan Documentation","text":"You will need to be an admin user in Atlan to invite users. Usernames in Atlan are of a permanent nature. Atlan uses usernames as a unique identifier across the platform and does not support making any changes to them. When logging into Atlan for the first time, ensure that you configure your username as per your preference. Without SSO To invite new users to Atlan, without SSO: From the left menu of any screen, click Admin . From the left menu of any screen, click Admin . Under Workspace click Users . Under Workspace click Users . Click the Invite Users button. Click the Invite Users button. Under Invite users to Default enter one or more email addresses of the users to invite. danger Atlan does not allow the use of disposable email addresses. You will receive an error indicating this if you attempt to send an invitation to one. Under Invite users to Default enter one or more email addresses of the users to invite. Atlan does not allow the use of disposable email addresses. You will receive an error indicating this if you attempt to send an invitation to one. (Optional) To the right of each email, click Member to change the role of the user . (Optional) To the right of each email, click Member to change the role of the user . Click the Send Invite button. Click the Send Invite button. Your user(s) will now receive an email with a link to sign up on Atlan! Ã° Note that the invitation link will remain valid for 7 days. If the link expires, you can resend the invitation to your new users. With SSO When SSO is enforced , you will not be able to invite users in Atlan - they can only be invited through the SSO provider. Any users mapped to the SAML app can log into Atlan via SSO. A user profile will be generated for them automatically, if one does not already exist. Admins can also assign default user roles for SSO to give appropriate permissions to users as soon as they log into Atlan."}
{"url":"https:\/\/docs.atlan.com\/product\/capabilities\/governance\/users-and-groups\/how-tos\/manage-users","title":"Manage users | Atlan Documentation","text":"You will need to be an admin user in Atlan to manage other users. As an admin user in Atlan, you can: Change a user's role Temporarily disable or permanently remove a user Reactivate a disabled user Change a user's role To change a user's role: From the left menu of any screen, click Admin . Under Workspace , click Users . Under the Role column, on a given user's row, click the role name. Under Change Role , select the new role to give the user, and then click Change . That's it, the user's role has now been changed! Ã° Deactivate a user To deactivate a user, you can either disable or remove them from Atlan. For example, you can disable a user to temporarily suspend their access to Atlan and reactivate it at a future date. If a user leaves the organization, you can remove their access. Atlan recommends that you proceed with caution if you want remove a user. While a disabled user can be reactivated , removing a user is a permanent and irreversible action. If a removed user needs to be restored to Atlan later on, you will need to add them as a new user. You can remove users irrespective of whether you're using basic authentication, SSO, or SCIM provisioning. Note that if you're using SCIM provisioning, disabling or reactivating users in Atlan is not allowed . If a SCIM-provisioned user is unassigned from the Atlan app in the identity provider, that user will be disabled in Atlan as well. Only then will you have the option to remove that user permanently. To deactivate a user: From the left menu of any screen, click Admin . Under Workspace , click Users . Scroll the table all the way to the right, if necessary. On an active user's row, click the 3-dot menu button and then from the dropdown: To temporarily remove a user, click Disable user and then click Disable . To permanently remove a user, click Remove user . In the Remove user dialog, for Transfer ownership , select an existing user to transfer any and all assets and workflows owned by the user you want to remove. Depending on the volume of assets to be transferred, the user removal process may take 15 minutes to a few hours to complete. You will be notified via email upon completion. Check the I understand this action can't be reversed and want to permanently remove this user box. Click Remove user . To temporarily remove a user, click Disable user and then click Disable . To permanently remove a user, click Remove user . In the Remove user dialog, for Transfer ownership , select an existing user to transfer any and all assets and workflows owned by the user you want to remove. Depending on the volume of assets to be transferred, the user removal process may take 15 minutes to a few hours to complete. You will be notified via email upon completion. Check the I understand this action can't be reversed and want to permanently remove this user box. Click Remove user . In the Remove user dialog, for Transfer ownership , select an existing user to transfer any and all assets and workflows owned by the user you want to remove. Depending on the volume of assets to be transferred, the user removal process may take 15 minutes to a few hours to complete. You will be notified via email upon completion. Check the I understand this action can't be reversed and want to permanently remove this user box. Click Remove user . That's it, the user is now deactivated and will no longer be able to access Atlan. Ã° Â¢ Deactivated user accounts do not count towards the total number of Atlan licenses procured by your organization. If the user you want to disable is a connection admin , you will need to ensure that other users can manage the connection before disabling the account. You can modify a connection to add more connection admins. Reactivate a user You can only reactivate a disabled user's account. To reactivate a user: From the left menu of any screen, click Admin . Under Workspace , click Users . Scroll the table all the way to the right, if necessary. On a deactivated user's row, click the 3-dot menu button. Click Enable user , and then Enable . That's it, the user can once again access Atlan! Ã° Atlan has a retention policy of 60 days for user login events. If a user has not logged into Atlan for more than 60 days, the Last Active column will display - for the user. Change a user's role Deactivate a user Reactivate a user"}
{"url":"https:\/\/docs.atlan.com\/product\/integrations\/identity-management\/sso","title":"SSO Integration | Atlan Documentation","text":"Get started How to enable Azure AD for SSO How to enable Google for SSO How to enable JumpCloud for SSO How to enable Okta for SSO How to enable OneLogin for SSO How to enable SAML 2.0 for SSO Authenticate SSO credentials to query data Authenticate SSO credentials to view sample data How to limit SSO automatically creating users when they log in How to set default user roles for SSO SSO integration with PingFederate using SAML Troubleshooting connector-specific SSO authentication Why do I get a 404 error when using PingFederate SSO? Why do I get an authentication error when logging in via Okta for the first time? Why do I get an error while logging in via Google dashboard? Unable to log into Atlan via SSO due to an internal error from Microsoft Defender Can Atlan integrate with multiple Azure AD tenants within a single instance? Can we use a Microsoft SSO login? What type of user provisioning does Atlan support for SSO integrations? When does Atlan become a personal data processor or subprocessor? Why did my users not receive an invite email from Atlan?"}
{"url":"https:\/\/docs.atlan.com\/product\/integrations\/identity-management\/scim\/how-tos\/configure-scim-provisioning","title":"Configure SCIM provisioning | Atlan Documentation","text":"You can automate the process of provisioning and deprovisioning your users and groups in Atlan with System for Cross-domain Identity Management (SCIM). Atlan supports SCIM 2.0 for SCIM provisioning. SCIM provisioning works in combination with your single sign-on (SSO) setup. Setting up SCIM enables you to manage all your users from one central location. Atlan currently supports SCIM provisioning for the following SSO providers: For more questions about SCIM provisioning, head over to Troubleshooting SCIM provisioning ."}
{"url":"https:\/\/docs.atlan.com\/product\/integrations\/communication\/smtp-and-announcements\/how-tos\/configure-smtp","title":"Configure SMTP | Atlan Documentation","text":"You will need to be an admin user in Atlan to configure SMTP. The default SMTP setup is preconfigured by Atlan. You should only update this configuration if you want to set up custom SMTP. Atlan uses SMTP to send emails, primarily for things like inviting users, login failure alerts, and scheduled queries . We provide an embedded SMTP server to do this, out-of-the-box. If desired, you can override this embedded SMTP se rver with your own. To override the embedded SMTP server: From the left menu of any screen, click Admin . Under Workspace , click SMTP . Fill in the configuration of your SMTP server, at least: For Host the fully-qualified hostname of the SMTP server. For From Email the email address that should be used to send emails from the server. For Username the username required by your SMTP server. For Host the fully-qualified hostname of the SMTP server. For From Email the email address that should be used to send emails from the server. For Username the username required by your SMTP server. At the bottom of the page, click the Test SMTP Config button to test your configuration. This will attempt to send an email to your profile's email address. Once you successfully receive the test email, at the bottom of the page click Save ."}
{"url":"https:\/\/docs.atlan.com\/get-started\/how-tos\/quick-start-for-admins#manage-access-control-from-the-governance-center","title":"Administrators | Atlan Documentation","text":"User management User management is a critical part of data governance. Atlan's user management capabilities should be a mainstay of how you organize and control access for people in your organization. Add and manage users from the admin center It's super simple to invite and remove users from Atlan from the Admin center . You can also manage existing users by adding them to groups, changing their roles, or set up SSO , SCIM , and SMTP configurations. Manage access control from the governance center The Governance center is where you can build access control mechanisms to manage user access . Personas allow you to group users into teams, such as Financial Analysts or Cloud Engineers , and set policies based on the access those personas should have. Purposes are where you can build policies based on the actions or access that a user might need. For example, you can use Atlan's policy-based access controls to manage access to PII and other sensitive data. This is a best practice for data governance. Once you set these policies, Atlan will enforce them throughout your users' experience. This means that users who don't have access to a particular type of data will not be able to see it. Governance workflows help you set up robust controls on data access management, metadata enrichment, new entity creation, and more, with out-of-the-box workflow templates and automated execution. Asset profile The asset profile in Atlan gives you a quick and clear understanding of what a data asset contains. You can think of the asset profile as the TL;DR about your data. The glossary provides key intel on your data assets so you can quickly understand important attributes of your data, such as: Owners of your data, so you know who to ask for clarification. Certificate status, to easily understand if metadata enrichment is still in progress or the asset is ready to be used. Linked assets that are relevant to the term, so you can explore other helpful material. Here are a few of the things that make Atlan's discovery awesome: Intelligent keyword recognition sees through your typos to show exactly what you wanted, no matter what you actually typed. Sort by popularity to quickly discover what assets your teammates are using every day."}
{"url":"https:\/\/docs.atlan.com\/product\/capabilities\/governance\/custom-metadata\/how-tos\/control-access-metadata-data","title":"Control access to metadata and data? | Atlan Documentation","text":"You can customize access for users through several mechanisms. User roles The most general mechanism is a user role . These define the very broad permissions a user has in Atlan - for example, whether they can administer other users, or only discover metadata. When it comes to what metadata and data a user can access, though, we need to use the additional mechanisms below. Connection admins Connection admins are users who manage connectivity to a data source. By default, these users can: Read and write all metadata on assets from that connection. Preview and query the data in all data assets from that connection. Manage access policies to grant others access to the assets from that connection. You define the connection admin when crawling a new data source for the first time. A connection admin can also extend the list of connection admins on their connection at any time. Access policies A user must be both an admin user and a connection admin to define access policies for the connection's assets. Access policies either allow or restrict access to certain assets. These allow you to be much more creative (and granular) about access than the all-or-nothing privileges of connection admins. You start by defining which assets to control with each policy. There are two complementary mechanisms to do this in Atlan - personas and purposes . Once you have defined the subset of assets, you can then define granular access to both metadata and data: Metadata policies Metadata policies control what users can do with the assets' metadata. Through them, you can control who can: Read : view an asset's activity log, custom metadata, and SQL queries Update : change asset metadata, including description, certification, owners, README, and resources Update Custom Metadata Values for the assets Add Tags to the assets Remove Tags from the assets Create : create new assets within the selected connection (via API) Delete : delete assets within the selected connection (via API) Data policies Data policies control what users can do with the assets' data. Through them, you can control who can: Query and preview the data within the assets Whether to hide any data, through various masking techniques: Show first 4 : replaces all the data with X except the first 4 characters of data. For example 1234 5678 9012 3456 would become 1234XXXX . Show last 4 : replaces all the data with X except the last 4 characters of data. For example 1234 5678 9012 3456 would become XXXX3456 . Hash : replaces the data with a consistent hashed value. Because the hash is consistent you can still join on it across assets. For example 1234 5678 9012 3456 would become f43jknscakc12nk21ak . Nullify : replaces the data with the null value. For example 1234 5678 9012 3456 would become null . Redact : replaces all alphabetic data with x and all numeric data with 0. For example 1234 Street Name would become 0000 Xxxxxx Xxxx . Show first 4 : replaces all the data with X except the first 4 characters of data. For example 1234 5678 9012 3456 would become 1234XXXX . Show last 4 : replaces all the data with X except the last 4 characters of data. For example 1234 5678 9012 3456 would become XXXX3456 . Hash : replaces the data with a consistent hashed value. Because the hash is consistent you can still join on it across assets. For example 1234 5678 9012 3456 would become f43jknscakc12nk21ak . Nullify : replaces the data with the null value. For example 1234 5678 9012 3456 would become null . Redact : replaces all alphabetic data with x and all numeric data with 0. For example 1234 Street Name would become 0000 Xxxxxx Xxxx . Glossary policies Glossary policies can only be defined through personas. All the mechanisms above can coexist. This is powerful, but can also be a bit overwhelming to think about. What takes priority when a user is under the control of all these mechanisms? Ã° Âµ Ã° Â« It's actually not as bad as you might think - only these three rules: Access is denied by default (implicitly) By default, users will not have the permissions listed above. This remains true until you explicitly grant a user a permission. For example, imagine you have not set up any access policies and a new user joins. They will not have any of the permissions above against any assets in Atlan. Explicit grants (allows) are combined When you grant a user a permission, this is combined with all other permissions you have granted the user. Continuing our example, imagine you add the new user to a group defined as the connection admins for Snowflake. The user will now have full read\/write access to all metadata for Snowflake assets, and be able to query and preview the data in those assets. Then you add the user to a persona that gives read\/write access to a Looker project. The user will now have access to all Snowflake assets and a Looker project's assets. Explicit restrictions (denies) take priority When you explicitly deny a user a permission, this takes priority over all other permissions you have granted the user. Continuing our example, imagine you define a purpose with a data policy that masks PII data. The user will still have full read\/write access to all metadata for Snowflake assets and a Looker project's assets. In general, they will still be able to query and preview the data in the Snowflake assets. However, any PII data in Snowflake will now be masked. Then you add a metadata policy to the purpose that denies permission to remove the PII tag. The user will no longer have full read\/write access to all metadata for Snowflake assets and a Looker project's assets. The user can no longer remove the PII tag from any of these assets. The combination of mechanisms in the example above shows their power. Through a small number of controls we can define wide-ranging but granular access permissions."}
{"url":"https:\/\/docs.atlan.com\/product\/capabilities\/governance\/access-control\/concepts\/what-are-personas","title":"What are personas? | Atlan Documentation","text":"Personas define policies to control which users can (or cannot) take certain actions on specific assets. They address a combination of two main objectives: Curating the assets that are relevant to a team of users Controlling the actions users can take on those assets (querying data, updating metadata, etc) Think of personas as a way of curating assets for a group of users. Team-based personalization Personas curate the assets that are discoverable by users, and the detailed metadata shown. In this way, users can focus on only those assets (and metadata) relevant to their role in the organization. This reduces distractions and \"noise\" for users. For example, a team of marketers may only work with a few data sets and dashboards. Rather than flooding the marketers with details about every asset in your company, you want to focus their attention. Broad-brush access control Combined with the personalization are broad-brush access control policies. When defining the policies in a persona, you not only select the assets but also what actions users can take on those assets. For example, your team of marketers should be able to describe their dashboards. But perhaps they should only be able to see the shared data sets that feed those dashboards, not change their descriptions or tags. You can define the persona so that they can read and write the dashboards, but only read the data sets. Broad-brush access control"}
{"url":"https:\/\/docs.atlan.com\/product\/capabilities\/governance\/access-control\/concepts\/what-are-purposes","title":"What are purposes? | Atlan Documentation","text":"Purposes provide ways to interact with tagged assets. They address two main objectives: Grouping assets together in ways they may be used by many teams - for example, by project or domain Controlling access to very granular, typically sensitive data When defining a purpose , you choose its tags. Atlan then considers all assets with at least one of those tags as part of the purpose. Think of purposes as a way of further protecting particularly sensitive data. Even if a user can see data in a table, you may not want them to see one or two sensitive columns within that table. Asset curation by domain One way you can use purposes is to curate assets. In this approach, the purpose's tag tends to be a domain. For example, this could be a project or an area of your organization's business. Through the purpose, you can grant permissions to assets with that tag. With purposes, any future assets given a tag will gain the same permissions - no policy changes needed. Granular data protection The other way you can use purposes is to enforce granular data protection. In this approach, the purpose's tag tends to be some level of information sensitivity. For example, this could be personally-identifiable information (PII) or confidential internal financial metrics. These sensitivity tags will tend to be against granular data assets - often columns. Personas tend to control permissions at a broader level, for example entire data sources, databases or schemas. Through these more granular tags, purposes give you more fine-grained control. And you can layer this on top of the permissions granted by personas . For example, you might grant permission to preview and query a database to a group of users through a persona . But you don't want those users to be able to see any PII data - specific columns - wherever they appear in the database. There could be hundreds of these columns, scattered across thousands of tables. By tagging the columns, you can restrict access to them through a single policy in a purpose. This way you don't need to maintain many separate per-column policies through a persona . Asset curation by domain Granular data protection"}
{"url":"https:\/\/docs.atlan.com\/product\/capabilities\/governance\/stewardship\/how-tos\/automate-data-governance","title":"Automate data governance | Atlan Documentation","text":"You must be an admin user in Atlan to enable , create , and manage governance workflows. Anyone with access to Atlan - admin, member, or guest user - can use the inbox . You can streamline your data governance requirements in Atlan with governance workflows and manage alerts, approvals, and tasks using the inbox . Governance workflows enable you to set up robust controls on data access management, metadata enrichment, new entity creation, and more, with out-of-the-box workflow templates and automated execution. For example, instead of allowing your users to directly query data or update the certification status of an asset, you can specify assets that require advanced controls and create governance workflows to govern them. These workflows will run in the background, ensure that all required approvals are in place, and only then approve users with appropriate permissions to perform any action. You can use governance workflows to ensure: Risk mitigation - determine how data is used and shared in your organization with automated access policies. Data security - manage requests for data access and processing to only allow access to authorized individuals or teams. Metadata change management - monitor and audit metadata changes to align with established organizational standards. New entity creation - manage and audit documentation of business context such as glossaries and tags to align with established organizational standards. Policy compliance - set up repeatable processes and approval flows for your data assets in Atlan to adhere to regulatory requirements - currently only applicable if you have also enabled the policy center module . Workflow properties A common set of properties are applicable to all governance workflows in Atlan: Only an admin user can create, update, or delete governance workflows. Out-of-the-box workflow templates. Predefined steps based on workflow selection. Must be associated with an asset type or action. Set up auto-approval rules for users, groups, or owners based on metadata attributes and policies. Activity logs for all workflows available by default. Visibility into the transition states of a workflow. Overlapping workflows - governance workflows provide you with the flexibility of creating workflows per team or business domain on the same set of assets instead of creating one complex workflow to cover all your use cases. Atlan will handle all the complexities, only allowing approvals to go through once all approval conditions have been met. Workflow templates You can choose from the following workflow templates to govern your assets and manage access: Change management This template allows you to control changes to metadata within your organization's data management and governance framework. Use cases include requests to: Add, update, and remove descriptions manually and using Atlan AI Add, update, and remove certificates Add, update, and remove an alias Add, update, and remove owners Attach, update, and remove tags Add, update, and remove custom metadata Add, update, and remove domains Add, update, and remove READMEs Add, update, and remove announcements Change management workflows will override any permissions assigned through user roles or access policies . For example, even for users with edit access , metadata update requests will go through change management workflows. If there are no change management workflows in place, then users with edit access will be able to update metadata while users without edit access will only be able to suggest changes to metadata . New entity creation This template allows you to control the creation and publication of new entities in Atlan. The new entity creation workflow will override existing glossary policies and user role permissions to create new entities. Creation of the following entities is currently supported for the new entity creation workflow: Data products : Creation of a new data product Change of a data product's status from Sunset , Archived , or Draft to Published Creation of a new data product Change of a data product's status from Sunset , Archived , or Draft to Published Access management This template allows you to automate the process of requesting, approving, and revoking access to data assets in Atlan. It includes the combination of a self-service approach as well as mandating human intervention for approval. You can also revoke data access in Atlan or other data sources. For data sources other than Atlan, you can configure additional actions to revoke data access in the data source. Use cases include requests to query data or view sample data for the following supported asset types - tables, views, and materialized views. Grant access in Atlan - allow requesters to request data access for querying data in Insights and previewing sample data within Atlan only. Raise Jira ticket to grant or revoke data access on source - allow requesters to request or revoke data access for any tool. Atlan will create a support ticket in Jira Cloud for your team to grant or revoke data access and display the status of your request in Atlan. You will need to: Integrate Jira Cloud and Atlan . Link your individual Jira Cloud account to Atlan . Install or register a webhook . Create an access management workflow to enable or revoke access everywhere using Jira. Add a Jira project and issue type and specify an issue status while creating the data access workflow. Your users will be granted access or their access will be revoked once the request is approved in Jira. Integrate Jira Cloud and Atlan . Link your individual Jira Cloud account to Atlan . Install or register a webhook . Create an access management workflow to enable or revoke access everywhere using Jira. Add a Jira project and issue type and specify an issue status while creating the data access workflow. Your users will be granted access or their access will be revoked once the request is approved in Jira. Add a Jira project and issue type and specify an issue status while creating the data access workflow. Your users will be granted access or their access will be revoked once the request is approved in Jira. Raise ServiceNow request to grant or revoke data access on source - allow requesters to request or revoke data access for any tool. Atlan will create a request in the Atlan Data Access catalog for your team in ServiceNow to grant or revoke data access and display the status of your request in Atlan. You will need to: Integrate ServiceNow and Atlan . Link your individual ServiceNow account to Atlan . Create a data access approval workflow to enable or revoke access everywhere using ServiceNow. Specify the request state(s) for approval while creating the data access workflow. Your users will be granted access or their access will be revoked once the request is approved in ServiceNow. Integrate ServiceNow and Atlan . Link your individual ServiceNow account to Atlan . Create a data access approval workflow to enable or revoke access everywhere using ServiceNow. Specify the request state(s) for approval while creating the data access workflow. Your users will be granted access or their access will be revoked once the request is approved in ServiceNow. Specify the request state(s) for approval while creating the data access workflow. Your users will be granted access or their access will be revoked once the request is approved in ServiceNow. Trigger a webhook - allow requesters to request or revoke data access for any tool. Atlan will trigger a webhook to a URL of your choice for your team to grant or revoke data access. For URL , enter the URL for where you want to receive events, including details on requester, approver, and asset, and then validate the URL. danger Atlan will send a sample payload to test if the webhook URL is correct. You must respond with a 2xx status for the validation to succeed. Atlan will also run this validation before you save your webhook as a precautionary measure. Copy the Secret Key and store it in a secure location to verify data access approval or revocation requests from Atlan. For URL , enter the URL for where you want to receive events, including details on requester, approver, and asset, and then validate the URL. danger Atlan will send a sample payload to test if the webhook URL is correct. You must respond with a 2xx status for the validation to succeed. Atlan will also run this validation before you save your webhook as a precautionary measure. For URL , enter the URL for where you want to receive events, including details on requester, approver, and asset, and then validate the URL. Atlan will send a sample payload to test if the webhook URL is correct. You must respond with a 2xx status for the validation to succeed. Atlan will also run this validation before you save your webhook as a precautionary measure. Copy the Secret Key and store it in a secure location to verify data access approval or revocation requests from Atlan. Copy the Secret Key and store it in a secure location to verify data access approval or revocation requests from Atlan. Policy approval You must enable the policy center module to use the policy approval workflow template. This template allows you to automate approvals for your data governance policies in Atlan. Automated policy approval workflows can help you streamline the approval process, facilitate compliance with regulatory standards, and simplify data governance for your organization. Use cases include requests to: Create new policies Revise existing policies Enable governance workflows and inbox You must be an admin user in Atlan to enable the governance workflows and inbox module for your organization. To enable governance workflows and inbox for your Atlan users: From the left menu of any screen in Atlan, click Admin . Under the Workspace heading, click Labs . On the Labs page, under Governance center , turn on Governance Workflows and Inbox to govern your assets and manage alerts, approvals, and tasks in Atlan more effectively. If you'd like to disable the Governance Workflows and Inbox module from your organization's Atlan workspace, follow the steps above to turn it off. Once enabled, you can also temporarily disable the module and turn it on again as needed. For any governance workflows you may have created or existing requests , this will not result in any data loss. Interactions with existing access control mechanisms Once you have turned on governance workflows and inbox, the module will interact with existing access control mechanisms in Atlan as follows: Requests : Atlan will channel requests and approvals through governance workflows and land them in the inbox. New requests - once you have enabled governance workflows and inbox, the requests widget will be replaced by an inbox and your member and guest users will not be able to raise any new requests until an admin user has created at least one governance workflow. To enable your member and guest users to raise new requests in Atlan: Create a change management governance workflow . Select all connections present in your Atlan workspace . Skip auto-approval . Select Anyone approves and list the users or groups designated as your Atlan admins . Publish your first governance workflow! Once published, this comprehensive workflow will allow your member and guest users to raise requests. Now you can focus on creating more use-case-driven workflows and consequently removing governed assets from the first workflow until you no longer need it. Existing requests - only admin users can take action on existing requests from the requests center . Your member and guest users will only be able to raise new requests on governed assets. New requests - once you have enabled governance workflows and inbox, the requests widget will be replaced by an inbox and your member and guest users will not be able to raise any new requests until an admin user has created at least one governance workflow. To enable your member and guest users to raise new requests in Atlan: Create a change management governance workflow . Select all connections present in your Atlan workspace . Skip auto-approval . Select Anyone approves and list the users or groups designated as your Atlan admins . Publish your first governance workflow! Once published, this comprehensive workflow will allow your member and guest users to raise requests. Now you can focus on creating more use-case-driven workflows and consequently removing governed assets from the first workflow until you no longer need it. Create a change management governance workflow . Select all connections present in your Atlan workspace . Skip auto-approval . Select Anyone approves and list the users or groups designated as your Atlan admins . Publish your first governance workflow! Once published, this comprehensive workflow will allow your member and guest users to raise requests. Now you can focus on creating more use-case-driven workflows and consequently removing governed assets from the first workflow until you no longer need it. Existing requests - only admin users can take action on existing requests from the requests center . Your member and guest users will only be able to raise new requests on governed assets. Metadata policies - your users must have read access to an asset for triggering governance workflows. If an asset is governed by a governance workflow, your users will be able to raise a request on that asset regardless of all allow\/deny permissions in metadata policies. Data policies : No data policy exists - if the workflow connection allows querying and previewing sample data but a data policy has not been configured, your users will be able to raise a data access request on governed assets in the connection. Data policy with explicit restrictions - if an existing data policy denies querying and previewing sample data and assets are governed by a governance workflow, your users will not be able to raise a data access request on governed assets in the connection. Data policy with explicit grants - if an existing data policy allows querying and previewing sample data and assets are governed by a governance workflow, your users will be able to raise a data access request on governed assets in the connection. No data policy exists - if the workflow connection allows querying and previewing sample data but a data policy has not been configured, your users will be able to raise a data access request on governed assets in the connection. Data policy with explicit restrictions - if an existing data policy denies querying and previewing sample data and assets are governed by a governance workflow, your users will not be able to raise a data access request on governed assets in the connection. Data policy with explicit grants - if an existing data policy allows querying and previewing sample data and assets are governed by a governance workflow, your users will be able to raise a data access request on governed assets in the connection. Domain policies - governance workflows are currently not applicable to domain policies. User roles - if an asset is governed by a governance workflow, your users will be able to raise a request on that asset regardless of their role or permissions. For any asset not governed by a governance workflow, default role permissions will apply. Connection admins - if an asset is governed by a governance workflow, connection admins will have to go through the approval process for governed assets in the connection. Add, update, and remove resources Add a README to a term using Atlan AI Bulk updates through spreadsheet tools Bulk updates using playbooks Bulk updates using Atlan AI Bulk updates through API, SDK, and CLI operations Metadata updates in supported tools using Atlan browser extension Enable governance workflows and inbox"}
{"url":"https:\/\/docs.atlan.com\/get-started\/how-tos\/quick-start-for-admins#asset-profile","title":"Administrators | Atlan Documentation","text":"User management User management is a critical part of data governance. Atlan's user management capabilities should be a mainstay of how you organize and control access for people in your organization. Add and manage users from the admin center It's super simple to invite and remove users from Atlan from the Admin center . You can also manage existing users by adding them to groups, changing their roles, or set up SSO , SCIM , and SMTP configurations. Manage access control from the governance center The Governance center is where you can build access control mechanisms to manage user access . Personas allow you to group users into teams, such as Financial Analysts or Cloud Engineers , and set policies based on the access those personas should have. Purposes are where you can build policies based on the actions or access that a user might need. For example, you can use Atlan's policy-based access controls to manage access to PII and other sensitive data. This is a best practice for data governance. Once you set these policies, Atlan will enforce them throughout your users' experience. This means that users who don't have access to a particular type of data will not be able to see it. Governance workflows help you set up robust controls on data access management, metadata enrichment, new entity creation, and more, with out-of-the-box workflow templates and automated execution. Asset profile The asset profile in Atlan gives you a quick and clear understanding of what a data asset contains. You can think of the asset profile as the TL;DR about your data. The glossary provides key intel on your data assets so you can quickly understand important attributes of your data, such as: Owners of your data, so you know who to ask for clarification. Certificate status, to easily understand if metadata enrichment is still in progress or the asset is ready to be used. Linked assets that are relevant to the term, so you can explore other helpful material. Here are a few of the things that make Atlan's discovery awesome: Intelligent keyword recognition sees through your typos to show exactly what you wanted, no matter what you actually typed. Sort by popularity to quickly discover what assets your teammates are using every day."}
{"url":"https:\/\/docs.atlan.com\/product\/capabilities\/discovery\/concepts\/what-are-asset-profiles","title":"What are asset profiles? | Atlan Documentation","text":"Every asset in Atlan has its own asset profile, which consists of all the information available for that particular asset. After you've discovered an asset, click to open the asset profile. This view gives you all the context you need about the asset. Components of an asset profile This section displays important details about the asset: Technical name and alias , if added Number of rows and columns Description of the asset Certification status (verified, draft, or deprecated) Owner of the asset For table profiles, you can also view the Columns tab. This tab allows you to update the metadata for your columns directly from the asset profile. Lineage offers a visual representation of the sources and transformations of your asset. You can also download lineage in a CSV file or as an image from here. Related assets Related assets displays all the assets that have associations with each other beyond a parent-child relationship. For example, a Microsoft Power BI workspace will generally have reports, datasets, dashboards, and dataflows, all of which are related to each other. Related assets are additionally listed in the Relations tab of the asset sidebar. Column preview Column preview offers a snapshot of all the columns in a data table, including the column name, data type, and description. This will change to Field preview in the asset profile and Fields tab in the asset sidebar for certain asset types - including but not limited to data source fields and calculated fields for Tableau data sources, columns for Microsoft Power BI tables, fields for Looker explores and views, and more. You can also export child assets such as columns or fields from the parent asset profile. Sample data Sample data shows the sample data for an asset. This helps users understand what kind of data is included in the asset and allows them to copy or export this data. Connection admins can also enforce users to validate their credentials before viewing sample data, helping you enforce better governance across your organization. Linked queries Linked queries displays any saved queries auto-linked to the asset when queried or referenced in the SQL query. This helps users quickly find the saved queries for additional context or launch the query in Insights directly from the asset profile. From the Linked queries tab: View all saved queries linked to the asset, along with a total count of such queries. Hover over a linked query to: View the total number of query runs in the last 30 days in a popover. Click the play icon to run the query in Insights. Click the open asset sidebar icon to view details about the query in the sidebar. View the total number of query runs in the last 30 days in a popover. Click the play icon to run the query in Insights. Click the open asset sidebar icon to view details about the query in the sidebar. Click the 3-dot icon to view any additional linked queries. If you do not have access to query data in Atlan, the linked queries will be displayed with a lock icon . Readme provides contextual information about the asset. It's a great place to crowdsource all the tribal knowledge and context that different users might have about the data asset. Asset profile header This section helps you perform quick actions. From the top right of the asset profile: Click the user avatars to view a list of recently visited users, total views on your asset, total number of unique visitors, and total views by user. Use the days filter to filter asset views and user activity in the last 7, 30, and 90 days. This feature is turned on by default - admins can turn off user activity . Use the days filter to filter asset views and user activity in the last 7, 30, and 90 days. This feature is turned on by default - admins can turn off user activity . Click the star button to star your asset and bookmark it for easy access. Expand the Query dropdown to view sample data or query the asset in Insights. Click the clipboard icon to copy the link for your asset. Click the Slack or Teams icon to post on a Slack or Microsoft Teams channel. Click the 3-dot icon to add an announcement or a resource to your asset. Asset sidebar The sidebar to the right of the asset profile provides high-level information about the asset. Here's what you can view: Relations shows a list of all the related assets. Usage displays usage metadata for your Snowflake and Google BigQuery assets. Lineage shows the upstream sources and downstream transformations for the asset. Fact-Dim Relations displays foreign-key relationships between fact and dimension tables. Fact-dimension relationships between assets can currently only be defined and published via API. Activity serves as a changelog for the asset . Resources are links to internal or external URLs that help your team understand the asset. Queries shows all the saved queries for the asset. Requests for an asset can be filtered by their status, such as Pending , Approved , and Rejected . Properties shows the unique identification number of the asset and other essential properties. Integrations show Slack messages and Jira tickets pertaining to the asset. Custom metadata tabs display custom metadata properties of the asset, if enabled. Components of an asset profile"}
{"url":"https:\/\/docs.atlan.com\/get-started\/how-tos\/quick-start-for-admins#glossary","title":"Administrators | Atlan Documentation","text":"User management User management is a critical part of data governance. Atlan's user management capabilities should be a mainstay of how you organize and control access for people in your organization. Add and manage users from the admin center It's super simple to invite and remove users from Atlan from the Admin center . You can also manage existing users by adding them to groups, changing their roles, or set up SSO , SCIM , and SMTP configurations. Manage access control from the governance center The Governance center is where you can build access control mechanisms to manage user access . Personas allow you to group users into teams, such as Financial Analysts or Cloud Engineers , and set policies based on the access those personas should have. Purposes are where you can build policies based on the actions or access that a user might need. For example, you can use Atlan's policy-based access controls to manage access to PII and other sensitive data. This is a best practice for data governance. Once you set these policies, Atlan will enforce them throughout your users' experience. This means that users who don't have access to a particular type of data will not be able to see it. Governance workflows help you set up robust controls on data access management, metadata enrichment, new entity creation, and more, with out-of-the-box workflow templates and automated execution. Asset profile The asset profile in Atlan gives you a quick and clear understanding of what a data asset contains. You can think of the asset profile as the TL;DR about your data. The glossary provides key intel on your data assets so you can quickly understand important attributes of your data, such as: Owners of your data, so you know who to ask for clarification. Certificate status, to easily understand if metadata enrichment is still in progress or the asset is ready to be used. Linked assets that are relevant to the term, so you can explore other helpful material. Here are a few of the things that make Atlan's discovery awesome: Intelligent keyword recognition sees through your typos to show exactly what you wanted, no matter what you actually typed. Sort by popularity to quickly discover what assets your teammates are using every day."}
{"url":"https:\/\/docs.atlan.com\/product\/capabilities\/governance\/glossary\/concepts\/what-is-a-glossary","title":"A Glossary | Atlan Documentation","text":"Using familiar terminology helps people quickly understand the data and its context. This is a crucial element of data governance since it adds business context to the data initiatives of an organization. Why do I need a glossary? In today's diverse data teams, which include people from different backgrounds and use cases, not all of them think about their data in the same way. For teams made up of data analysts, data engineers, data scientists, and decision makers, having a shared language is an important step towards ensuring better collaboration. Building a glossary allows your team to define the metrics, columns, and assets with the same meaning for everyone. Highlights of the Atlan glossary Here's how the Atlan glossary can help your organization: Supports automated metadata management through auto-glossary suggestions from the Atlan bot Anatomy of the Atlan glossary This structure allows for glossaries from multiple domains. A term is the lowest unit that is unique to each glossary. It describes the content of the data assets in a useful and precise way. It can exist independently, without belonging to any particular category or subcategory. Subcategories can be added within categories to provide more context in a glossary. Related term Similar in definition - serves the purpose of a \"see also\" section in a dictionary. Client is a related term for Customer . Recommended term Preferred form of usage for the current term applied. User may be preferred over Customer in the context of your organization. Interchangeable in meaning as another term. Glossary and Dictionary , or Client and Customer . Opposite in meaning to a particular term. Minimum is an antonym for the term Maximum , or Loss and Profit are antonyms. Translated term Translated version of the same term in additional languages. Cliente is the Spanish term for Customer . Valid values for Defines values that are considered appropriate for a related term. Red , Green , Blue , and Yellow are valid values for the term Color . Classifies and Classified by Country classifies United States , while United States is classified by Country . Why do I need a glossary? Highlights of the Atlan glossary Anatomy of the Atlan glossary"}
{"url":"https:\/\/docs.atlan.com\/product\/capabilities\/discovery\/how-tos\/add-owners","title":"Add owners | Atlan Documentation","text":"Atlan allows you to add owners for each data asset. This enriches the asset profile and helps build trust among users. Users can then quickly reach out to the owner of the asset for any questions about the data. The owner is responsible for maintaining the data asset. They are the right person to contact for questions about the data's frequency of update, progress, status, and more. Add owners to your assets To add or update owners for a data asset, follow these steps: On the Atlan homepage, click Assets in the left menu. Click on an asset to view its asset profile. To assign owners, you can either: Click the checkbox next to an individual user's name. Toggle to groups in the top right and select an entire group of users. Click the checkbox next to an individual user's name. Toggle to groups in the top right and select an entire group of users. Click Save . Your asset profile will now display the asset owners! Ã° You can also filter your assets by asset owners. Here are the steps: In the Filters menu on the Assets page, click Owners . Click the checkbox next to an owner name to view their owned assets. You can select No Owners in the Owners filter to view assets that currently do not have assigned owners and assign accordingly if needed. Add owners to your assets"}
{"url":"https:\/\/docs.atlan.com\/product\/capabilities\/discovery\/how-tos\/add-certificates","title":"Add certificates | Atlan Documentation","text":"How many times has someone complained to you that the data is incomplete or has issues? And how many times have you responded that it's still a work in progress or they're using the wrong data! Wouldn't it be really convenient if the data could answer these questions? Certificates in Atlan can help! The certification tags help users quickly identify whether a data asset is ready to use, a work in progress, or has some issues. You can add the following four certification tags to any data asset: Verified for ready to be used. Draft for work in progress. Deprecated if the asset no longer exists. No certificate if not required or needs documentation down the line. Certificates can be used to quickly filter data assets on the Assets page. This helps build trust in your data assets among users. Add certificates to your assets To add or update the certificate for your data assets, follow these steps: On the Atlan homepage, click Assets in the left menu. Click on the asset to open its asset profile. In the right menu, click + under Certificate and choose the relevant certification option. Write a message to add more context. Now your data can proudly display its status for all to see! Ã° Once you have selected a certification tag for your data asset, you will get a popup that your certificate has been saved. Add certificates to your assets"}
{"url":"https:\/\/docs.atlan.com\/product\/capabilities\/governance\/glossary\/how-tos\/link-terms-to-assets","title":"Link terms to assets | Atlan Documentation","text":"Provide additional context for your assets to other users in your organization. Create common definitions once and apply them many times to multiple assets. Offer an abstract point for applying tags to be propagated to all linked assets - including their downstream and child assets - if propagation is enabled . If your data assets include personal information - for example, email addresses - you can link your assets to an Email Address term to provide context to your users. You can define the term Email Address once in the glossary. You can link the term to all the columns where an individual's email address appears. You can also tag the term as PII - and all of the linked assets will be tagged as PII . To link a term to an asset: From a term From the left menu on any screen, click Glossary . Under Glossary in the left menu, click the name of your glossary. Under your glossary name, click the category in which your term is nested and then click the term you would like to link to your assets. Click + Link Assets to get started. In the sidebar on the right, select the asset(s) to which you would like to link the term. At the bottom of the sidebar, click Link asset(s) to confirm your selections. From an asset From the left menu on any screen, click Assets . On the Assets page, select the asset to which you would like to link a term. In the dialog, expand the glossary menu and then click the term you would like to link to your assets. Click Save to confirm your selections. You can now view linked assets for your glossary term! Ã° To unlink a term from an asset: From a term From the left menu on any screen, click Glossary . Under Glossary in the left menu, click the name of your glossary. Under your glossary name, click the category in which your term is nested and then click the term you would like to unlink from your assets. Under Linked assets , navigate to the asset(s) from which you would like to unlink the term. To the right of the asset name, click the three dots and then click Unlink asset . From an asset From the left menu on any screen, click Assets . On the Assets page, select the asset from which you would like to unlink a term. Your assets will now be unlinked from the glossary term."}
{"url":"https:\/\/docs.atlan.com\/product\/capabilities\/discovery\/how-tos\/search-and-discover-assets","title":"Search and discover assets | Atlan Documentation","text":"Atlan is a living catalog of all your data assets and knowledge. It lets you quickly discover and access your data, along with the tribal knowledge and business context. Certify your assets Enrich your assets with descriptions Star your assets for easy access Intelligent keyword recognition Click Assets in the left-side panel. Domains : Filter assets by domains , such as a single domain, multiple domains, or no domain. Owners : Filter by selecting one or more users. You can also toggle between users and groups to filter based on a group of users. Tags : Filter by user-generated tags, such as public , PII , and more. Name : Sort by the asset name in an alphabetical or a reverse alphabetical order. Updated on Atlan : Sort by the newest or oldest updated assets. Star count : Sort assets by most or fewest stars . Popularity : Sort Snowflake and Google BigQuery assets by the most or least popular assets . The sorting options may vary depending on the asset type selected. For example, if you are viewing the results while filtering by the Table tab, you'll also have the option of sorting by the most or fewest number of rows and columns. See only what you want to see"}
{"url":"https:\/\/docs.atlan.com\/get-started\/how-tos\/quick-start-for-admins#discovery","title":"Administrators | Atlan Documentation","text":"User management User management is a critical part of data governance. Atlan's user management capabilities should be a mainstay of how you organize and control access for people in your organization. Add and manage users from the admin center It's super simple to invite and remove users from Atlan from the Admin center . You can also manage existing users by adding them to groups, changing their roles, or set up SSO , SCIM , and SMTP configurations. Manage access control from the governance center The Governance center is where you can build access control mechanisms to manage user access . Personas allow you to group users into teams, such as Financial Analysts or Cloud Engineers , and set policies based on the access those personas should have. Purposes are where you can build policies based on the actions or access that a user might need. For example, you can use Atlan's policy-based access controls to manage access to PII and other sensitive data. This is a best practice for data governance. Once you set these policies, Atlan will enforce them throughout your users' experience. This means that users who don't have access to a particular type of data will not be able to see it. Governance workflows help you set up robust controls on data access management, metadata enrichment, new entity creation, and more, with out-of-the-box workflow templates and automated execution. Asset profile The asset profile in Atlan gives you a quick and clear understanding of what a data asset contains. You can think of the asset profile as the TL;DR about your data. The glossary provides key intel on your data assets so you can quickly understand important attributes of your data, such as: Owners of your data, so you know who to ask for clarification. Certificate status, to easily understand if metadata enrichment is still in progress or the asset is ready to be used. Linked assets that are relevant to the term, so you can explore other helpful material. Here are a few of the things that make Atlan's discovery awesome: Intelligent keyword recognition sees through your typos to show exactly what you wanted, no matter what you actually typed. Sort by popularity to quickly discover what assets your teammates are using every day."}
{"url":"https:\/\/docs.atlan.com\/secure-agent#__docusaurus_skipToContent_fallback","title":"Secure Agent | Atlan Documentation","text":"The Atlan Secure Agent is a lightweight, Kubernetes-based application that enables secure metadata extraction. It connects internal systems with Atlan SaaS while keeping sensitive data protected and doesn t require inbound connectivity. Running within an organization s controlled environment, the Secure Agent ensures compliance with security policies and automates metadata processing. Figure 1: The Secure Agent runs in the customer environment and acts as a gateway. Key capabilities The Secure Agent is designed for secure, scalable, and efficient metadata extraction. Security-first architecture Runs entirely within the organization's infrastructure, preventing secrets from leaving its boundary. Uses outbound, encrypted communication to interact with Atlan SaaS. Supports logging and monitoring and integrates with external monitoring systems for auditing and compliance. Scalable metadata extraction A single deployment of the Agent can connect to multiple source systems. Supports multiple concurrent metadata extraction jobs. Uses Kubernetes-based workloads for efficient resource management. Flexible deployment Deploys on cloud-based Kubernetes environments (such as Amazon EKS, Azure AKS, and Google GKE) or on-premises clusters. Scales dynamically based on workload demands. Automated operations Continuously monitors system health and sends heartbeats to Atlan. Captures and uploads execution logs for troubleshooting and auditing. Provides performance insights through metrics and alerts. How it works The Secure Agent follows a job-based execution model where metadata extraction tasks are scheduled and executed within the organization's environment. The workflow typically involves: Atlan triggers a metadata extraction job. The Secure Agent retrieves job details and extracts metadata using source-specific connectors. Extracted metadata is shared with Atlan either through cloud storage or direct ingestion. Atlan workflows process the extracted metadata and publish the assets. Logs and execution status are sent to Atlan for monitoring and auditing. See also Deployment architecture : Learn more about how the Secure Agent integrates with your environment and supports secure metadata extraction. How it works"}
{"url":"https:\/\/docs.atlan.com\/secure-agent#key-capabilities","title":"Secure Agent | Atlan Documentation","text":"The Atlan Secure Agent is a lightweight, Kubernetes-based application that enables secure metadata extraction. It connects internal systems with Atlan SaaS while keeping sensitive data protected and doesn t require inbound connectivity. Running within an organization s controlled environment, the Secure Agent ensures compliance with security policies and automates metadata processing. Figure 1: The Secure Agent runs in the customer environment and acts as a gateway. Key capabilities The Secure Agent is designed for secure, scalable, and efficient metadata extraction. Security-first architecture Runs entirely within the organization's infrastructure, preventing secrets from leaving its boundary. Uses outbound, encrypted communication to interact with Atlan SaaS. Supports logging and monitoring and integrates with external monitoring systems for auditing and compliance. Scalable metadata extraction A single deployment of the Agent can connect to multiple source systems. Supports multiple concurrent metadata extraction jobs. Uses Kubernetes-based workloads for efficient resource management. Flexible deployment Deploys on cloud-based Kubernetes environments (such as Amazon EKS, Azure AKS, and Google GKE) or on-premises clusters. Scales dynamically based on workload demands. Automated operations Continuously monitors system health and sends heartbeats to Atlan. Captures and uploads execution logs for troubleshooting and auditing. Provides performance insights through metrics and alerts. How it works The Secure Agent follows a job-based execution model where metadata extraction tasks are scheduled and executed within the organization's environment. The workflow typically involves: Atlan triggers a metadata extraction job. The Secure Agent retrieves job details and extracts metadata using source-specific connectors. Extracted metadata is shared with Atlan either through cloud storage or direct ingestion. Atlan workflows process the extracted metadata and publish the assets. Logs and execution status are sent to Atlan for monitoring and auditing. See also Deployment architecture : Learn more about how the Secure Agent integrates with your environment and supports secure metadata extraction. How it works"}
{"url":"https:\/\/docs.atlan.com\/secure-agent#security-first-architecture","title":"Secure Agent | Atlan Documentation","text":"The Atlan Secure Agent is a lightweight, Kubernetes-based application that enables secure metadata extraction. It connects internal systems with Atlan SaaS while keeping sensitive data protected and doesn t require inbound connectivity. Running within an organization s controlled environment, the Secure Agent ensures compliance with security policies and automates metadata processing. Figure 1: The Secure Agent runs in the customer environment and acts as a gateway. Key capabilities The Secure Agent is designed for secure, scalable, and efficient metadata extraction. Security-first architecture Runs entirely within the organization's infrastructure, preventing secrets from leaving its boundary. Uses outbound, encrypted communication to interact with Atlan SaaS. Supports logging and monitoring and integrates with external monitoring systems for auditing and compliance. Scalable metadata extraction A single deployment of the Agent can connect to multiple source systems. Supports multiple concurrent metadata extraction jobs. Uses Kubernetes-based workloads for efficient resource management. Flexible deployment Deploys on cloud-based Kubernetes environments (such as Amazon EKS, Azure AKS, and Google GKE) or on-premises clusters. Scales dynamically based on workload demands. Automated operations Continuously monitors system health and sends heartbeats to Atlan. Captures and uploads execution logs for troubleshooting and auditing. Provides performance insights through metrics and alerts. How it works The Secure Agent follows a job-based execution model where metadata extraction tasks are scheduled and executed within the organization's environment. The workflow typically involves: Atlan triggers a metadata extraction job. The Secure Agent retrieves job details and extracts metadata using source-specific connectors. Extracted metadata is shared with Atlan either through cloud storage or direct ingestion. Atlan workflows process the extracted metadata and publish the assets. Logs and execution status are sent to Atlan for monitoring and auditing. See also Deployment architecture : Learn more about how the Secure Agent integrates with your environment and supports secure metadata extraction. How it works"}
{"url":"https:\/\/docs.atlan.com\/secure-agent#scalable-metadata-extraction","title":"Secure Agent | Atlan Documentation","text":"The Atlan Secure Agent is a lightweight, Kubernetes-based application that enables secure metadata extraction. It connects internal systems with Atlan SaaS while keeping sensitive data protected and doesn t require inbound connectivity. Running within an organization s controlled environment, the Secure Agent ensures compliance with security policies and automates metadata processing. Figure 1: The Secure Agent runs in the customer environment and acts as a gateway. Key capabilities The Secure Agent is designed for secure, scalable, and efficient metadata extraction. Security-first architecture Runs entirely within the organization's infrastructure, preventing secrets from leaving its boundary. Uses outbound, encrypted communication to interact with Atlan SaaS. Supports logging and monitoring and integrates with external monitoring systems for auditing and compliance. Scalable metadata extraction A single deployment of the Agent can connect to multiple source systems. Supports multiple concurrent metadata extraction jobs. Uses Kubernetes-based workloads for efficient resource management. Flexible deployment Deploys on cloud-based Kubernetes environments (such as Amazon EKS, Azure AKS, and Google GKE) or on-premises clusters. Scales dynamically based on workload demands. Automated operations Continuously monitors system health and sends heartbeats to Atlan. Captures and uploads execution logs for troubleshooting and auditing. Provides performance insights through metrics and alerts. How it works The Secure Agent follows a job-based execution model where metadata extraction tasks are scheduled and executed within the organization's environment. The workflow typically involves: Atlan triggers a metadata extraction job. The Secure Agent retrieves job details and extracts metadata using source-specific connectors. Extracted metadata is shared with Atlan either through cloud storage or direct ingestion. Atlan workflows process the extracted metadata and publish the assets. Logs and execution status are sent to Atlan for monitoring and auditing. See also Deployment architecture : Learn more about how the Secure Agent integrates with your environment and supports secure metadata extraction. How it works"}
{"url":"https:\/\/docs.atlan.com\/secure-agent#flexible-deployment","title":"Secure Agent | Atlan Documentation","text":"The Atlan Secure Agent is a lightweight, Kubernetes-based application that enables secure metadata extraction. It connects internal systems with Atlan SaaS while keeping sensitive data protected and doesn t require inbound connectivity. Running within an organization s controlled environment, the Secure Agent ensures compliance with security policies and automates metadata processing. Figure 1: The Secure Agent runs in the customer environment and acts as a gateway. Key capabilities The Secure Agent is designed for secure, scalable, and efficient metadata extraction. Security-first architecture Runs entirely within the organization's infrastructure, preventing secrets from leaving its boundary. Uses outbound, encrypted communication to interact with Atlan SaaS. Supports logging and monitoring and integrates with external monitoring systems for auditing and compliance. Scalable metadata extraction A single deployment of the Agent can connect to multiple source systems. Supports multiple concurrent metadata extraction jobs. Uses Kubernetes-based workloads for efficient resource management. Flexible deployment Deploys on cloud-based Kubernetes environments (such as Amazon EKS, Azure AKS, and Google GKE) or on-premises clusters. Scales dynamically based on workload demands. Automated operations Continuously monitors system health and sends heartbeats to Atlan. Captures and uploads execution logs for troubleshooting and auditing. Provides performance insights through metrics and alerts. How it works The Secure Agent follows a job-based execution model where metadata extraction tasks are scheduled and executed within the organization's environment. The workflow typically involves: Atlan triggers a metadata extraction job. The Secure Agent retrieves job details and extracts metadata using source-specific connectors. Extracted metadata is shared with Atlan either through cloud storage or direct ingestion. Atlan workflows process the extracted metadata and publish the assets. Logs and execution status are sent to Atlan for monitoring and auditing. See also Deployment architecture : Learn more about how the Secure Agent integrates with your environment and supports secure metadata extraction. How it works"}
{"url":"https:\/\/docs.atlan.com\/secure-agent#automated-operations","title":"Secure Agent | Atlan Documentation","text":"The Atlan Secure Agent is a lightweight, Kubernetes-based application that enables secure metadata extraction. It connects internal systems with Atlan SaaS while keeping sensitive data protected and doesn t require inbound connectivity. Running within an organization s controlled environment, the Secure Agent ensures compliance with security policies and automates metadata processing. Figure 1: The Secure Agent runs in the customer environment and acts as a gateway. Key capabilities The Secure Agent is designed for secure, scalable, and efficient metadata extraction. Security-first architecture Runs entirely within the organization's infrastructure, preventing secrets from leaving its boundary. Uses outbound, encrypted communication to interact with Atlan SaaS. Supports logging and monitoring and integrates with external monitoring systems for auditing and compliance. Scalable metadata extraction A single deployment of the Agent can connect to multiple source systems. Supports multiple concurrent metadata extraction jobs. Uses Kubernetes-based workloads for efficient resource management. Flexible deployment Deploys on cloud-based Kubernetes environments (such as Amazon EKS, Azure AKS, and Google GKE) or on-premises clusters. Scales dynamically based on workload demands. Automated operations Continuously monitors system health and sends heartbeats to Atlan. Captures and uploads execution logs for troubleshooting and auditing. Provides performance insights through metrics and alerts. How it works The Secure Agent follows a job-based execution model where metadata extraction tasks are scheduled and executed within the organization's environment. The workflow typically involves: Atlan triggers a metadata extraction job. The Secure Agent retrieves job details and extracts metadata using source-specific connectors. Extracted metadata is shared with Atlan either through cloud storage or direct ingestion. Atlan workflows process the extracted metadata and publish the assets. Logs and execution status are sent to Atlan for monitoring and auditing. See also Deployment architecture : Learn more about how the Secure Agent integrates with your environment and supports secure metadata extraction. How it works"}
{"url":"https:\/\/docs.atlan.com\/secure-agent#how-it-works","title":"Secure Agent | Atlan Documentation","text":"The Atlan Secure Agent is a lightweight, Kubernetes-based application that enables secure metadata extraction. It connects internal systems with Atlan SaaS while keeping sensitive data protected and doesn t require inbound connectivity. Running within an organization s controlled environment, the Secure Agent ensures compliance with security policies and automates metadata processing. Figure 1: The Secure Agent runs in the customer environment and acts as a gateway. Key capabilities The Secure Agent is designed for secure, scalable, and efficient metadata extraction. Security-first architecture Runs entirely within the organization's infrastructure, preventing secrets from leaving its boundary. Uses outbound, encrypted communication to interact with Atlan SaaS. Supports logging and monitoring and integrates with external monitoring systems for auditing and compliance. Scalable metadata extraction A single deployment of the Agent can connect to multiple source systems. Supports multiple concurrent metadata extraction jobs. Uses Kubernetes-based workloads for efficient resource management. Flexible deployment Deploys on cloud-based Kubernetes environments (such as Amazon EKS, Azure AKS, and Google GKE) or on-premises clusters. Scales dynamically based on workload demands. Automated operations Continuously monitors system health and sends heartbeats to Atlan. Captures and uploads execution logs for troubleshooting and auditing. Provides performance insights through metrics and alerts. How it works The Secure Agent follows a job-based execution model where metadata extraction tasks are scheduled and executed within the organization's environment. The workflow typically involves: Atlan triggers a metadata extraction job. The Secure Agent retrieves job details and extracts metadata using source-specific connectors. Extracted metadata is shared with Atlan either through cloud storage or direct ingestion. Atlan workflows process the extracted metadata and publish the assets. Logs and execution status are sent to Atlan for monitoring and auditing. See also Deployment architecture : Learn more about how the Secure Agent integrates with your environment and supports secure metadata extraction. How it works"}
{"url":"https:\/\/docs.atlan.com\/secure-agent#see-also","title":"Secure Agent | Atlan Documentation","text":"The Atlan Secure Agent is a lightweight, Kubernetes-based application that enables secure metadata extraction. It connects internal systems with Atlan SaaS while keeping sensitive data protected and doesn t require inbound connectivity. Running within an organization s controlled environment, the Secure Agent ensures compliance with security policies and automates metadata processing. Figure 1: The Secure Agent runs in the customer environment and acts as a gateway. Key capabilities The Secure Agent is designed for secure, scalable, and efficient metadata extraction. Security-first architecture Runs entirely within the organization's infrastructure, preventing secrets from leaving its boundary. Uses outbound, encrypted communication to interact with Atlan SaaS. Supports logging and monitoring and integrates with external monitoring systems for auditing and compliance. Scalable metadata extraction A single deployment of the Agent can connect to multiple source systems. Supports multiple concurrent metadata extraction jobs. Uses Kubernetes-based workloads for efficient resource management. Flexible deployment Deploys on cloud-based Kubernetes environments (such as Amazon EKS, Azure AKS, and Google GKE) or on-premises clusters. Scales dynamically based on workload demands. Automated operations Continuously monitors system health and sends heartbeats to Atlan. Captures and uploads execution logs for troubleshooting and auditing. Provides performance insights through metrics and alerts. How it works The Secure Agent follows a job-based execution model where metadata extraction tasks are scheduled and executed within the organization's environment. The workflow typically involves: Atlan triggers a metadata extraction job. The Secure Agent retrieves job details and extracts metadata using source-specific connectors. Extracted metadata is shared with Atlan either through cloud storage or direct ingestion. Atlan workflows process the extracted metadata and publish the assets. Logs and execution status are sent to Atlan for monitoring and auditing. See also Deployment architecture : Learn more about how the Secure Agent integrates with your environment and supports secure metadata extraction. How it works"}
{"url":"https:\/\/docs.atlan.com\/secure-agent\/references\/deployment-architecture","title":"Deployment architecture | Atlan Documentation","text":"The Atlan Secure Agent is a Kubernetes-based application that runs within a customer's environment. It acts as a gateway between the single-tenant Atlan SaaS and external systems like Snowflake, Tableau, and other data sources. This document explains the Secure Agent's deployment architecture, key components, communication flows, and security considerations. High-level architecture This section describes how the Secure Agent is structured and deployed. It explains the core components that enable metadata extraction, job execution, and communication with Atlan. Figure 1: Atlan Secure Agent deployment architecture. Core components The Secure Agent runs as a Kubernetes-based application within a customer's private cloud or on-premises environment. It consists of several key components that work together to execute metadata extraction tasks. Argo Workflows An Argo Workflow server is deployed to coordinate all activities and launch Kubernetes workloads. The Secure Agent uses Argo Workflows to orchestrate and manage metadata extraction jobs. Each workflow represents a unit of work, such as extracting metadata from a source system. Agent orchestrator A scheduled job that runs every five minutes to check for jobs that need to be executed. It connects to the Atlan tenant, retrieves job details, and initiates workflows accordingly. Auxiliary services Additional services that support agent operations: Health monitoring service sends periodic heartbeats to Atlan to confirm the agent is active. Logging service uploads execution logs to Atlan for monitoring and debugging. Metadata extraction workflows Connector-specific jobs that extract metadata from source systems. Workflows run in isolated containers, ensuring security and reliability. Data flow The Secure Agent supports two modes of metadata transfer. Each mode determines how extracted metadata is delivered to Atlan. Bucket relay Metadata extraction in bucket relay mode stores metadata in enterprise-managed cloud storage before Atlan retrieves it. Figure 2: Data flow in bucket relay mode. The Secure Agent extracts metadata and writes it to a storage bucket in the customer s cloud environment (such as AWS S3, Azure Blob Storage, or Google Cloud Storage). This is managed by providing the agent write access to cloud storage. Atlan retrieves metadata from the storage bucket and processes it further. This is managed by providing Atlan read access to list and read files in cloud storage. This mode ensures the extracted data remains within the customer s infrastructure until Atlan explicitly fetches it. Customers can also use this data for auditing. Direct ingestion In direct ingestion mode, metadata is transferred directly from the Secure Agent to Atlan. Figure 3: Data flow in direct ingestion mode. The Secure Agent uses pre-signed URLs to upload metadata directly to Atlan. Some cloud storage providers that use pre-signed URLs include: The Secure Agent uses pre-signed URLs to upload metadata directly to Atlan. Some cloud storage providers that use pre-signed URLs include: A pre-signed URL grants temporary access to upload files without exposing long-term credentials. A pre-signed URL grants temporary access to upload files without exposing long-term credentials. Each URL has an expiration time, ensuring access is only available for a limited duration. Each URL has an expiration time, ensuring access is only available for a limited duration. See also Secure Agent - Security : Details on security mechanisms."}
{"url":"https:\/\/docs.atlan.com\/product\/capabilities\/playbooks#__docusaurus_skipToContent_fallback","title":"Playbooks | Atlan Documentation","text":"Get started How to set up playbooks Playbook management How to manage playbooks : Monitor and maintain your playbook workflows. How to automate data profiling : Set up automated data quality checks. Troubleshooting playbooks : Solutions for common playbook issues."}
{"url":"https:\/\/docs.atlan.com\/product\/capabilities\/playbooks#get-started","title":"Playbooks | Atlan Documentation","text":"Get started How to set up playbooks Playbook management How to manage playbooks : Monitor and maintain your playbook workflows. How to automate data profiling : Set up automated data quality checks. Troubleshooting playbooks : Solutions for common playbook issues."}
{"url":"https:\/\/docs.atlan.com\/product\/capabilities\/playbooks\/how-tos\/set-up-playbooks","title":"Set up playbooks | Atlan Documentation","text":"Ã° Â¤ Who can do this? You will need to be an admin user in Atlan to create playbooks. A common question that data teams often face is how to automate metadata at scale. Having started out as a data team ourselves, we know that automating repetitive tasks can help data teams maximize the value they provide to their organization. One way of doing so is through Atlan's playbooks! Playbooks help power metadata automation for your data assets in Atlan. You can create rule-based automations at scale and update metadata in bulk, helping you streamline your workflows. You can update the following asset metadata using playbooks: For example, imagine your organization needs to transfer ownership of several data assets. Instead of your data team manually updating the ownership of each and every asset, you can create a playbook to automate this process and update the metadata of your assets at scale. Playbook recommendations Before you begin, review some general guidelines on running playbooks in Atlan: Avoid running multiple playbooks simultaneously on the same set of assets. Allow one playbook run to be completed before proceeding with another operation on the same set of assets. Otherwise, you may experience performance issues and inconsistencies. Review and understand the depth of your asset lineage or hierarchy prior to enabling a tag propagation playbook. For assets with complex lineage, tag propagation may take longer to complete than the playbook runtime. You may want to review and judiciously select a list of assets that need to be tagged directly. For their child and\/or downstream assets, Atlan recommends that you enable tag propagation . Create a playbook To create a playbook in Atlan: From the Filters menu on the left or the tabs along the top, apply any asset filters . Click Governance to navigate to the governance center. Under the Governance heading of the Governance center , click Playbooks . Click Create New to get started. Under the Governance heading of the Governance center , click Playbooks . Click Create New to get started. In the Create new playbook dialog box, enter the following details: For Name , enter a name for the task to be accomplished - for example, Update ownership . (Atlan recommends that the length of a playbook name must be no longer than 46 characters.) (Optional) For Description , enter a description. (Optional) Select an icon for your playbook. For Name , enter a name for the task to be accomplished - for example, Update ownership . (Atlan recommends that the length of a playbook name must be no longer than 46 characters.) (Optional) For Description , enter a description. (Optional) Select an icon for your playbook. Click Create to save your playbook. Set up rules as filters To set up rules as filters for your playbook: In the Build Rules page of your playbook, click Filters . For name, add a name to your filter. To set a matching condition for the filters, select Match all or Match any . Match all will logically AND the criteria, while Match any will logically OR the criteria. For this example, we'll click Connection and then select a Snowflake connection. (Optional) To further refine your asset selection: Click All databases to filter by databases in a selected connection. Click All schemas to filter by schemas in a selected connection. Click All databases to filter by databases in a selected connection. Click All schemas to filter by schemas in a selected connection. Click Connector to filter assets by supported connectors . Click Asset type to filter by specific asset types - for example, tables, columns, queries, glossaries, and more. Click Certificate to filter assets by certification status . Click Owners to filter assets by asset owners . Click Tags to filter assets by your tags in Atlan, including imported Snowflake and dbt tags. (Optional) For Snowflake tags only, to the left of the checkbox, click Select value , and then from the Select tag value dialog, select any value(s) to filter assets by tag value. (Optional) For Snowflake tags only, to the left of the checkbox, click Select value , and then from the Select tag value dialog, select any value(s) to filter assets by tag value. Click Domains to filter by specific domains or subdomains to bulk update all the assets included in those data domains or subdomains. Click Products to filter for data products by specific data domains or subdomains. Click Schema qualified Name to filter assets by the qualified name of a given schema. Click Database qualified Name to filter assets by the qualified name of a given database. Click dbt to filter assets by dbt-specific filters and then select a dbt Cloud or dbt Core filter. Click Properties to filter assets by common asset properties . Click Usage to filter assets by usage metrics . Click Monte Carlo to filter assets by Monte Carlo-specific filters . Click Soda to filter assets by Soda-specific filters . Click Table\/View to filter tables or views by row count, column count, or size. Click Column to filter columns by column-specific filters , including parent asset type or name, data type, or column keys . Click Process to filter lineage processes by the SQL query. Click Query to filter assets by associated visual queries . Click Measure to filter Microsoft Power BI measures using the external measures filter. Select Starts With or Ends With to filter assets using the starting or ending sequence of values. Select Contains or Does not contain to find assets with or without specified values contained within the attribute. Select Pattern to filter assets using supported Elastic DSL regular expressions . Select Is empty to filter assets with null values. Select Belongs to or Doesn't belong to to filter data products by specific data domains or subdomains . For Values , select the relevant values. The values will vary depending on the selected attributes. (Optional) To add more filters, click Add filter and select Filter to add individual filters or Filter Group to nest more filters in a group. (Optional) To view all the assets that match your rules, in the Filters card, click View all for a preview. (Optional) To remove a playbook filter, to the right of any filter, click the three horizontal dots and then click Delete . (Optional) To turn off a playbook filter, to the right of any filter, click the three horizontal dots and then click Disable . Click Enable to turn on any disabled filters. Select the actions To select the actions to be performed based on your rules: In the Build Rules page of your playbook, click Actions . Click Certificate to update the certification status of assets to Verified , Draft , Deprecated , or No certificate . Click Description to update the description of your assets. Click Owners to add, remove, or replace asset owners . In this example, we'll update the ownership of the assets. Click Tags to add tags to your assets or remove or replace them from tagged or propagated assets. Note that if there are multiple tag actions to be performed, Atlan will execute them in the following order: ADD , REMOVE , and then REPLACE . Click Domain to add your assets to a specific domain or subdomain or remove them from an existing linked domain or subdomain . Click any custom metadata structure and then select a custom metadata property to update or unlink it from your assets. For Select operator , select the relevant option. The operators will vary depending on the selected action. For Values , select the relevant option(s). The values will vary depending on the selected actions. (Optional) To add more actions, click Add Action . You can control tag propagation when adding tags as an action in playbooks. Tag propagation is disabled by default. If you enable tag propagation, you will also be able to configure how tags are propagated . Run the playbook If you'd like to continue working on your playbook, you can save it as a draft. If your playbook is ready, you can proceed to running it. To run the playbook: You can either: To run the playbook once immediately, click Run once . To schedule the playbook to run hourly, daily, weekly, or monthly, click Schedule and choose the preferred frequency, timezone, and time. danger If you're scheduling multiple playbooks, Atlan recommends spacing out the schedules as much as possible to minimize any overlap between the playbook workflow runs. For more about workflows in general, see workflow recommendations . You can either: To run the playbook once immediately, click Run once . To run the playbook once immediately, click Run once . To schedule the playbook to run hourly, daily, weekly, or monthly, click Schedule and choose the preferred frequency, timezone, and time. danger If you're scheduling multiple playbooks, Atlan recommends spacing out the schedules as much as possible to minimize any overlap between the playbook workflow runs. For more about workflows in general, see workflow recommendations . To schedule the playbook to run hourly, daily, weekly, or monthly, click Schedule and choose the preferred frequency, timezone, and time. If you're scheduling multiple playbooks, Atlan recommends spacing out the schedules as much as possible to minimize any overlap between the playbook workflow runs. For more about workflows in general, see workflow recommendations . Click Complete to run the playbook. Click Complete to run the playbook. In the resulting screen, click Go to profile to view your playbook profile. In the resulting screen, click Go to profile to view your playbook profile. Once your playbook has completed its run, you will see the metadata updated for your assets! Ã° If you have any questions about setting up playbooks, head over here . Create a playbook Set up rules as filters Select the actions Run the playbook"}
{"url":"https:\/\/docs.atlan.com\/product\/capabilities\/playbooks#guides","title":"Playbooks | Atlan Documentation","text":"Get started How to set up playbooks Playbook management How to manage playbooks : Monitor and maintain your playbook workflows. How to automate data profiling : Set up automated data quality checks. Troubleshooting playbooks : Solutions for common playbook issues."}
{"url":"https:\/\/docs.atlan.com\/product\/capabilities\/playbooks#playbook-management","title":"Playbooks | Atlan Documentation","text":"Get started How to set up playbooks Playbook management How to manage playbooks : Monitor and maintain your playbook workflows. How to automate data profiling : Set up automated data quality checks. Troubleshooting playbooks : Solutions for common playbook issues."}
{"url":"https:\/\/docs.atlan.com\/product\/capabilities\/playbooks\/how-tos\/manage-playbooks","title":"Manage playbooks | Atlan Documentation","text":"Once you've created a playbook , you can monitor, modify, or delete it at any time. You can also enable notifications to monitor your playbook runs directly in Slack or Microsoft Teams. Monitor a playbook To monitor your playbooks runs: When running a playbook immediately, you will be redirected to the monitoring page within 5 seconds. From the left menu of any screen in Atlan, click Governance . Under the Governance heading of the Governance center , click Playbooks . In the playbooks manager, select the playbook you'd like to view. A summary of the rules, actions, and updated assets. An activity log for recent runs and updates over time. The activity log in the playbooks manager can help you keep track of playbook runs, ranging from 24 hours to 30 days. Select any of the entries to navigate to the corresponding playbook. Modify a playbook To modify an existing playbook: From the left menu of any screen in Atlan, click Governance. Under the Governance heading of the Governance center , click Playbooks . In the playbooks manager, click the playbook you'd like to modify. On your playbook page: To edit the name and description of your playbook, hover over your playbook and click Edit . To modify the rules of your playbook, click Rules to make your changes and then click Update to save them. (Optional) To add a new rule to an existing playbook, in the left menu for playbook rules, click + Add new Rule . To turn off a playbook filter, to the right of any filter, click the three horizontal dots and then click Disable . Click Enable to turn on any disabled filters. To modify the schedule for your playbook, in the upper right of your screen: Click Run Now to run it immediately. Click the pencil icon to modify or remove the schedule. To edit the name and description of your playbook, hover over your playbook and click Edit . To modify the rules of your playbook, click Rules to make your changes and then click Update to save them. (Optional) To add a new rule to an existing playbook, in the left menu for playbook rules, click + Add new Rule . (Optional) To add a new rule to an existing playbook, in the left menu for playbook rules, click + Add new Rule . To turn off a playbook filter, to the right of any filter, click the three horizontal dots and then click Disable . Click Enable to turn on any disabled filters. To modify the schedule for your playbook, in the upper right of your screen: Click Run Now to run it immediately. Click the pencil icon to modify or remove the schedule. Click Run Now to run it immediately. Click the pencil icon to modify or remove the schedule. Delete a playbook To delete an existing playbook: From the left menu of any screen in Atlan, click Governance. Under the Governance heading of the Governance center , click Playbooks . In the playbooks manager, hover over the playbook you'd like to delete and click Delete Playbook . Click Delete to confirm. Enable playbook notifications You can set up Slack or Microsoft Teams alerts for your playbook runs in Atlan. This can help you monitor your playbooks directly in Slack or Microsoft Teams. You can also choose to receive alerts for failed playbook runs only. Before you can enable notifications for playbooks, you will need to either: Integrate Slack and Atlan Integrate Microsoft Teams and Atlan To enable notifications for playbook runs: From the left menu of any screen in Atlan, click Governance . Under the Governance heading of the Governance center , click Playbooks . In the upper-right of the playbooks manager, under Activity , click the bell icon. In the Enable notifications popup: Click Setup now to integrate Slack or Microsoft Teams . If you have already integrated Slack or Microsoft Teams, click Enable . Click Setup now to integrate Slack or Microsoft Teams . If you have already integrated Slack or Microsoft Teams, click Enable . In the notifications setup dialog, configure the following: For Notifications channel , you can either: If you have already configured a Slack or Microsoft Teams channel to receive workflow alerts, that channel will be preselected. You can use the same channel to receive both workflow and playbook run alerts and skip to the next step. If you have not configured a workflow alerts channel or want to add a different one, enter the channel name to receive notifications for playbook runs. This channel will be displayed as the Playbooks alert channel in your Slack or Microsoft Teams integration. To select the type of notifications you want to receive, you can either: Click Both success and failure alerts to receive notifications for both successful and failed playbook runs. Click Failure alerts only to limit notifications to failed playbook runs only. Click Save to save your notification preferences. For Notifications channel , you can either: If you have already configured a Slack or Microsoft Teams channel to receive workflow alerts, that channel will be preselected. You can use the same channel to receive both workflow and playbook run alerts and skip to the next step. If you have not configured a workflow alerts channel or want to add a different one, enter the channel name to receive notifications for playbook runs. This channel will be displayed as the Playbooks alert channel in your Slack or Microsoft Teams integration. If you have already configured a Slack or Microsoft Teams channel to receive workflow alerts, that channel will be preselected. You can use the same channel to receive both workflow and playbook run alerts and skip to the next step. If you have not configured a workflow alerts channel or want to add a different one, enter the channel name to receive notifications for playbook runs. This channel will be displayed as the Playbooks alert channel in your Slack or Microsoft Teams integration. To select the type of notifications you want to receive, you can either: Click Both success and failure alerts to receive notifications for both successful and failed playbook runs. Click Failure alerts only to limit notifications to failed playbook runs only. Click Both success and failure alerts to receive notifications for both successful and failed playbook runs. Click Failure alerts only to limit notifications to failed playbook runs only. Click Save to save your notification preferences. (Optional) To disable notifications, from the notifications setup dialog, remove the playbook alerts channel configured for Slack or Microsoft Teams . You will now receive Slack or Microsoft Teams notifications for all your playbook runs in Atlan! Ã° The Atlan bot will share playbook run alerts, including details like run status, start time, run time, trigger type, last three runs, and more. Monitor a playbook Modify a playbook Delete a playbook Enable playbook notifications"}
{"url":"https:\/\/docs.atlan.com\/product\/capabilities\/playbooks\/how-tos\/automate-data-profiling","title":"Automate data profiling | Atlan Documentation","text":"Available via the Data Quality Studio package Ã° Â¤ Who can do this? You need to be an admin user in Atlan to create profiling playbooks. Monitoring and improving data quality is critical to building trust in your data assets. Atlan solves for this with profiling playbooks! Profiling playbooks help power data observability for your assets in Atlan. You can create profiling playbooks to scan your assets at scale, identify any issues or inconsistencies, and improve the data quality of your assets. Supported sources Atlan currently supports column profiling for the following connectors: Microsoft SQL Server Create a profiling playbook To create a profiling playbook: In the left menu in Atlan, click Governance . Under the Governance heading of the Governance center , click Playbooks . To the right of the Create New button, click the downward arrow and then select Profiling Playbook . In the Create new profiling playbook dialog, enter the following details: For Name , enter a name for the task to be accomplished - for example, Tables scan . (Atlan recommends that the length of a playbook name must be no longer than 46 characters.) For Connection , select a supported connection from the dropdown menu - in this example, we'll select a Google BigQuery connection development . (Optional) For Description , enter a description for your playbook. (Optional) Select an icon for your playbook. For Name , enter a name for the task to be accomplished - for example, Tables scan . (Atlan recommends that the length of a playbook name must be no longer than 46 characters.) For Connection , select a supported connection from the dropdown menu - in this example, we'll select a Google BigQuery connection development . (Optional) For Description , enter a description for your playbook. (Optional) Select an icon for your playbook. Click Create to save your playbook. Set up rules as filters The assets to be scanned are pre-filled based on your selected connection. To set up rules as filters for your profiling playbook: In the Build Rules page of your profiling playbook, click Filters . For the name field, add a name to your filter - for example, Profiling action . To set a matching condition for the filters, select Match all or Match any . Match all will logically AND the criteria, while Match any will logically OR the criteria. Click Connection to select a specific connection. Click All databases to filter by databases in a selected connection. Click All schemas to filter by schemas in a selected connection. Click All databases to filter by databases in a selected connection. Click All schemas to filter by schemas in a selected connection. Click Connector to filter assets by supported connectors . Click Asset type to filter by specific asset types - for example, tables, columns, queries, glossaries, and more. Click Certificate to filter assets by certification status . Click Owners to filter assets by asset owners . Click Tags to filter assets by your tags in Atlan, including imported Snowflake and dbt tags. (Optional) For Snowflake tags only, to the left of the checkbox, click Select value , and then from the Select tag value dialog, select any value(s) to filter assets by tag value. (Optional) For Snowflake tags only, to the left of the checkbox, click Select value , and then from the Select tag value dialog, select any value(s) to filter assets by tag value. Click Schema qualified Name to filter assets by the qualified name of a given schema. Click Database qualified Name to filter assets by the qualified name of a given database. Click dbt to filter assets by dbt-specific filters and then select a dbt Cloud or dbt Core filter. Click Properties to filter assets by common asset properties . Click Usage to filter assets by usage metrics . Click Monte Carlo to filter assets by Monte Carlo-specific filters . Click Soda to filter assets by Soda-specific filters . Click Table\/View to filter tables or views by row count, column count, or size. Click Column to filter columns by column-specific filters , including parent asset type or name, data type, or column keys . Click Process to filter lineage processes by the SQL query. Click Query to filter assets by associated visual queries . Click Measure to filter Microsoft Power BI measures using the external measures filter. Select Starts With or Ends With to filter assets using the starting or ending sequence of values. Select Contains or Does not contain to find assets with or without specified values contained within the attribute. Select Pattern to filter assets using supported Elastic DSL regular expressions . Select Is empty to filter assets with null values. For Values , select the relevant values. The values will vary depending on the selected attributes. (Optional) To add more filters, click Add filter and select Filter to add individual filters or Filter Group to nest more filters in a group. (Optional) To view all the assets that match your rules, in the Filters card, click View all for a preview. Confirm profiling actions Column profiling is currently only supported for number and text data types. The profiled column assets will be populated with preconfigured metrics. To select the actions to be performed based on your rules: The default profiling actions to be performed include: Base metrics : Distinct count - number of rows that contain distinct values, relative to the column. Missing count - number of rows that do not contain specific values. Numeric metrics : Minimum and maximum values - smallest and greatest values in a numeric column. Average - calculated average of values in a numeric column. Standard deviation - calculated standard deviation of values in a numeric column. Variance - calculated variance of values in a numeric column. String metrics : Average length - average length of string values in a column. Minimum and maximum length - minimum and maximum length of string values in a column. Base metrics : Distinct count - number of rows that contain distinct values, relative to the column. Missing count - number of rows that do not contain specific values. Distinct count - number of rows that contain distinct values, relative to the column. Missing count - number of rows that do not contain specific values. Numeric metrics : Minimum and maximum values - smallest and greatest values in a numeric column. Average - calculated average of values in a numeric column. Standard deviation - calculated standard deviation of values in a numeric column. Variance - calculated variance of values in a numeric column. Minimum and maximum values - smallest and greatest values in a numeric column. Average - calculated average of values in a numeric column. Standard deviation - calculated standard deviation of values in a numeric column. Variance - calculated variance of values in a numeric column. String metrics : Average length - average length of string values in a column. Minimum and maximum length - minimum and maximum length of string values in a column. Average length - average length of string values in a column. Minimum and maximum length - minimum and maximum length of string values in a column. Click Next to proceed to the next step. In the Optimize your Profiling query popup, the following message will appear: This Profiling playbook will query x rows across y assets. To avoid significant computing costs, review your applied filters before proceeding . Click Review filters to review your existing filters or click Continue anyway to proceed. Note that Atlan is working to support sampling functionality in the future. Run the playbook If you'd like to continue working on your playbook, you can save it as a draft. If your playbook is ready, you can proceed to running it. To run the playbook: You can either: To run the playbook once immediately, click Run once . To schedule the playbook to run hourly, daily, weekly, or monthly, click Schedule and choose the preferred frequency, timezone, and time. To run the playbook once immediately, click Run once . To schedule the playbook to run hourly, daily, weekly, or monthly, click Schedule and choose the preferred frequency, timezone, and time. Click Complete to confirm your selections. In the resulting screen, click Go to profile to view your playbook profile. Once your playbook run is completed, you will see the data profile updated for your assets! Ã° View profiled assets To view the profiled assets for your playbook: In the sidebar to the right, profiled assets will be indicated with a bar graph icon. Click any profiled asset to proceed to viewing profiling data. From the table sidebar, click the Column tab to view column assets and then select any of the profiled columns. In the column sidebar to the right, click Profile to view profiling data for the selected column asset. Once you've created a profiling playbook, you can monitor, modify, or delete it at any time. Create a profiling playbook Set up rules as filters Confirm profiling actions Run the playbook View profiled assets"}
{"url":"https:\/\/docs.atlan.com\/product\/capabilities\/playbooks#troubleshooting","title":"Playbooks | Atlan Documentation","text":"Get started How to set up playbooks Playbook management How to manage playbooks : Monitor and maintain your playbook workflows. How to automate data profiling : Set up automated data quality checks. Troubleshooting playbooks : Solutions for common playbook issues."}
{"url":"https:\/\/docs.atlan.com\/product\/capabilities\/playbooks\/troubleshooting\/troubleshooting-playbooks","title":"Troubleshooting playbooks | Atlan Documentation","text":"Ã° Â¤ Who can do this? You will need to be an admin user in Atlan to create playbooks. Here are a few things to know about setting up playbooks : Following are the known issues or limitations when using the domain action : If you do not have read permission on the assets you want to add to a domain, those assets will be removed from the playbook workflow during processing. If you do not have update permission on the assets you want to add to a domain, the playbook workflow will fail. However, some assets may still be linked to the domain before the failure occurs. We recommend building no more than a maximum of 20 rules per playbook. However, the total number of playbooks that can be run is still to be determined. From a technical standpoint, playbooks leverage the workflow infrastructure, which means there are no hard limits. Depending on the number of playbooks that need to be run, the infrastructure will have to be scaled accordingly. Yes. You need to have the permission to update assets in Atlan in order to run playbooks for updating them. If you do not have the permission to update an asset, you will be unable to update it using playbooks. Additionally, Atlan uses the permissions of the playbook creator in determining the assets to be updated and not that of the user who runs the playbook. Your user permissions are used to determine the bulk updates you can make to ensure that there is no adverse impact on assets beyond your scope of access. No, Atlan currently does not support automating asset update requests through playbooks. Currently, there is no button to undo asset updates. However, you can modify your existing playbooks . You can either turn off the filters or add new rules to reverse the updates. Currently, no. You can monitor your existing playbooks to view a high-level summary of asset updates from previous playbook runs. Observability of results is on the roadmap. Currently, no. However, you can set up Slack or Microsoft Teams alerts for your playbook runs in Atlan. If you encounter an offload node status is not supported error message, the playbook workflow may have exceeded the EtcD size limit. Playbooks use Argo workflow templates, which are stored as Kubernetes resources. This creates a limit to their size. To handle this error, Atlan recommends the following: Reduce the number of rules in your playbook Optimize filters for asset selection"}
{"url":"https:\/\/docs.atlan.com\/product\/capabilities\/governance\/contracts#__docusaurus_skipToContent_fallback","title":"Contracts | Atlan Documentation","text":"Get started Follow these steps to implement contracts in Atlan: Create data contracts Add contract impact analysis in GitHub : Detailed instructions on adding contracts for impact analysis in GitHub."}
{"url":"https:\/\/docs.atlan.com\/product\/capabilities\/governance\/contracts#get-started","title":"Contracts | Atlan Documentation","text":"Get started Follow these steps to implement contracts in Atlan: Create data contracts Add contract impact analysis in GitHub : Detailed instructions on adding contracts for impact analysis in GitHub."}
{"url":"https:\/\/docs.atlan.com\/product\/capabilities\/governance\/contracts\/how-tos\/create-data-contracts","title":"Create data contracts | Atlan Documentation","text":"A data contract is an agreement between a data producer and consumer that specifies requirements for generating and using high-quality, reliable data. As a powerful tool for data management, data contracts can help you standardize contractual obligations between data producers and consumers, organize your assets with embeddable contract metadata, and enforce them with data quality rules. In Atlan, you can directly add a data contract to supported assets and provide helpful context to your downstream users. For a data contract to help build trust in your assets, it should be: Templatized and easily comprehensible - use Atlan's YAML contract template to create standardized contracts and push to Atlan. Version-controlled - continuously validate and monitor your data contracts either in runtime or real-time. Embeddable - embed the contract as metadata for a supported asset. Enforceable - enforce your contracts with data quality rules. Extensible - identify new specifications, generate new versions, and then compare and contrast them. You can create webhooks for data contracts and receive notifications for when a contract is added or updated to a URL of your choice. Supported asset types You can create data contracts for the following asset types: Output port assets of data products Supported asset metadata Atlan maps the following asset metadata properties to it contract properties: Add a data contract to an asset Any non-guest user with edit access to an asset's metadata can create, deploy, and manage data contracts. This only includes admin and member users in Atlan. To add a data contract to an asset, you can either: Create a contract directly in Atlan from the Contracts tab of the asset profile. You can create and maintain data contracts as easily as editing a word document. Use Atlan CLI to import an existing contract from your local machine to Atlan directly or through a CI\/CD pipeline. Atlan CLI is a command-line tool that you can download directly from Atlan to your local machine to create and push data contracts to Atlan. Once you have published the contract, you can also sync metadata from a contract to the governed asset in Atlan. Once created, you will be able to monitor and manage your data contracts in Atlan. To add a data contract: From the left menu of any screen in Atlan, click Assets . (Optional) From the Filters menu on the left, click Properties and then click Has contract . Click No to filter for assets without a contract. From the Assets page, select an asset to open the asset sidebar. In the Contract tab of the asset profile, you can either: Click Create contract to create a draft contract directly in Atlan based on asset metadata. Click Import contract to use Atlan CLI to import an existing contract from your local environment to Atlan. You will first need to install and connect Atlan CLI and then push the contract to Atlan. Refer to our developer documentation to complete the steps. Click Create contract to create a draft contract directly in Atlan based on asset metadata. Click Import contract to use Atlan CLI to import an existing contract from your local environment to Atlan. You will first need to install and connect Atlan CLI and then push the contract to Atlan. Refer to our developer documentation to complete the steps. (Optional) Click the Edit button to edit the contract. Congrats on adding a data contract in Atlan! Ã° View a data contract To view a data contract: From the left menu of any screen in Atlan, click Assets . From the Assets page, select an asset to open the asset sidebar. Click the Document icon to open a read-only, simplified view of your contract. Next to Published version , click the version dropdown to view the latest version of the contract. Select an older version and then click Compare with published version to compare them side by side. Click the Edit button to edit the contract. Click the clipboard icon to copy the YAML code. Under Timeline , view a timeline for the evolution of your contract. Supported asset types Supported asset metadata Add a data contract to an asset View a data contract"}
{"url":"https:\/\/docs.atlan.com\/product\/capabilities\/governance\/contracts#guides","title":"Contracts | Atlan Documentation","text":"Get started Follow these steps to implement contracts in Atlan: Create data contracts Add contract impact analysis in GitHub : Detailed instructions on adding contracts for impact analysis in GitHub."}
{"url":"https:\/\/docs.atlan.com\/product\/capabilities\/governance\/contracts\/how-tos\/add-contract-impact-analysis-in-github","title":"Add contract impact analysis in GitHub | Atlan Documentation","text":"Impact analysis helps you identify how modifications to your data contracts might impact downstream processes, data quality, and overall business operations. This can help you analyze proposed changes and mitigate potential risks before implementation. If you have ever changed a data contract only to find out later that it broke a downstream table or dashboard, Atlan provides a GitHub Action to help you out. This action places Atlan's impact analysis right into your pull request. So, you can view the potential downstream impact of your changes before merging the pull request. Before running the action, you will need to create an Atlan API token . You will also need to assign a persona to the API token and add a metadata policy that provides the requisite permissions on assets for the Atlan action to work. For example, you can add the following permissions: Asset, such as a table - Read only Any downstream connections, such as Microsoft Power BI - Read only Asset, such as a table - Read only Any downstream connections, such as Microsoft Power BI - Read only You will need to configure the default GITHUB_TOKEN permissions. Grant Read and write permissions to the GITHUB_TOKEN in your repository to allow the atlan-action to seamlessly add or update comments on pull requests. Refer to GitHub documentation to learn more. Configure the action To set up the Atlan action in GitHub: Create repository secrets in your repository: ATLAN_INSTANCE_URL with the URL of your Atlan instance. ATLAN_API_TOKEN with the value of the API token. ATLAN_INSTANCE_URL with the URL of your Atlan instance. ATLAN_API_TOKEN with the value of the API token. Add the GitHub Action to your workflow: Create a workflow file in your repository - .github\/workflows\/atlan-action.yml . Add the following code to your workflow file: name : Atlan action on : pull_request : types : [ opened , edited , synchronize , reopened , closed ] jobs : get-downstream-impact : name : Get Downstream Assets runs-on : ubuntu - latest steps : - name : Checkout uses : actions\/checkout@v4 - name : Run Action uses : atlanhq\/atlan - action@v1 with : GITHUB_TOKEN : $ { { secrets.GITHUB_TOKEN } } ATLAN_INSTANCE_URL : $ { { secrets.ATLAN_INSTANCE_URL } } ATLAN_API_TOKEN : $ { { secrets.ATLAN_API_TOKEN } } ATLAN_CONFIG : .atlan\/config.yaml Create a workflow file in your repository - .github\/workflows\/atlan-action.yml . Create a workflow file in your repository - .github\/workflows\/atlan-action.yml . Add the following code to your workflow file: name : Atlan action on : pull_request : types : [ opened , edited , synchronize , reopened , closed ] jobs : get-downstream-impact : name : Get Downstream Assets runs-on : ubuntu - latest steps : - name : Checkout uses : actions\/checkout@v4 - name : Run Action uses : atlanhq\/atlan - action@v1 with : GITHUB_TOKEN : $ { { secrets.GITHUB_TOKEN } } ATLAN_INSTANCE_URL : $ { { secrets.ATLAN_INSTANCE_URL } } ATLAN_API_TOKEN : $ { { secrets.ATLAN_API_TOKEN } } ATLAN_CONFIG : .atlan\/config.yaml Add the following code to your workflow file: Test the action After you've completed the configuration above, create a pull request with a changed Atlan data contract file to test the action. You should see the Atlan GitHub action running and then adding comments in your pull request: The GitHub workflow will add and update a single comment for every file change. The impacted assets in the comment will be displayed in a collapsible section and grouped by source and asset type. View impacted assets directly in Atlan. Configure the action Test the action"}
{"url":"https:\/\/docs.atlan.com\/product\/integrations#__docusaurus_skipToContent_fallback","title":"Integrations | Atlan Documentation","text":"Atlan integrates with a wide range of tools to help you automate workflows, connect with your favorite apps, and manage identity and access. These integrations connect your data catalog with the tools your teams already use, creating a seamless data experience across your tech stack. Key concepts Integration categories : Atlan offers integrations across five categories: project management, communication, collaboration, automation, and identity management. Connection methods : Most integrations use secure authentication methods like OAuth, API keys, or service accounts. Bi-directional sync : Updates flow between Atlan and integrated tools, ensuring data consistency across platforms. Custom webhooks : Extend Atlan's capabilities by building custom integrations using the provided APIs and webhooks. Core offerings Connect with platforms like AWS Lambda to automate data workflows and streamline routine tasks. Integrate with tools like Slack and Microsoft Teams to enhance team collaboration and knowledge sharing. Connect with SMTP for real-time alerts. Integrate with identity providers like Okta and Azure AD for seamless authentication and user management. Connect with tools like Jira and Service Now to link data assets to projects and track data-related tasks. Get started Select an integration Choose from Atlan's available integrations based on your team's tools and workflows. Follow the integration-specific setup guide to establish a secure connection with your tool. Test and activate Verify the integration is working correctly with a test action, then activate for your organization. Need a custom integration? Atlan provides APIs and webhooks that let you build custom integrations with any tool in your tech stack."}
{"url":"https:\/\/docs.atlan.com\/product\/integrations#key-concepts","title":"Integrations | Atlan Documentation","text":"Atlan integrates with a wide range of tools to help you automate workflows, connect with your favorite apps, and manage identity and access. These integrations connect your data catalog with the tools your teams already use, creating a seamless data experience across your tech stack. Key concepts Integration categories : Atlan offers integrations across five categories: project management, communication, collaboration, automation, and identity management. Connection methods : Most integrations use secure authentication methods like OAuth, API keys, or service accounts. Bi-directional sync : Updates flow between Atlan and integrated tools, ensuring data consistency across platforms. Custom webhooks : Extend Atlan's capabilities by building custom integrations using the provided APIs and webhooks. Core offerings Connect with platforms like AWS Lambda to automate data workflows and streamline routine tasks. Integrate with tools like Slack and Microsoft Teams to enhance team collaboration and knowledge sharing. Connect with SMTP for real-time alerts. Integrate with identity providers like Okta and Azure AD for seamless authentication and user management. Connect with tools like Jira and Service Now to link data assets to projects and track data-related tasks. Get started Select an integration Choose from Atlan's available integrations based on your team's tools and workflows. Follow the integration-specific setup guide to establish a secure connection with your tool. Test and activate Verify the integration is working correctly with a test action, then activate for your organization. Need a custom integration? Atlan provides APIs and webhooks that let you build custom integrations with any tool in your tech stack."}
{"url":"https:\/\/docs.atlan.com\/product\/integrations#core-offerings","title":"Integrations | Atlan Documentation","text":"Atlan integrates with a wide range of tools to help you automate workflows, connect with your favorite apps, and manage identity and access. These integrations connect your data catalog with the tools your teams already use, creating a seamless data experience across your tech stack. Key concepts Integration categories : Atlan offers integrations across five categories: project management, communication, collaboration, automation, and identity management. Connection methods : Most integrations use secure authentication methods like OAuth, API keys, or service accounts. Bi-directional sync : Updates flow between Atlan and integrated tools, ensuring data consistency across platforms. Custom webhooks : Extend Atlan's capabilities by building custom integrations using the provided APIs and webhooks. Core offerings Connect with platforms like AWS Lambda to automate data workflows and streamline routine tasks. Integrate with tools like Slack and Microsoft Teams to enhance team collaboration and knowledge sharing. Connect with SMTP for real-time alerts. Integrate with identity providers like Okta and Azure AD for seamless authentication and user management. Connect with tools like Jira and Service Now to link data assets to projects and track data-related tasks. Get started Select an integration Choose from Atlan's available integrations based on your team's tools and workflows. Follow the integration-specific setup guide to establish a secure connection with your tool. Test and activate Verify the integration is working correctly with a test action, then activate for your organization. Need a custom integration? Atlan provides APIs and webhooks that let you build custom integrations with any tool in your tech stack."}
{"url":"https:\/\/docs.atlan.com\/product\/integrations#get-started","title":"Integrations | Atlan Documentation","text":"Atlan integrates with a wide range of tools to help you automate workflows, connect with your favorite apps, and manage identity and access. These integrations connect your data catalog with the tools your teams already use, creating a seamless data experience across your tech stack. Key concepts Integration categories : Atlan offers integrations across five categories: project management, communication, collaboration, automation, and identity management. Connection methods : Most integrations use secure authentication methods like OAuth, API keys, or service accounts. Bi-directional sync : Updates flow between Atlan and integrated tools, ensuring data consistency across platforms. Custom webhooks : Extend Atlan's capabilities by building custom integrations using the provided APIs and webhooks. Core offerings Connect with platforms like AWS Lambda to automate data workflows and streamline routine tasks. Integrate with tools like Slack and Microsoft Teams to enhance team collaboration and knowledge sharing. Connect with SMTP for real-time alerts. Integrate with identity providers like Okta and Azure AD for seamless authentication and user management. Connect with tools like Jira and Service Now to link data assets to projects and track data-related tasks. Get started Select an integration Choose from Atlan's available integrations based on your team's tools and workflows. Follow the integration-specific setup guide to establish a secure connection with your tool. Test and activate Verify the integration is working correctly with a test action, then activate for your organization. Need a custom integration? Atlan provides APIs and webhooks that let you build custom integrations with any tool in your tech stack."}
{"url":"https:\/\/developer.atlan.com\/getting-started\/#an-introductory-walkthrough","title":"Introductory walkthrough - Developer","text":"An introductory walkthrough You might also like our Atlan Platform Essentials certification . Not sure where to start? Allow us to introduce Atlan development through example. 1 We strongly recommend using one of our SDKs to simplify the development process. As a first step, set one up: The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include logback as a simple binding mechanism to send any logging information out to your console (standard out). Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on PyPI . You can use pip to install it as follows: Provide two values to create an Atlan client: Provide your Atlan tenant URL to the base_url parameter. (You can also do this through environment variables .) Provide your API token to the api_key parameter. (You can also do this through environment variables .) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include slf4j-simple as a simple binding mechanism to send any logging information out to your console (standard out), along with the kotlin-logging-jvm microutil. Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on GitHub , ready to be included in your project: Provide two values to set up connectivity to Atlan: Provide your Atlan tenant URL to the assets.Context() method. If you prefer using the value from an environment variable, you can use assets.NewContext() without any parameters. Provide your API token as the second parameter to the assets.Context() method. (Or again, have it picked up automatically by the assets.NewContext() method.) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. Don't forget to give permissions If you want to be able to access existing metadata with an API token, don't forget that you need to assign one or more personas to the API token that grant it access to metadata. Now that you have an SDK installed and configured, you are ready to code! Before we jump straight to code, though, let's first introduce some key concepts in Atlan: What is an asset? In Atlan, we refer to all objects that provide context to your data as assets . Each type of asset in Atlan has a set of: Properties , such as: Certificates Announcements Properties , such as: Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table In an object-oriented programming sense, think of an asset as an instance of a class. The structure of an asset (the class itself, in this analogy) is defined by something called a type definition , but that's for another day. So as you can see: There are many different kinds of assets: tables, columns, schemas, databases, business intelligence dashboards, reports, and so on. Assets inter-relate with each other: a table has a parent schema and child columns, a schema has a parent database and child tables, and so on. Different kinds of assets have some common properties (like certificates) and other properties that are unique to that kind of asset (like a columnCount that only exists on tables, not on schemas or databases). When you know the asset When you already know which asset you want to retrieve, you can read it from Atlan using one of its identifiers . We'll discuss these in more detail as part of updates, but for now you can think of them as: is a primary key for an asset: completely unique, but meaningless by itself is a business key for an asset: unique for a given kind of asset, and interpretable You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the asset.get_by_guid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the asset.get_by_qualified_name() method on the Atlan client, providing the type of asset you expect to retrieve and its qualified_name . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the assets.GetByGuid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the assets.GetByQualifiedName() method on the Atlan client, providing the type of asset you expect to retrieve and its qualifiedName . (Each asset type is its own unique class in the SDK.) Note that the response is strongly typed: If you are retrieving a table, you will get a table back (as long as it exists). You do not need to figure out what properties or relationships exist on a table - the Table class defines them for for you already. In any modern IDE, this means you have type-ahead support for retrieving the properties and relationships from the table variable. You can also refer to the types reference in this portal for full details of every kind of asset. Retrieval by identifier can be more costly than you might expect Even though you are retrieving an asset by an identifier, this can be more costly than you might expect. Retrieving an asset in this way will: Retrieve all its properties and their values Retrieve all its relationships Imagine the asset you are retrieving has 100's or 1000's of these. If you only care about its certificate and any owners, you will be retrieving far more information than you need. When you need to find it first For example, imagine you want to find all tables named MY_TABLE : You can then run the request using Execute() . For example, if you want to know the certificate of the asset you only need to tack that onto the query: Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many include_on_results calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can include as many attributes in IncludeOnResults as you want to specify the properties and relationships you want to retrieve for matching assets. Also gives the best performance If all you want to do is check or report on metadata, you should have a starting point from the information above. Or, now that you've found an asset of interest, maybe you want to update the asset with additional metadata ? Once again, before we jump to code, let's first understand some key concepts about how Atlan handles updates: Importance of identifiers Most operations on assets are upserts , that is, they could either create (insert) or update a given asset. How do you know which is going to happen? To answer this question, you need to understand how Atlan uniquely identifies each asset. Recall earlier we discussed asset's different identifiers in Atlan . Every asset in Atlan has at least the following two unique identifiers. These are both mandatory for every asset, so no asset can exist without these: Atlan uses globally-unique identifiers (GUIDs) to uniquely identify each asset, globally . They look something like this: As the name implies, GUIDs are: Globally unique (across all systems). Generated in a way that makes it nearly impossible for anything else to ever generate that same ID. 2 Note that this means the GUID itself is not : Meaningful or capable of being interpreted in any way Atlan uses qualifiedName s to uniquely identify assets based on their characteristics. They look something like this: Qualified names are not : Globally unique (across all systems). Instead, they are: Consistently constructed in a meaningful way, making it possible for them to be reconstructed. Note that this means the qualifiedName is: Meaningful and capable of being interpreted How these impact updates Since they are truly unique, operations that include a GUID will only update an asset, not create one. Conversely, operations that take a qualifiedName can: Create an asset, if no exactly-matching qualifiedName is found in Atlan. Update an asset, if an exact-match for the qualifiedName is found in Atlan. These operations also require a typeName , so that if creation does occur the correct type of asset is created. Unintended consequences of this behavior Be careful when using operations with only the qualifiedName . You may end up creating assets when you were only expecting them to be updated or to fail if they did not already exist. This is particularly true when you do not give the exact, case-sensitive qualifiedName of an asset. a\/b\/c\/d is not the same as a\/B\/c\/d when it comes to qualifiedName s. Perhaps this leaves you wondering: why have a qualifiedName at all? The qualifiedName 's purpose is to identify what is a unique asset. Many different tools might all have information about that asset. Having a common \"identity\" means that many different systems can each independently construct its identifier the same way. If a crawler gets table details from Snowflake it can upsert based on those identity characteristics in Atlan. The crawler will not create duplicate tables every time it runs. This gives idempotency. Looker knows the same identity characteristics for the Snowflake tables and columns. So if you get details from Looker about the tables it uses for reporting, you can link them together in lineage. (Looker can construct the same identifier for the table as Snowflake itself.) These characteristics are not possible using GUIDs alone. Limit to changes only Now that you understand the nuances of identifiers, let's look at how you can update metadata in Atlan. In general, you only need to send changes to Atlan. You do not need to send an entire asset each time you want to make changes to it. For example, imagine you want to mark a table as certified but do not want to change anything else (its name, description, owner details, and so on): You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualified_name . Using the updater() class method on any asset type, you pass in (typically) the qualified_name and name of the asset. You can then add onto the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to client.asset.save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the Updater() method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns an object into which you can then place any updates. You can place into the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to .Save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. Atlan will handle idempotency By sending only the changes you want to apply, Atlan can make idempotent updates. Atlan will only attempt to update the asset with the changes you send. Atlan leaves any existing metadata on the asset as-is. If the asset already has the metadata values you are sending, Atlan does nothing. It will not even update audit details like the last update timestamp, and is thus idempotent. What if you want to make changes to many assets, as efficiently as possible? Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Where to go from here Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Real code samples our customers use to solve particular use cases. Review live samples Events Delve deep into the details of the events Atlan triggers. Learn more about events Delve deep into the details of the events Atlan triggers. Learn more about events Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†©"}
{"url":"https:\/\/developer.atlan.com\/getting-started\/#fn%3A1","title":"Introductory walkthrough - Developer","text":"An introductory walkthrough You might also like our Atlan Platform Essentials certification . Not sure where to start? Allow us to introduce Atlan development through example. 1 We strongly recommend using one of our SDKs to simplify the development process. As a first step, set one up: The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include logback as a simple binding mechanism to send any logging information out to your console (standard out). Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on PyPI . You can use pip to install it as follows: Provide two values to create an Atlan client: Provide your Atlan tenant URL to the base_url parameter. (You can also do this through environment variables .) Provide your API token to the api_key parameter. (You can also do this through environment variables .) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include slf4j-simple as a simple binding mechanism to send any logging information out to your console (standard out), along with the kotlin-logging-jvm microutil. Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on GitHub , ready to be included in your project: Provide two values to set up connectivity to Atlan: Provide your Atlan tenant URL to the assets.Context() method. If you prefer using the value from an environment variable, you can use assets.NewContext() without any parameters. Provide your API token as the second parameter to the assets.Context() method. (Or again, have it picked up automatically by the assets.NewContext() method.) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. Don't forget to give permissions If you want to be able to access existing metadata with an API token, don't forget that you need to assign one or more personas to the API token that grant it access to metadata. Now that you have an SDK installed and configured, you are ready to code! Before we jump straight to code, though, let's first introduce some key concepts in Atlan: What is an asset? In Atlan, we refer to all objects that provide context to your data as assets . Each type of asset in Atlan has a set of: Properties , such as: Certificates Announcements Properties , such as: Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table In an object-oriented programming sense, think of an asset as an instance of a class. The structure of an asset (the class itself, in this analogy) is defined by something called a type definition , but that's for another day. So as you can see: There are many different kinds of assets: tables, columns, schemas, databases, business intelligence dashboards, reports, and so on. Assets inter-relate with each other: a table has a parent schema and child columns, a schema has a parent database and child tables, and so on. Different kinds of assets have some common properties (like certificates) and other properties that are unique to that kind of asset (like a columnCount that only exists on tables, not on schemas or databases). When you know the asset When you already know which asset you want to retrieve, you can read it from Atlan using one of its identifiers . We'll discuss these in more detail as part of updates, but for now you can think of them as: is a primary key for an asset: completely unique, but meaningless by itself is a business key for an asset: unique for a given kind of asset, and interpretable You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the asset.get_by_guid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the asset.get_by_qualified_name() method on the Atlan client, providing the type of asset you expect to retrieve and its qualified_name . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the assets.GetByGuid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the assets.GetByQualifiedName() method on the Atlan client, providing the type of asset you expect to retrieve and its qualifiedName . (Each asset type is its own unique class in the SDK.) Note that the response is strongly typed: If you are retrieving a table, you will get a table back (as long as it exists). You do not need to figure out what properties or relationships exist on a table - the Table class defines them for for you already. In any modern IDE, this means you have type-ahead support for retrieving the properties and relationships from the table variable. You can also refer to the types reference in this portal for full details of every kind of asset. Retrieval by identifier can be more costly than you might expect Even though you are retrieving an asset by an identifier, this can be more costly than you might expect. Retrieving an asset in this way will: Retrieve all its properties and their values Retrieve all its relationships Imagine the asset you are retrieving has 100's or 1000's of these. If you only care about its certificate and any owners, you will be retrieving far more information than you need. When you need to find it first For example, imagine you want to find all tables named MY_TABLE : You can then run the request using Execute() . For example, if you want to know the certificate of the asset you only need to tack that onto the query: Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many include_on_results calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can include as many attributes in IncludeOnResults as you want to specify the properties and relationships you want to retrieve for matching assets. Also gives the best performance If all you want to do is check or report on metadata, you should have a starting point from the information above. Or, now that you've found an asset of interest, maybe you want to update the asset with additional metadata ? Once again, before we jump to code, let's first understand some key concepts about how Atlan handles updates: Importance of identifiers Most operations on assets are upserts , that is, they could either create (insert) or update a given asset. How do you know which is going to happen? To answer this question, you need to understand how Atlan uniquely identifies each asset. Recall earlier we discussed asset's different identifiers in Atlan . Every asset in Atlan has at least the following two unique identifiers. These are both mandatory for every asset, so no asset can exist without these: Atlan uses globally-unique identifiers (GUIDs) to uniquely identify each asset, globally . They look something like this: As the name implies, GUIDs are: Globally unique (across all systems). Generated in a way that makes it nearly impossible for anything else to ever generate that same ID. 2 Note that this means the GUID itself is not : Meaningful or capable of being interpreted in any way Atlan uses qualifiedName s to uniquely identify assets based on their characteristics. They look something like this: Qualified names are not : Globally unique (across all systems). Instead, they are: Consistently constructed in a meaningful way, making it possible for them to be reconstructed. Note that this means the qualifiedName is: Meaningful and capable of being interpreted How these impact updates Since they are truly unique, operations that include a GUID will only update an asset, not create one. Conversely, operations that take a qualifiedName can: Create an asset, if no exactly-matching qualifiedName is found in Atlan. Update an asset, if an exact-match for the qualifiedName is found in Atlan. These operations also require a typeName , so that if creation does occur the correct type of asset is created. Unintended consequences of this behavior Be careful when using operations with only the qualifiedName . You may end up creating assets when you were only expecting them to be updated or to fail if they did not already exist. This is particularly true when you do not give the exact, case-sensitive qualifiedName of an asset. a\/b\/c\/d is not the same as a\/B\/c\/d when it comes to qualifiedName s. Perhaps this leaves you wondering: why have a qualifiedName at all? The qualifiedName 's purpose is to identify what is a unique asset. Many different tools might all have information about that asset. Having a common \"identity\" means that many different systems can each independently construct its identifier the same way. If a crawler gets table details from Snowflake it can upsert based on those identity characteristics in Atlan. The crawler will not create duplicate tables every time it runs. This gives idempotency. Looker knows the same identity characteristics for the Snowflake tables and columns. So if you get details from Looker about the tables it uses for reporting, you can link them together in lineage. (Looker can construct the same identifier for the table as Snowflake itself.) These characteristics are not possible using GUIDs alone. Limit to changes only Now that you understand the nuances of identifiers, let's look at how you can update metadata in Atlan. In general, you only need to send changes to Atlan. You do not need to send an entire asset each time you want to make changes to it. For example, imagine you want to mark a table as certified but do not want to change anything else (its name, description, owner details, and so on): You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualified_name . Using the updater() class method on any asset type, you pass in (typically) the qualified_name and name of the asset. You can then add onto the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to client.asset.save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the Updater() method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns an object into which you can then place any updates. You can place into the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to .Save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. Atlan will handle idempotency By sending only the changes you want to apply, Atlan can make idempotent updates. Atlan will only attempt to update the asset with the changes you send. Atlan leaves any existing metadata on the asset as-is. If the asset already has the metadata values you are sending, Atlan does nothing. It will not even update audit details like the last update timestamp, and is thus idempotent. What if you want to make changes to many assets, as efficiently as possible? Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Where to go from here Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Real code samples our customers use to solve particular use cases. Review live samples Events Delve deep into the details of the events Atlan triggers. Learn more about events Delve deep into the details of the events Atlan triggers. Learn more about events Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†©"}
{"url":"https:\/\/developer.atlan.com\/getting-started\/#setting-up","title":"Introductory walkthrough - Developer","text":"An introductory walkthrough You might also like our Atlan Platform Essentials certification . Not sure where to start? Allow us to introduce Atlan development through example. 1 We strongly recommend using one of our SDKs to simplify the development process. As a first step, set one up: The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include logback as a simple binding mechanism to send any logging information out to your console (standard out). Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on PyPI . You can use pip to install it as follows: Provide two values to create an Atlan client: Provide your Atlan tenant URL to the base_url parameter. (You can also do this through environment variables .) Provide your API token to the api_key parameter. (You can also do this through environment variables .) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include slf4j-simple as a simple binding mechanism to send any logging information out to your console (standard out), along with the kotlin-logging-jvm microutil. Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on GitHub , ready to be included in your project: Provide two values to set up connectivity to Atlan: Provide your Atlan tenant URL to the assets.Context() method. If you prefer using the value from an environment variable, you can use assets.NewContext() without any parameters. Provide your API token as the second parameter to the assets.Context() method. (Or again, have it picked up automatically by the assets.NewContext() method.) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. Don't forget to give permissions If you want to be able to access existing metadata with an API token, don't forget that you need to assign one or more personas to the API token that grant it access to metadata. Now that you have an SDK installed and configured, you are ready to code! Before we jump straight to code, though, let's first introduce some key concepts in Atlan: What is an asset? In Atlan, we refer to all objects that provide context to your data as assets . Each type of asset in Atlan has a set of: Properties , such as: Certificates Announcements Properties , such as: Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table In an object-oriented programming sense, think of an asset as an instance of a class. The structure of an asset (the class itself, in this analogy) is defined by something called a type definition , but that's for another day. So as you can see: There are many different kinds of assets: tables, columns, schemas, databases, business intelligence dashboards, reports, and so on. Assets inter-relate with each other: a table has a parent schema and child columns, a schema has a parent database and child tables, and so on. Different kinds of assets have some common properties (like certificates) and other properties that are unique to that kind of asset (like a columnCount that only exists on tables, not on schemas or databases). When you know the asset When you already know which asset you want to retrieve, you can read it from Atlan using one of its identifiers . We'll discuss these in more detail as part of updates, but for now you can think of them as: is a primary key for an asset: completely unique, but meaningless by itself is a business key for an asset: unique for a given kind of asset, and interpretable You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the asset.get_by_guid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the asset.get_by_qualified_name() method on the Atlan client, providing the type of asset you expect to retrieve and its qualified_name . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the assets.GetByGuid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the assets.GetByQualifiedName() method on the Atlan client, providing the type of asset you expect to retrieve and its qualifiedName . (Each asset type is its own unique class in the SDK.) Note that the response is strongly typed: If you are retrieving a table, you will get a table back (as long as it exists). You do not need to figure out what properties or relationships exist on a table - the Table class defines them for for you already. In any modern IDE, this means you have type-ahead support for retrieving the properties and relationships from the table variable. You can also refer to the types reference in this portal for full details of every kind of asset. Retrieval by identifier can be more costly than you might expect Even though you are retrieving an asset by an identifier, this can be more costly than you might expect. Retrieving an asset in this way will: Retrieve all its properties and their values Retrieve all its relationships Imagine the asset you are retrieving has 100's or 1000's of these. If you only care about its certificate and any owners, you will be retrieving far more information than you need. When you need to find it first For example, imagine you want to find all tables named MY_TABLE : You can then run the request using Execute() . For example, if you want to know the certificate of the asset you only need to tack that onto the query: Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many include_on_results calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can include as many attributes in IncludeOnResults as you want to specify the properties and relationships you want to retrieve for matching assets. Also gives the best performance If all you want to do is check or report on metadata, you should have a starting point from the information above. Or, now that you've found an asset of interest, maybe you want to update the asset with additional metadata ? Once again, before we jump to code, let's first understand some key concepts about how Atlan handles updates: Importance of identifiers Most operations on assets are upserts , that is, they could either create (insert) or update a given asset. How do you know which is going to happen? To answer this question, you need to understand how Atlan uniquely identifies each asset. Recall earlier we discussed asset's different identifiers in Atlan . Every asset in Atlan has at least the following two unique identifiers. These are both mandatory for every asset, so no asset can exist without these: Atlan uses globally-unique identifiers (GUIDs) to uniquely identify each asset, globally . They look something like this: As the name implies, GUIDs are: Globally unique (across all systems). Generated in a way that makes it nearly impossible for anything else to ever generate that same ID. 2 Note that this means the GUID itself is not : Meaningful or capable of being interpreted in any way Atlan uses qualifiedName s to uniquely identify assets based on their characteristics. They look something like this: Qualified names are not : Globally unique (across all systems). Instead, they are: Consistently constructed in a meaningful way, making it possible for them to be reconstructed. Note that this means the qualifiedName is: Meaningful and capable of being interpreted How these impact updates Since they are truly unique, operations that include a GUID will only update an asset, not create one. Conversely, operations that take a qualifiedName can: Create an asset, if no exactly-matching qualifiedName is found in Atlan. Update an asset, if an exact-match for the qualifiedName is found in Atlan. These operations also require a typeName , so that if creation does occur the correct type of asset is created. Unintended consequences of this behavior Be careful when using operations with only the qualifiedName . You may end up creating assets when you were only expecting them to be updated or to fail if they did not already exist. This is particularly true when you do not give the exact, case-sensitive qualifiedName of an asset. a\/b\/c\/d is not the same as a\/B\/c\/d when it comes to qualifiedName s. Perhaps this leaves you wondering: why have a qualifiedName at all? The qualifiedName 's purpose is to identify what is a unique asset. Many different tools might all have information about that asset. Having a common \"identity\" means that many different systems can each independently construct its identifier the same way. If a crawler gets table details from Snowflake it can upsert based on those identity characteristics in Atlan. The crawler will not create duplicate tables every time it runs. This gives idempotency. Looker knows the same identity characteristics for the Snowflake tables and columns. So if you get details from Looker about the tables it uses for reporting, you can link them together in lineage. (Looker can construct the same identifier for the table as Snowflake itself.) These characteristics are not possible using GUIDs alone. Limit to changes only Now that you understand the nuances of identifiers, let's look at how you can update metadata in Atlan. In general, you only need to send changes to Atlan. You do not need to send an entire asset each time you want to make changes to it. For example, imagine you want to mark a table as certified but do not want to change anything else (its name, description, owner details, and so on): You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualified_name . Using the updater() class method on any asset type, you pass in (typically) the qualified_name and name of the asset. You can then add onto the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to client.asset.save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the Updater() method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns an object into which you can then place any updates. You can place into the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to .Save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. Atlan will handle idempotency By sending only the changes you want to apply, Atlan can make idempotent updates. Atlan will only attempt to update the asset with the changes you send. Atlan leaves any existing metadata on the asset as-is. If the asset already has the metadata values you are sending, Atlan does nothing. It will not even update audit details like the last update timestamp, and is thus idempotent. What if you want to make changes to many assets, as efficiently as possible? Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Where to go from here Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Real code samples our customers use to solve particular use cases. Review live samples Events Delve deep into the details of the events Atlan triggers. Learn more about events Delve deep into the details of the events Atlan triggers. Learn more about events Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†©"}
{"url":"https:\/\/developer.atlan.com\/sdks\/java\/#logging","title":"Java SDK - Developer","text":"Walk through step-by-step in our intro to custom integration course (30 mins). Obtain the SDK The SDK is available on Maven Central, ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include logback as a simple binding mechanism to send any logging information out to your console (standard out), at INFO -level or above. Configure the SDK There are two ways to configure the SDK: Using environment variables ATLAN_API_KEY should be given your Atlan API token , for authentication ( don't forget to assign one or more personas to the API token to give access to existing assets! ) ATLAN_BASE_URL should be given your Atlan URL (for example, https:\/\/tenant.atlan.com ) Here's an example of setting those environment variables: On client creation If you prefer to not use environment variables, you can do the following: Careful not to expose your API token! We generally discourage including your API token directly in your code, in case you accidentally commit it into a (public) version control system. But it's your choice exactly how you manage the API token and including it for use within the client. (Note that you can also explicity provide only the tenant URL, and the constructor will look for only the API key through an environment variable.) That's it â€” once these are set you can start using your SDK to make live calls against your Atlan instance! ðŸŽ‰ Delve into more detailed examples: Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Real code samples our customers use to solve particular use cases. Review live samples Events Delve deep into the details of the events Atlan triggers. Learn more about events Delve deep into the details of the events Atlan triggers. Learn more about events The SDK defines checked exceptions for the following categories of error: A given API call could fail due to all of the errors above. So these all extend a generic AtlanException checked exception, and all API operations throw AtlanException . For example, when creating a connection there is an asynchronous process that grants permissions to the admins of that connection. So there can be a slight delay between creating the connection and being permitted to do any operations with the connection. During that delay, any attempt to interact with the connection will result in a PermissionException , even if your API token was used to create connection in the first place. Another example you may occasionally hit is some network issue that causes your connection to Atlan to be interrupted. In these cases, an ApiConnectionException will be raised. Don't worry, the SDK retries automatically While these are useful to know for detecting issues, the SDK automatically retries on such problems. Atlan is a distributed, cloud-native application, where network problems can arise. These advanced configuration options allow you to optimize how the SDK handles such ephemeral problems. The SDK uses slf4j to be logging framework-agnostic. You can therefore configure your own preferred logging framework: Replace the ch.qos.logback:logback-classic:1.2.11 logback binding with log4j2 bindings. The SDK handles automatically retrying your requests when it detects certain problems: When an ApiConnectionException occurs that is caused by an underlying ConnectException or SocketTimeoutException . When there is a 403 response indicating that permission for an operation is not (yet) available. When there is a 500 response indicating that something went wrong on the server side. If any request encounters one of these problems, it will be retried. Before each retry, the SDK will apply a delay using: An exponential backoff (starting from 500ms) A jitter (in the range of 75-100% of the backoff delay) Each retry will be at least 500ms, and at most 5s. (Currently these values are not configurable.) For each request that encounters any of these problems, only up to a maximum number of retries will be attempted. (This is set to 3 by default.) You can configure the maximum number of retries globally using setMaxNetworkRetries() on a client. Set this to an integer: The SDK will only wait so long for a response before assuming a network problem has occurred and the request should be timed out. By default, this is set to 80 seconds. You can configure the maximum time the SDK will wait before timing out a request using setReadTimeout() on a client. Set this to an integer giving the number of milliseconds before timing out: Remember this must be given in milliseconds. This example sets the timeout to 2 minutes (120 seconds * 1000 milliseconds). Since version 0.9.0, the Java SDK supports connecting to multiple tenants. From version 4.0.0 onwards you can create any number of clients against any number of different tenants, since every operation that interacts with a tenant now explicitly requires a client to be provided to it: Constructing a new client with a different tenant's URL is sufficient to create connectivity to that other tenant. You can also (optionally) provide a second argument to directly give the API token for the tenant. Create an object as usual. You can access the operations for assets directly on the client, under client.assets . These will generally give you the most flexibility â€” they can handle multiple objects at a time and allow overrides. Every operation on the client itself has a variant with an (optional) final argument through which you can override settings like retry limits or timeouts for this single request. You can use the from(client) factory method to initialize the request options with all the settings of your client, and then you only need to chain on those you want to override for this particular request. Alternatively, you can pass the client to the operation on the object itself. Limit the number of clients to those you must have Each client you create maintains its own independent copy of various caches. So the more clients you have, the more resources your code will consume. For this reason, we recommended limiting the number of clients you create to the bare minimum you require â€” ideally just a single client per tenant. Using a proxy To use the Java SDK with a proxy, you need to send in some additional parameters when running any java . command. These are described in detail in the Java documentation , but are summarized here for simplicity: socksProxyHost should be set to the hostname for your SOCKS proxy socksProxyPort should be set to the port for your SOCKS proxy (default being 1080) Providing credentials to the proxy In either case, if you need to authenticate to your proxy, you will need to wrap whatever code you want to run to set up these credentials using something like the following: You need to create a built-in Java PasswordAuthentication object. Provide your username as the first argument. . and your password as the second argument, as a char[] . (Of course, you should not hard-code your password in your code itself, but rather pull it from elsewhere.) Then use setProxyCredential() to pass this PasswordAuthentication object to the Atlan client, before any of the rest of the code will execute."}
{"url":"https:\/\/developer.atlan.com\/sdks\/python\/#using-environment-variables","title":"Python SDK - Developer","text":"Walk through step-by-step in our intro to custom integration course (30 mins). Obtain the SDK The SDK is currently available on pypi . You can use pip to install it as follows: Configure the SDK There are two ways to configure the SDK: Using environment variables ATLAN_API_KEY should be given your Atlan API token , for authentication ( don't forget to assign one or more personas to the API token to give access to existing assets! ) ATLAN_BASE_URL should be given your Atlan URL (for example, https:\/\/tenant.atlan.com ) Here's an example of setting those environment variables: On client creation If you prefer to not use environment variables, you can do the following: Careful not to expose your API token! We generally discourage including your API token directly in your code, in case you accidentally commit it into a (public) version control system. But it's your choice exactly how you manage the API token and including it for use within the client. In some scenarios, you may not want to expose the entire API token or manage environment variables. Instead, you can provide the GUID of the API token, and the SDK will internally fetch the actual access token. When to use this approach: Building apps that use the SDK where token security is a concern When you want to avoid exposing full API tokens in your configuration For containerized applications that need secure token management Before using this approach, ensure your Argo template is configured with CLIENT_ID and CLIENT_SECRET : Create client from token GUID : Use AtlanClient.from_token_guid() to create a client using the GUID of an API token. The SDK will automatically fetch the actual access token using the configured CLIENT_ID and CLIENT_SECRET . That's it â€” once these are set you can start using your SDK to make live calls against your Atlan instance! ðŸŽ‰ Delve into more detailed examples: Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Real code samples our customers use to solve particular use cases. Review live samples Events Delve deep into the details of the events Atlan triggers. Learn more about events Delve deep into the details of the events Atlan triggers. Learn more about events The SDK defines exceptions for the following categories of error: A given API call could fail due to all of the errors above. So these all extend a generic AtlanError exception, and all API operations can potentially raise AtlanError . For example, when creating a connection there is an asynchronous process that grants permissions to the admins of that connection. So there can be a slight delay between creating the connection and being permitted to do any operations with the connection. During that delay, any attempt to interact with the connection will result in a PermissionError , even if your API token was used to create connection in the first place. Another example you may occasionally hit is some network issue that causes your connection to Atlan to be interrupted. In these cases, an ApiConnectionError will be raised. Don't worry, the SDK retries automatically While these are useful to know for detecting issues, the SDK automatically retries on such problems. Atlan is a distributed, cloud-native application, where network problems can arise. The SDK therefore automatically attempts to handle ephemeral problems. The SDK uses logging module of the standard library that can provide a flexible framework for emitting log messages. You can enable logging for your SDK script by adding the following lines above your snippets: You can enable logging by using basicConfig with various logging levels: logging.DEBUG : used to give detailed information, typically of interest only when diagnosing problems (mostly used level in SDK). logging.INFO : used to confirm that things are working as expected. logging.WARN : used as an indication that something unexpected happened, or as a warning of some problem in the near future. logging.ERROR : indicates that due to a more serious problem, the SDK has not been able to perform some operation. logging.CRITICAL : indicates a serious error, suggesting that the program itself may be unable to continue running (not used in SDK as of now). You can enable logging by using basicConfig with various logging levels: logging.DEBUG : used to give detailed information, typically of interest only when diagnosing problems (mostly used level in SDK). logging.INFO : used to confirm that things are working as expected. logging.WARN : used as an indication that something unexpected happened, or as a warning of some problem in the near future. logging.ERROR : indicates that due to a more serious problem, the SDK has not been able to perform some operation. logging.CRITICAL : indicates a serious error, suggesting that the program itself may be unable to continue running (not used in SDK as of now). By default, logs will appear in your console. If you want to use file logging, you can add the following line: logging.config.fileConfig('pyatlan\/logging.conf') : this will generate logs according to the configuration defined in pyatlan\/logging.conf and will generate two log files: \/tmp\/pyatlan.log : default log file. \/tmp\/pyatlan.json : log file in JSON format. By default, logs will appear in your console. If you want to use file logging, you can add the following line: logging.config.fileConfig('pyatlan\/logging.conf') : this will generate logs according to the configuration defined in pyatlan\/logging.conf and will generate two log files: \/tmp\/pyatlan.log : default log file. \/tmp\/pyatlan.json : log file in JSON format. logging.config.fileConfig('pyatlan\/logging.conf') : this will generate logs according to the configuration defined in pyatlan\/logging.conf and will generate two log files: \/tmp\/pyatlan.log : default log file. \/tmp\/pyatlan.json : log file in JSON format. The SDK handles automatically retrying your requests when it detects certain problems: When there is a 403 response indicating that permission for an operation is not (yet) available. When there is a 429 response indicating that the request rate limit has been exceeded, and you need to retry after some time. When there is a 50x response indicating that something went wrong on the server side. If any request encounters one of these problems, it will be retried. Before each retry, the SDK will apply a delay using an exponential backoff. (Currently the values for the exponential backoff are not configurable.) For each request that encounters any of these problems, only up to a maximum number of retries will be attempted. (This is set to 5 by default.) By default, the SDK AtlanClient() has the following timeout settings: read_timeout : 900.0 seconds ( 15 minutes) connect_timeout : 30.0 seconds If you need to override these defaults, you can do so as shown in the example below: Since version 1.0.0, the Python SDK supports connecting to multiple tenants.[^1] When you use the AtlanClient() method you are actually setting a default client. This default client will be used behind-the-scenes for any operations that need information specific to an Atlan tenant. When you want to override that default client you can create a new one and use the set_default_client() method to change it: The AtlanClient() method will return a client for the given base URL, creating a new client and setting this new client as the default client. If you want to switch between clients that you have already created, you can use Atlan.set_default_client() to change between them. Limit the number of clients to those you must have Each client you create maintains its own independent copy of various caches. So the more clients you have, the more resources your code will consume. For this reason, we recommended limiting the number of clients you create to the bare minimum you require â€” ideally just a single client per tenant. (And since in the majority of use cases you only need access to a single tenant, this means you can most likely just rely on the default client and the fallback behavior.) Pyatlan uses the Requests library which supports proxy configuration via environment variables. Requests relies on the proxy configuration defined by standard environment variables http_proxy, https_proxy, no_proxy, and all_proxy. Uppercase variants of these variables are also supported. You can therefore set them to configure Pyatlan (only set the ones relevant to your needs): To use HTTP Basic Auth with your proxy, use the http:\/\/user:password@host\/ syntax in any of the above configuration entries: Currently, the way this is implemented limits you to either avoiding multiple threads in your Python code (if you need to use multiple clients), or if you want to use multiple threads you should only use a single client. Asynchronous SDK operations To get started, you need to initialize an AsyncAtlanClient : Create an async client using the same configuration pattern as the synchronous client. Concurrent operations for improved performance The real power of async comes from running multiple operations concurrently . Instead of waiting for each operation to complete sequentially, you can execute them in parallel and reduce total execution time: Synchronous : Total time = operationâ‚ + operationâ‚‚ + . + operationâ‚™ Asynchronous : Total time = max(operationâ‚, operationâ‚‚, . , operationâ‚™) When to use async Async is most beneficial when you have: Multiple independent operations that can run concurrently I\/O-heavy workloads like API calls, database queries, or file operations Long-running operations where parallelization provides significant time savings For simple, single operations, the synchronous client may be more straightforward to use."}
{"url":"https:\/\/developer.atlan.com\/sdks\/python\/#logging","title":"Python SDK - Developer","text":"Walk through step-by-step in our intro to custom integration course (30 mins). Obtain the SDK The SDK is currently available on pypi . You can use pip to install it as follows: Configure the SDK There are two ways to configure the SDK: Using environment variables ATLAN_API_KEY should be given your Atlan API token , for authentication ( don't forget to assign one or more personas to the API token to give access to existing assets! ) ATLAN_BASE_URL should be given your Atlan URL (for example, https:\/\/tenant.atlan.com ) Here's an example of setting those environment variables: On client creation If you prefer to not use environment variables, you can do the following: Careful not to expose your API token! We generally discourage including your API token directly in your code, in case you accidentally commit it into a (public) version control system. But it's your choice exactly how you manage the API token and including it for use within the client. In some scenarios, you may not want to expose the entire API token or manage environment variables. Instead, you can provide the GUID of the API token, and the SDK will internally fetch the actual access token. When to use this approach: Building apps that use the SDK where token security is a concern When you want to avoid exposing full API tokens in your configuration For containerized applications that need secure token management Before using this approach, ensure your Argo template is configured with CLIENT_ID and CLIENT_SECRET : Create client from token GUID : Use AtlanClient.from_token_guid() to create a client using the GUID of an API token. The SDK will automatically fetch the actual access token using the configured CLIENT_ID and CLIENT_SECRET . That's it â€” once these are set you can start using your SDK to make live calls against your Atlan instance! ðŸŽ‰ Delve into more detailed examples: Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Real code samples our customers use to solve particular use cases. Review live samples Events Delve deep into the details of the events Atlan triggers. Learn more about events Delve deep into the details of the events Atlan triggers. Learn more about events The SDK defines exceptions for the following categories of error: A given API call could fail due to all of the errors above. So these all extend a generic AtlanError exception, and all API operations can potentially raise AtlanError . For example, when creating a connection there is an asynchronous process that grants permissions to the admins of that connection. So there can be a slight delay between creating the connection and being permitted to do any operations with the connection. During that delay, any attempt to interact with the connection will result in a PermissionError , even if your API token was used to create connection in the first place. Another example you may occasionally hit is some network issue that causes your connection to Atlan to be interrupted. In these cases, an ApiConnectionError will be raised. Don't worry, the SDK retries automatically While these are useful to know for detecting issues, the SDK automatically retries on such problems. Atlan is a distributed, cloud-native application, where network problems can arise. The SDK therefore automatically attempts to handle ephemeral problems. The SDK uses logging module of the standard library that can provide a flexible framework for emitting log messages. You can enable logging for your SDK script by adding the following lines above your snippets: You can enable logging by using basicConfig with various logging levels: logging.DEBUG : used to give detailed information, typically of interest only when diagnosing problems (mostly used level in SDK). logging.INFO : used to confirm that things are working as expected. logging.WARN : used as an indication that something unexpected happened, or as a warning of some problem in the near future. logging.ERROR : indicates that due to a more serious problem, the SDK has not been able to perform some operation. logging.CRITICAL : indicates a serious error, suggesting that the program itself may be unable to continue running (not used in SDK as of now). You can enable logging by using basicConfig with various logging levels: logging.DEBUG : used to give detailed information, typically of interest only when diagnosing problems (mostly used level in SDK). logging.INFO : used to confirm that things are working as expected. logging.WARN : used as an indication that something unexpected happened, or as a warning of some problem in the near future. logging.ERROR : indicates that due to a more serious problem, the SDK has not been able to perform some operation. logging.CRITICAL : indicates a serious error, suggesting that the program itself may be unable to continue running (not used in SDK as of now). By default, logs will appear in your console. If you want to use file logging, you can add the following line: logging.config.fileConfig('pyatlan\/logging.conf') : this will generate logs according to the configuration defined in pyatlan\/logging.conf and will generate two log files: \/tmp\/pyatlan.log : default log file. \/tmp\/pyatlan.json : log file in JSON format. By default, logs will appear in your console. If you want to use file logging, you can add the following line: logging.config.fileConfig('pyatlan\/logging.conf') : this will generate logs according to the configuration defined in pyatlan\/logging.conf and will generate two log files: \/tmp\/pyatlan.log : default log file. \/tmp\/pyatlan.json : log file in JSON format. logging.config.fileConfig('pyatlan\/logging.conf') : this will generate logs according to the configuration defined in pyatlan\/logging.conf and will generate two log files: \/tmp\/pyatlan.log : default log file. \/tmp\/pyatlan.json : log file in JSON format. The SDK handles automatically retrying your requests when it detects certain problems: When there is a 403 response indicating that permission for an operation is not (yet) available. When there is a 429 response indicating that the request rate limit has been exceeded, and you need to retry after some time. When there is a 50x response indicating that something went wrong on the server side. If any request encounters one of these problems, it will be retried. Before each retry, the SDK will apply a delay using an exponential backoff. (Currently the values for the exponential backoff are not configurable.) For each request that encounters any of these problems, only up to a maximum number of retries will be attempted. (This is set to 5 by default.) By default, the SDK AtlanClient() has the following timeout settings: read_timeout : 900.0 seconds ( 15 minutes) connect_timeout : 30.0 seconds If you need to override these defaults, you can do so as shown in the example below: Since version 1.0.0, the Python SDK supports connecting to multiple tenants.[^1] When you use the AtlanClient() method you are actually setting a default client. This default client will be used behind-the-scenes for any operations that need information specific to an Atlan tenant. When you want to override that default client you can create a new one and use the set_default_client() method to change it: The AtlanClient() method will return a client for the given base URL, creating a new client and setting this new client as the default client. If you want to switch between clients that you have already created, you can use Atlan.set_default_client() to change between them. Limit the number of clients to those you must have Each client you create maintains its own independent copy of various caches. So the more clients you have, the more resources your code will consume. For this reason, we recommended limiting the number of clients you create to the bare minimum you require â€” ideally just a single client per tenant. (And since in the majority of use cases you only need access to a single tenant, this means you can most likely just rely on the default client and the fallback behavior.) Pyatlan uses the Requests library which supports proxy configuration via environment variables. Requests relies on the proxy configuration defined by standard environment variables http_proxy, https_proxy, no_proxy, and all_proxy. Uppercase variants of these variables are also supported. You can therefore set them to configure Pyatlan (only set the ones relevant to your needs): To use HTTP Basic Auth with your proxy, use the http:\/\/user:password@host\/ syntax in any of the above configuration entries: Currently, the way this is implemented limits you to either avoiding multiple threads in your Python code (if you need to use multiple clients), or if you want to use multiple threads you should only use a single client. Asynchronous SDK operations To get started, you need to initialize an AsyncAtlanClient : Create an async client using the same configuration pattern as the synchronous client. Concurrent operations for improved performance The real power of async comes from running multiple operations concurrently . Instead of waiting for each operation to complete sequentially, you can execute them in parallel and reduce total execution time: Synchronous : Total time = operationâ‚ + operationâ‚‚ + . + operationâ‚™ Asynchronous : Total time = max(operationâ‚, operationâ‚‚, . , operationâ‚™) When to use async Async is most beneficial when you have: Multiple independent operations that can run concurrently I\/O-heavy workloads like API calls, database queries, or file operations Long-running operations where parallelization provides significant time savings For simple, single operations, the synchronous client may be more straightforward to use."}
{"url":"https:\/\/developer.atlan.com\/sdks\/kotlin\/#logging","title":"Kotlin SDK - Developer","text":"Obtain the SDK For Kotlin, you can reuse the existing Java SDK as-is. It is available on Maven Central, ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include slf4j-simple as a simple binding mechanism to send any logging information out to your console (standard out), along with the kotlin-logging-jvm microutil. Configure the SDK There are two ways to configure the SDK: Using environment variables ATLAN_API_KEY should be given your Atlan API token , for authentication ( don't forget to assign one or more personas to the API token to give access to existing assets! ) ATLAN_BASE_URL should be given your Atlan URL (for example, https:\/\/tenant.atlan.com ) Here's an example of setting those environment variables: On client creation If you prefer to not use environment variables, you can do the following: Delve into more detailed examples: Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Real code samples our customers use to solve particular use cases. Review live samples Events Delve deep into the details of the events Atlan triggers. Learn more about events Delve deep into the details of the events Atlan triggers. Learn more about events The SDK defines checked exceptions for the following categories of error: A given API call could fail due to all of the errors above. So these all extend a generic AtlanException checked exception, and all API operations throw AtlanException . For example, when creating a connection there is an asynchronous process that grants permissions to the admins of that connection. So there can be a slight delay between creating the connection and being permitted to do any operations with the connection. During that delay, any attempt to interact with the connection will result in a PermissionException , even if your API token was used to create connection in the first place. Another example you may occasionally hit is some network issue that causes your connection to Atlan to be interrupted. In these cases, an ApiConnectionException will be raised. Don't worry, the SDK retries automatically While these are useful to know for detecting issues, the SDK automatically retries on such problems. Atlan is a distributed, cloud-native application, where network problems can arise. These advanced configuration options allow you to optimize how the SDK handles such ephemeral problems. The SDK uses slf4j to be logging framework-agnostic. You can therefore configure your own preferred logging framework: Replace the org.slf4j:slf4j-simple:2.0.7 binding with log4j2 bindings. The SDK handles automatically retrying your requests when it detects certain problems: When an ApiConnectionException occurs that is caused by an underlying ConnectException or SocketTimeoutException . When there is a 403 response indicating that permission for an operation is not (yet) available. When there is a 500 response indicating that something went wrong on the server side. If any request encounters one of these problems, it will be retried. Before each retry, the SDK will apply a delay using: An exponential backoff (starting from 500ms) A jitter (in the range of 75-100% of the backoff delay) Each retry will be at least 500ms, and at most 5s. (Currently these values are not configurable.) For each request that encounters any of these problems, only up to a maximum number of retries will be attempted. (This is set to 3 by default.) You can configure the maximum number of retries globally using setMaxNetworkRetries() on a client. Set this to an integer: The SDK will only wait so long for a response before assuming a network problem has occurred and the request should be timed out. By default, this is set to 80 seconds. You can configure the maximum time the SDK will wait before timing out a request using setReadTimeout() on a client. Set this to an integer giving the number of milliseconds before timing out: Remember this must be given in milliseconds. This example sets the timeout to 2 minutes (120 seconds * 1000 milliseconds). Since version 0.9.0, the Java SDK supports connecting to multiple tenants. From version 4.0.0 onwards you can create any number of clients against any number of different tenants, since every operation that interacts with a tenant now explicitly requires a client to be provided to it: Constructing a new client with a different tenant's URL is sufficient to create connectivity to that other tenant. You can also (optionally) provide a second argument to directly give the API token for the tenant. Create an object as usual. You can access the operations for assets directly on the client, under client.assets . These will generally give you the most flexibility â€” they can handle multiple objects at a time and allow overrides. Every operation on the client itself has a variant with an (optional) final argument through which you can override settings like retry limits or timeouts for this single request. You can use the from(client) factory method to initialize the request options with all the settings of your client, and then you only need to chain on those you want to override for this particular request. Alternatively, you can pass the client to the operation on the object itself. Limit the number of clients to those you must have Each client you create maintains its own independent copy of various caches. So the more clients you have, the more resources your code will consume. For this reason, we recommended limiting the number of clients you create to the bare minimum you require â€” ideally just a single client per tenant. Using a proxy To use the Java SDK with a proxy, you need to send in some additional parameters when running any java . command. These are described in detail in the Java documentation , but are summarized here for simplicity: socksProxyHost should be set to the hostname for your SOCKS proxy socksProxyPort should be set to the port for your SOCKS proxy (default being 1080) Providing credentials to the proxy In either case, if you need to authenticate to your proxy, you will need to wrap whatever code you want to run to set up these credentials using something like the following: You need to create a built-in Java PasswordAuthentication object. Provide your username as the first argument. . and your password as the second argument, as a char[] . (Of course, you should not hard-code your password in your code itself, but rather pull it from elsewhere.) Then use setProxyCredential() to pass this PasswordAuthentication object to the Atlan client, before any of the rest of the code will execute."}
{"url":"https:\/\/developer.atlan.com\/sdks\/go\/#logging","title":"Go SDK - Developer","text":"Obtain the SDK The Go SDK is currently in a pre-release, experimental state. While in this state, we reserve the right to make any changes to it (including breaking changes) without worrying about backwards compatibility, semantic versioning, and so on. If you are eager to experiment with it, it is available on GitHub . You can use Go dependencies to install it directly from there. We welcome your feedback during the pre-release, but cannot commit to any specific revisions or timelines at this point in time. Configure the SDK There are two ways to configure the SDK: Using environment variables ATLAN_API_KEY should be given your Atlan API token , for authentication ( don't forget to assign one or more personas to the API token to give access to existing assets! ) ATLAN_BASE_URL should be given your Atlan URL (for example, https:\/\/tenant.atlan.com ) Here's an example of setting those environment variables: On client creation If you prefer to not use environment variables, you can do the following: Careful not to expose your API token! We generally discourage including your API token directly in your code, in case you accidentally commit it into a (public) version control system. But it's your choice exactly how you manage the API token and including it for use within the client. That's it â€” once these are set you can start using your SDK to make live calls against your Atlan instance! ðŸŽ‰ Delve into more detailed examples: Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Real code samples our customers use to solve particular use cases. Review live samples Events Delve deep into the details of the events Atlan triggers. Learn more about events Delve deep into the details of the events Atlan triggers. Learn more about events The SDK defines exceptions for the following categories of error: A given API call could fail due to all of the errors above. So these all extend a generic AtlanError exception, and all API operations can potentially raise AtlanError . For example, when creating a connection there is an asynchronous process that grants permissions to the admins of that connection. So there can be a slight delay between creating the connection and being permitted to do any operations with the connection. During that delay, any attempt to interact with the connection will result in a PermissionError , even if your API token was used to create connection in the first place. Another example you may occasionally hit is some network issue that causes your connection to Atlan to be interrupted. In these cases, an ApiConnectionError will be raised. Atlan is a distributed, cloud-native application, where network problems can arise. The SDK therefore automatically attempts to handle ephemeral problems. The SDK uses the slog library internally to provide a flexible framework for emitting log messages. You can enable logging for your SDK script by adding the following lines above your snippets: You can enable logging by using .SetLogger() on the context object with various logging levels: \"debug\" : used to give detailed information, typically of interest only when diagnosing problems (mostly used level in SDK). \"info\" : used to confirm that things are working as expected. \"warn\" : used as an indication that something unexpected happened, or as a warning of some problem in the near future. \"error\" : indicates that due to a more serious problem, the SDK has not been able to perform some operation. You can enable logging by using .SetLogger() on the context object with various logging levels: \"debug\" : used to give detailed information, typically of interest only when diagnosing problems (mostly used level in SDK). \"info\" : used to confirm that things are working as expected. \"warn\" : used as an indication that something unexpected happened, or as a warning of some problem in the near future. \"error\" : indicates that due to a more serious problem, the SDK has not been able to perform some operation."}
{"url":"https:\/\/developer.atlan.com\/getting-started\/#retrieving-metadata","title":"Introductory walkthrough - Developer","text":"An introductory walkthrough You might also like our Atlan Platform Essentials certification . Not sure where to start? Allow us to introduce Atlan development through example. 1 We strongly recommend using one of our SDKs to simplify the development process. As a first step, set one up: The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include logback as a simple binding mechanism to send any logging information out to your console (standard out). Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on PyPI . You can use pip to install it as follows: Provide two values to create an Atlan client: Provide your Atlan tenant URL to the base_url parameter. (You can also do this through environment variables .) Provide your API token to the api_key parameter. (You can also do this through environment variables .) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include slf4j-simple as a simple binding mechanism to send any logging information out to your console (standard out), along with the kotlin-logging-jvm microutil. Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on GitHub , ready to be included in your project: Provide two values to set up connectivity to Atlan: Provide your Atlan tenant URL to the assets.Context() method. If you prefer using the value from an environment variable, you can use assets.NewContext() without any parameters. Provide your API token as the second parameter to the assets.Context() method. (Or again, have it picked up automatically by the assets.NewContext() method.) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. Don't forget to give permissions If you want to be able to access existing metadata with an API token, don't forget that you need to assign one or more personas to the API token that grant it access to metadata. Now that you have an SDK installed and configured, you are ready to code! Before we jump straight to code, though, let's first introduce some key concepts in Atlan: What is an asset? In Atlan, we refer to all objects that provide context to your data as assets . Each type of asset in Atlan has a set of: Properties , such as: Certificates Announcements Properties , such as: Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table In an object-oriented programming sense, think of an asset as an instance of a class. The structure of an asset (the class itself, in this analogy) is defined by something called a type definition , but that's for another day. So as you can see: There are many different kinds of assets: tables, columns, schemas, databases, business intelligence dashboards, reports, and so on. Assets inter-relate with each other: a table has a parent schema and child columns, a schema has a parent database and child tables, and so on. Different kinds of assets have some common properties (like certificates) and other properties that are unique to that kind of asset (like a columnCount that only exists on tables, not on schemas or databases). When you know the asset When you already know which asset you want to retrieve, you can read it from Atlan using one of its identifiers . We'll discuss these in more detail as part of updates, but for now you can think of them as: is a primary key for an asset: completely unique, but meaningless by itself is a business key for an asset: unique for a given kind of asset, and interpretable You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the asset.get_by_guid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the asset.get_by_qualified_name() method on the Atlan client, providing the type of asset you expect to retrieve and its qualified_name . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the assets.GetByGuid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the assets.GetByQualifiedName() method on the Atlan client, providing the type of asset you expect to retrieve and its qualifiedName . (Each asset type is its own unique class in the SDK.) Note that the response is strongly typed: If you are retrieving a table, you will get a table back (as long as it exists). You do not need to figure out what properties or relationships exist on a table - the Table class defines them for for you already. In any modern IDE, this means you have type-ahead support for retrieving the properties and relationships from the table variable. You can also refer to the types reference in this portal for full details of every kind of asset. Retrieval by identifier can be more costly than you might expect Even though you are retrieving an asset by an identifier, this can be more costly than you might expect. Retrieving an asset in this way will: Retrieve all its properties and their values Retrieve all its relationships Imagine the asset you are retrieving has 100's or 1000's of these. If you only care about its certificate and any owners, you will be retrieving far more information than you need. When you need to find it first For example, imagine you want to find all tables named MY_TABLE : You can then run the request using Execute() . For example, if you want to know the certificate of the asset you only need to tack that onto the query: Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many include_on_results calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can include as many attributes in IncludeOnResults as you want to specify the properties and relationships you want to retrieve for matching assets. Also gives the best performance If all you want to do is check or report on metadata, you should have a starting point from the information above. Or, now that you've found an asset of interest, maybe you want to update the asset with additional metadata ? Once again, before we jump to code, let's first understand some key concepts about how Atlan handles updates: Importance of identifiers Most operations on assets are upserts , that is, they could either create (insert) or update a given asset. How do you know which is going to happen? To answer this question, you need to understand how Atlan uniquely identifies each asset. Recall earlier we discussed asset's different identifiers in Atlan . Every asset in Atlan has at least the following two unique identifiers. These are both mandatory for every asset, so no asset can exist without these: Atlan uses globally-unique identifiers (GUIDs) to uniquely identify each asset, globally . They look something like this: As the name implies, GUIDs are: Globally unique (across all systems). Generated in a way that makes it nearly impossible for anything else to ever generate that same ID. 2 Note that this means the GUID itself is not : Meaningful or capable of being interpreted in any way Atlan uses qualifiedName s to uniquely identify assets based on their characteristics. They look something like this: Qualified names are not : Globally unique (across all systems). Instead, they are: Consistently constructed in a meaningful way, making it possible for them to be reconstructed. Note that this means the qualifiedName is: Meaningful and capable of being interpreted How these impact updates Since they are truly unique, operations that include a GUID will only update an asset, not create one. Conversely, operations that take a qualifiedName can: Create an asset, if no exactly-matching qualifiedName is found in Atlan. Update an asset, if an exact-match for the qualifiedName is found in Atlan. These operations also require a typeName , so that if creation does occur the correct type of asset is created. Unintended consequences of this behavior Be careful when using operations with only the qualifiedName . You may end up creating assets when you were only expecting them to be updated or to fail if they did not already exist. This is particularly true when you do not give the exact, case-sensitive qualifiedName of an asset. a\/b\/c\/d is not the same as a\/B\/c\/d when it comes to qualifiedName s. Perhaps this leaves you wondering: why have a qualifiedName at all? The qualifiedName 's purpose is to identify what is a unique asset. Many different tools might all have information about that asset. Having a common \"identity\" means that many different systems can each independently construct its identifier the same way. If a crawler gets table details from Snowflake it can upsert based on those identity characteristics in Atlan. The crawler will not create duplicate tables every time it runs. This gives idempotency. Looker knows the same identity characteristics for the Snowflake tables and columns. So if you get details from Looker about the tables it uses for reporting, you can link them together in lineage. (Looker can construct the same identifier for the table as Snowflake itself.) These characteristics are not possible using GUIDs alone. Limit to changes only Now that you understand the nuances of identifiers, let's look at how you can update metadata in Atlan. In general, you only need to send changes to Atlan. You do not need to send an entire asset each time you want to make changes to it. For example, imagine you want to mark a table as certified but do not want to change anything else (its name, description, owner details, and so on): You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualified_name . Using the updater() class method on any asset type, you pass in (typically) the qualified_name and name of the asset. You can then add onto the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to client.asset.save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the Updater() method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns an object into which you can then place any updates. You can place into the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to .Save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. Atlan will handle idempotency By sending only the changes you want to apply, Atlan can make idempotent updates. Atlan will only attempt to update the asset with the changes you send. Atlan leaves any existing metadata on the asset as-is. If the asset already has the metadata values you are sending, Atlan does nothing. It will not even update audit details like the last update timestamp, and is thus idempotent. What if you want to make changes to many assets, as efficiently as possible? Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Where to go from here Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Real code samples our customers use to solve particular use cases. Review live samples Events Delve deep into the details of the events Atlan triggers. Learn more about events Delve deep into the details of the events Atlan triggers. Learn more about events Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†©"}
{"url":"https:\/\/developer.atlan.com\/getting-started\/#what-is-an-asset","title":"Introductory walkthrough - Developer","text":"An introductory walkthrough You might also like our Atlan Platform Essentials certification . Not sure where to start? Allow us to introduce Atlan development through example. 1 We strongly recommend using one of our SDKs to simplify the development process. As a first step, set one up: The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include logback as a simple binding mechanism to send any logging information out to your console (standard out). Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on PyPI . You can use pip to install it as follows: Provide two values to create an Atlan client: Provide your Atlan tenant URL to the base_url parameter. (You can also do this through environment variables .) Provide your API token to the api_key parameter. (You can also do this through environment variables .) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include slf4j-simple as a simple binding mechanism to send any logging information out to your console (standard out), along with the kotlin-logging-jvm microutil. Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on GitHub , ready to be included in your project: Provide two values to set up connectivity to Atlan: Provide your Atlan tenant URL to the assets.Context() method. If you prefer using the value from an environment variable, you can use assets.NewContext() without any parameters. Provide your API token as the second parameter to the assets.Context() method. (Or again, have it picked up automatically by the assets.NewContext() method.) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. Don't forget to give permissions If you want to be able to access existing metadata with an API token, don't forget that you need to assign one or more personas to the API token that grant it access to metadata. Now that you have an SDK installed and configured, you are ready to code! Before we jump straight to code, though, let's first introduce some key concepts in Atlan: What is an asset? In Atlan, we refer to all objects that provide context to your data as assets . Each type of asset in Atlan has a set of: Properties , such as: Certificates Announcements Properties , such as: Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table In an object-oriented programming sense, think of an asset as an instance of a class. The structure of an asset (the class itself, in this analogy) is defined by something called a type definition , but that's for another day. So as you can see: There are many different kinds of assets: tables, columns, schemas, databases, business intelligence dashboards, reports, and so on. Assets inter-relate with each other: a table has a parent schema and child columns, a schema has a parent database and child tables, and so on. Different kinds of assets have some common properties (like certificates) and other properties that are unique to that kind of asset (like a columnCount that only exists on tables, not on schemas or databases). When you know the asset When you already know which asset you want to retrieve, you can read it from Atlan using one of its identifiers . We'll discuss these in more detail as part of updates, but for now you can think of them as: is a primary key for an asset: completely unique, but meaningless by itself is a business key for an asset: unique for a given kind of asset, and interpretable You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the asset.get_by_guid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the asset.get_by_qualified_name() method on the Atlan client, providing the type of asset you expect to retrieve and its qualified_name . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the assets.GetByGuid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the assets.GetByQualifiedName() method on the Atlan client, providing the type of asset you expect to retrieve and its qualifiedName . (Each asset type is its own unique class in the SDK.) Note that the response is strongly typed: If you are retrieving a table, you will get a table back (as long as it exists). You do not need to figure out what properties or relationships exist on a table - the Table class defines them for for you already. In any modern IDE, this means you have type-ahead support for retrieving the properties and relationships from the table variable. You can also refer to the types reference in this portal for full details of every kind of asset. Retrieval by identifier can be more costly than you might expect Even though you are retrieving an asset by an identifier, this can be more costly than you might expect. Retrieving an asset in this way will: Retrieve all its properties and their values Retrieve all its relationships Imagine the asset you are retrieving has 100's or 1000's of these. If you only care about its certificate and any owners, you will be retrieving far more information than you need. When you need to find it first For example, imagine you want to find all tables named MY_TABLE : You can then run the request using Execute() . For example, if you want to know the certificate of the asset you only need to tack that onto the query: Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many include_on_results calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can include as many attributes in IncludeOnResults as you want to specify the properties and relationships you want to retrieve for matching assets. Also gives the best performance If all you want to do is check or report on metadata, you should have a starting point from the information above. Or, now that you've found an asset of interest, maybe you want to update the asset with additional metadata ? Once again, before we jump to code, let's first understand some key concepts about how Atlan handles updates: Importance of identifiers Most operations on assets are upserts , that is, they could either create (insert) or update a given asset. How do you know which is going to happen? To answer this question, you need to understand how Atlan uniquely identifies each asset. Recall earlier we discussed asset's different identifiers in Atlan . Every asset in Atlan has at least the following two unique identifiers. These are both mandatory for every asset, so no asset can exist without these: Atlan uses globally-unique identifiers (GUIDs) to uniquely identify each asset, globally . They look something like this: As the name implies, GUIDs are: Globally unique (across all systems). Generated in a way that makes it nearly impossible for anything else to ever generate that same ID. 2 Note that this means the GUID itself is not : Meaningful or capable of being interpreted in any way Atlan uses qualifiedName s to uniquely identify assets based on their characteristics. They look something like this: Qualified names are not : Globally unique (across all systems). Instead, they are: Consistently constructed in a meaningful way, making it possible for them to be reconstructed. Note that this means the qualifiedName is: Meaningful and capable of being interpreted How these impact updates Since they are truly unique, operations that include a GUID will only update an asset, not create one. Conversely, operations that take a qualifiedName can: Create an asset, if no exactly-matching qualifiedName is found in Atlan. Update an asset, if an exact-match for the qualifiedName is found in Atlan. These operations also require a typeName , so that if creation does occur the correct type of asset is created. Unintended consequences of this behavior Be careful when using operations with only the qualifiedName . You may end up creating assets when you were only expecting them to be updated or to fail if they did not already exist. This is particularly true when you do not give the exact, case-sensitive qualifiedName of an asset. a\/b\/c\/d is not the same as a\/B\/c\/d when it comes to qualifiedName s. Perhaps this leaves you wondering: why have a qualifiedName at all? The qualifiedName 's purpose is to identify what is a unique asset. Many different tools might all have information about that asset. Having a common \"identity\" means that many different systems can each independently construct its identifier the same way. If a crawler gets table details from Snowflake it can upsert based on those identity characteristics in Atlan. The crawler will not create duplicate tables every time it runs. This gives idempotency. Looker knows the same identity characteristics for the Snowflake tables and columns. So if you get details from Looker about the tables it uses for reporting, you can link them together in lineage. (Looker can construct the same identifier for the table as Snowflake itself.) These characteristics are not possible using GUIDs alone. Limit to changes only Now that you understand the nuances of identifiers, let's look at how you can update metadata in Atlan. In general, you only need to send changes to Atlan. You do not need to send an entire asset each time you want to make changes to it. For example, imagine you want to mark a table as certified but do not want to change anything else (its name, description, owner details, and so on): You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualified_name . Using the updater() class method on any asset type, you pass in (typically) the qualified_name and name of the asset. You can then add onto the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to client.asset.save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the Updater() method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns an object into which you can then place any updates. You can place into the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to .Save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. Atlan will handle idempotency By sending only the changes you want to apply, Atlan can make idempotent updates. Atlan will only attempt to update the asset with the changes you send. Atlan leaves any existing metadata on the asset as-is. If the asset already has the metadata values you are sending, Atlan does nothing. It will not even update audit details like the last update timestamp, and is thus idempotent. What if you want to make changes to many assets, as efficiently as possible? Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Where to go from here Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Real code samples our customers use to solve particular use cases. Review live samples Events Delve deep into the details of the events Atlan triggers. Learn more about events Delve deep into the details of the events Atlan triggers. Learn more about events Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†©"}
{"url":"https:\/\/developer.atlan.com\/getting-started\/#when-you-know-the-asset","title":"Introductory walkthrough - Developer","text":"An introductory walkthrough You might also like our Atlan Platform Essentials certification . Not sure where to start? Allow us to introduce Atlan development through example. 1 We strongly recommend using one of our SDKs to simplify the development process. As a first step, set one up: The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include logback as a simple binding mechanism to send any logging information out to your console (standard out). Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on PyPI . You can use pip to install it as follows: Provide two values to create an Atlan client: Provide your Atlan tenant URL to the base_url parameter. (You can also do this through environment variables .) Provide your API token to the api_key parameter. (You can also do this through environment variables .) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include slf4j-simple as a simple binding mechanism to send any logging information out to your console (standard out), along with the kotlin-logging-jvm microutil. Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on GitHub , ready to be included in your project: Provide two values to set up connectivity to Atlan: Provide your Atlan tenant URL to the assets.Context() method. If you prefer using the value from an environment variable, you can use assets.NewContext() without any parameters. Provide your API token as the second parameter to the assets.Context() method. (Or again, have it picked up automatically by the assets.NewContext() method.) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. Don't forget to give permissions If you want to be able to access existing metadata with an API token, don't forget that you need to assign one or more personas to the API token that grant it access to metadata. Now that you have an SDK installed and configured, you are ready to code! Before we jump straight to code, though, let's first introduce some key concepts in Atlan: What is an asset? In Atlan, we refer to all objects that provide context to your data as assets . Each type of asset in Atlan has a set of: Properties , such as: Certificates Announcements Properties , such as: Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table In an object-oriented programming sense, think of an asset as an instance of a class. The structure of an asset (the class itself, in this analogy) is defined by something called a type definition , but that's for another day. So as you can see: There are many different kinds of assets: tables, columns, schemas, databases, business intelligence dashboards, reports, and so on. Assets inter-relate with each other: a table has a parent schema and child columns, a schema has a parent database and child tables, and so on. Different kinds of assets have some common properties (like certificates) and other properties that are unique to that kind of asset (like a columnCount that only exists on tables, not on schemas or databases). When you know the asset When you already know which asset you want to retrieve, you can read it from Atlan using one of its identifiers . We'll discuss these in more detail as part of updates, but for now you can think of them as: is a primary key for an asset: completely unique, but meaningless by itself is a business key for an asset: unique for a given kind of asset, and interpretable You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the asset.get_by_guid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the asset.get_by_qualified_name() method on the Atlan client, providing the type of asset you expect to retrieve and its qualified_name . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the assets.GetByGuid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the assets.GetByQualifiedName() method on the Atlan client, providing the type of asset you expect to retrieve and its qualifiedName . (Each asset type is its own unique class in the SDK.) Note that the response is strongly typed: If you are retrieving a table, you will get a table back (as long as it exists). You do not need to figure out what properties or relationships exist on a table - the Table class defines them for for you already. In any modern IDE, this means you have type-ahead support for retrieving the properties and relationships from the table variable. You can also refer to the types reference in this portal for full details of every kind of asset. Retrieval by identifier can be more costly than you might expect Even though you are retrieving an asset by an identifier, this can be more costly than you might expect. Retrieving an asset in this way will: Retrieve all its properties and their values Retrieve all its relationships Imagine the asset you are retrieving has 100's or 1000's of these. If you only care about its certificate and any owners, you will be retrieving far more information than you need. When you need to find it first For example, imagine you want to find all tables named MY_TABLE : You can then run the request using Execute() . For example, if you want to know the certificate of the asset you only need to tack that onto the query: Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many include_on_results calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can include as many attributes in IncludeOnResults as you want to specify the properties and relationships you want to retrieve for matching assets. Also gives the best performance If all you want to do is check or report on metadata, you should have a starting point from the information above. Or, now that you've found an asset of interest, maybe you want to update the asset with additional metadata ? Once again, before we jump to code, let's first understand some key concepts about how Atlan handles updates: Importance of identifiers Most operations on assets are upserts , that is, they could either create (insert) or update a given asset. How do you know which is going to happen? To answer this question, you need to understand how Atlan uniquely identifies each asset. Recall earlier we discussed asset's different identifiers in Atlan . Every asset in Atlan has at least the following two unique identifiers. These are both mandatory for every asset, so no asset can exist without these: Atlan uses globally-unique identifiers (GUIDs) to uniquely identify each asset, globally . They look something like this: As the name implies, GUIDs are: Globally unique (across all systems). Generated in a way that makes it nearly impossible for anything else to ever generate that same ID. 2 Note that this means the GUID itself is not : Meaningful or capable of being interpreted in any way Atlan uses qualifiedName s to uniquely identify assets based on their characteristics. They look something like this: Qualified names are not : Globally unique (across all systems). Instead, they are: Consistently constructed in a meaningful way, making it possible for them to be reconstructed. Note that this means the qualifiedName is: Meaningful and capable of being interpreted How these impact updates Since they are truly unique, operations that include a GUID will only update an asset, not create one. Conversely, operations that take a qualifiedName can: Create an asset, if no exactly-matching qualifiedName is found in Atlan. Update an asset, if an exact-match for the qualifiedName is found in Atlan. These operations also require a typeName , so that if creation does occur the correct type of asset is created. Unintended consequences of this behavior Be careful when using operations with only the qualifiedName . You may end up creating assets when you were only expecting them to be updated or to fail if they did not already exist. This is particularly true when you do not give the exact, case-sensitive qualifiedName of an asset. a\/b\/c\/d is not the same as a\/B\/c\/d when it comes to qualifiedName s. Perhaps this leaves you wondering: why have a qualifiedName at all? The qualifiedName 's purpose is to identify what is a unique asset. Many different tools might all have information about that asset. Having a common \"identity\" means that many different systems can each independently construct its identifier the same way. If a crawler gets table details from Snowflake it can upsert based on those identity characteristics in Atlan. The crawler will not create duplicate tables every time it runs. This gives idempotency. Looker knows the same identity characteristics for the Snowflake tables and columns. So if you get details from Looker about the tables it uses for reporting, you can link them together in lineage. (Looker can construct the same identifier for the table as Snowflake itself.) These characteristics are not possible using GUIDs alone. Limit to changes only Now that you understand the nuances of identifiers, let's look at how you can update metadata in Atlan. In general, you only need to send changes to Atlan. You do not need to send an entire asset each time you want to make changes to it. For example, imagine you want to mark a table as certified but do not want to change anything else (its name, description, owner details, and so on): You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualified_name . Using the updater() class method on any asset type, you pass in (typically) the qualified_name and name of the asset. You can then add onto the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to client.asset.save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the Updater() method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns an object into which you can then place any updates. You can place into the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to .Save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. Atlan will handle idempotency By sending only the changes you want to apply, Atlan can make idempotent updates. Atlan will only attempt to update the asset with the changes you send. Atlan leaves any existing metadata on the asset as-is. If the asset already has the metadata values you are sending, Atlan does nothing. It will not even update audit details like the last update timestamp, and is thus idempotent. What if you want to make changes to many assets, as efficiently as possible? Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Where to go from here Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Real code samples our customers use to solve particular use cases. Review live samples Events Delve deep into the details of the events Atlan triggers. Learn more about events Delve deep into the details of the events Atlan triggers. Learn more about events Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†©"}
{"url":"https:\/\/developer.atlan.com\/snippets\/advanced-examples\/read\/","title":"Retrieving assets - Developer","text":"Strictly speaking, no, you do not. And in fact if you ultimately intend to update an asset you should trim it down to only what you intend to change and not send a complete asset. See Updating an asset for more details. Retrieving an asset uses a slightly different pattern from the other operations. For this you can use static methods provided by the Asset class: To retrieve an asset by its GUID: If no exception is thrown, the returned object will be non-null and of the type requested. Because this operation will read the asset from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Compile-time type checking This operation will type-check the asset you are retrieving is of the type requested. If it is not, you will receive a NotFoundException , even if the GUID represents some other asset. client.asset.get_by_guid() method takes following parameters: guid : specify the (GUID) of the asset to retrieve. asset_type ( optional ): specify the type of asset to retrieve. Defaults to Asset . If no exception is thrown, the returned object will be non-null and of the type requested. min_ext_info ( optional ): minimizes additional information when set to True . Defaults to False ignore_relationships ( optional ): specify whether to include relationships ( False ) or exclude them ( True ). Defaults to True attributes ( optional ): defines the list of attributes to retrieve for the asset. Accepts either a list of strings or a list of AtlanField . related_attributes ( optional ): defines the list of relationship attributes to retrieve for the asset. Accepts either a list of strings or a list of AtlanField . Attributes and Related attributes Run-time type checking This operation will type-check the asset you are retrieving is of the type requested. If it is not, you will receive a NotFoundException , even if the GUID represents some other asset. If no exception is thrown, the returned object will be non-null and of the type requested. Because this operation will read the asset from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Compile-time type checking This operation will type-check the asset you are retrieving is of the type requested. If it is not, you will receive a NotFoundException , even if the GUID represents some other asset. In the case of retrieving an asset, all necessary information is included in the URL of the request. There is no payload for the body of the request. By GUID (runtime typing) To retrieve an asset by GUID, but only resolve the type at runtime: Retrieve the asset by its GUID. Since GUIDs are globally unique, you do not need to specify a type. (And this is why the operation returns a generic Asset , since the SDK can only determine the type at runtime, once it has a response back from Atlan.) Since the operation returns a generic Asset , you need to check and cast it to a more specific type if you want to access the more specific attributes of that type. client.asset.get_by_guid() method takes following parameters: guid : specify the (GUID) of the asset to retrieve. asset_type ( optional ): specify the type of asset to retrieve. Defaults to Asset . If no exception is thrown, the returned object will be non-null and of the type requested. min_ext_info ( optional ): minimizes additional information when set to True . Defaults to False ignore_relationships ( optional ): specify whether to include relationships ( False ) or exclude them ( True ). Defaults to True attributes ( optional ): defines the list of attributes to retrieve for the asset. Accepts either a list of strings or a list of AtlanField . related_attributes ( optional ): defines the list of relationship attributes to retrieve for the asset. Accepts either a list of strings or a list of AtlanField . Attributes and Related attributes Since the operation returns a generic Asset , you need to use isinstance() to cast it to a more specific type in the block if you want an IDE to provide more specific type hints. Since the operation returns a generic Asset , you need to use isinstance() to cast it to a more specific type in the block if you want an IDE to provide more specific type hints. Retrieve the asset by its GUID. Since GUIDs are globally unique, you do not need to specify a type. (And this is why the operation returns a generic Asset , since the SDK can only determine the type at runtime, once it has a response back from Atlan.) Since the operation returns a generic Asset , you need to check and cast it to a more specific type if you want to access the more specific attributes of that type. Does not apply to a raw API request There is no concept of typing in a raw API request â€” all responses to the raw API will simply be JSON objects. To retrieve an asset by its qualifiedName : If no exception is thrown, the returned object will be non-null and of the type requested. Because this operation will read the asset from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Qualified name, not name For most objects, you can probably build-up the qualifiedName in your code directly. Because this operation will read the asset from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Finding the connection portion The one exception is likely to be the connection portion of the name ( default\/snowflake\/1657037873 in this example). To find this portion, see Find connections . For most objects, you can probably build-up the qualifiedName in your code directly. Because this operation will read the asset from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Finding the connection portion The one exception is likely to be the connection portion of the name ( default\/snowflake\/1657037873 in this example). To find this portion, see Find connections . client.asset.get_by_qualified_name() method takes following parameters: qualified_name : specify the qualified name of the asset to retrieve. asset_type : specify the type of asset to retrieve. If no exception is thrown, the returned object will be non-null and of the type requested. min_ext_info ( optional ): minimizes additional information when set to True . Defaults to False ignore_relationships ( optional ): specify whether to include relationships ( False ) or exclude them ( True ). Defaults to True attributes ( optional ): defines the list of attributes to retrieve for the asset. Accepts either a list of strings or a list of AtlanField . related_attributes ( optional ): defines the list of relationship attributes to retrieve for the asset. Accepts either a list of strings or a list of AtlanField . Attributes and Related attributes For most objects, you can probably build-up the qualified_name in your code directly. Finding the connection portion The one exception is likely to be the connection portion of the name ( default\/snowflake\/1657037873 in this example). To find this portion, see Find connections . For most objects, you can probably build-up the qualified_name in your code directly. Finding the connection portion The one exception is likely to be the connection portion of the name ( default\/snowflake\/1657037873 in this example). To find this portion, see Find connections . If no exception is thrown, the returned object will be non-null and of the type requested. Because this operation will read the asset from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Qualified name, not name For most objects, you can probably build-up the qualifiedName in your code directly. Because this operation will read the asset from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Finding the connection portion The one exception is likely to be the connection portion of the name ( default\/snowflake\/1657037873 in this example). To find this portion, see Find connections . For most objects, you can probably build-up the qualifiedName in your code directly. Because this operation will read the asset from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Finding the connection portion The one exception is likely to be the connection portion of the name ( default\/snowflake\/1657037873 in this example). To find this portion, see Find connections . In the case of retrieving an asset, all necessary information is included in the URL of the request. There is no payload for the body of the request. URL encoding may be needed Note that depending on the qualifiedName, you may need to URL-encode its value before sending. This is to replace any parts of the name that could be misinterpreted as actual URL components (like \/ or spaces). In the case of retrieving an asset, all necessary information is included in the URL of the request. There is no payload for the body of the request. URL encoding may be needed Note that depending on the qualifiedName, you may need to URL-encode its value before sending. This is to replace any parts of the name that could be misinterpreted as actual URL components (like \/ or spaces). Full vs minimal assets The examples above illustrate how to retrieve: an asset with all of its relationships (a complete asset). an asset without any of its relationships (a minimal asset). You can also retrieve the opposite by explicitly requesting it: Retrieve the full asset, with all of its relationships, by its GUID. The last (optional) parameter being true indicates you want to retrieve the asset with all its relationships (a \"full\" asset). Similar variations exist on every asset as well as on the dynamically-typed Asset static methods. Optionally, you can provide the asset type: If no exception is thrown, the returned object will be non-null and of the type requested. If no exception is thrown, the returned object will be non-null and of the type requested. Retrieve the full asset, with all of its relationships, by its GUID. The last (optional) parameter being true indicates you want to retrieve the asset with all its relationships (a \"full\" asset). Similar variations exist on every asset as well as on the dynamically-typed Asset static methods. In the case of retrieving an asset, all necessary information is included in the URL of the request. Retrieving a minimal asset is a matter of setting the query parameters ignoreRelationships and minExtInfo to true . Retrieve minimal assets where possible You should retrieve minimal assets for better performance in cases where you do not need all of the relationships of the asset. Keep in mind that although the relationships will not be visible in the object after retrieving a minimal asset, this does not mean that there are no relationships on that asset (in Atlan)."}
{"url":"https:\/\/developer.atlan.com\/getting-started\/#guid","title":"Introductory walkthrough - Developer","text":"An introductory walkthrough You might also like our Atlan Platform Essentials certification . Not sure where to start? Allow us to introduce Atlan development through example. 1 We strongly recommend using one of our SDKs to simplify the development process. As a first step, set one up: The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include logback as a simple binding mechanism to send any logging information out to your console (standard out). Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on PyPI . You can use pip to install it as follows: Provide two values to create an Atlan client: Provide your Atlan tenant URL to the base_url parameter. (You can also do this through environment variables .) Provide your API token to the api_key parameter. (You can also do this through environment variables .) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include slf4j-simple as a simple binding mechanism to send any logging information out to your console (standard out), along with the kotlin-logging-jvm microutil. Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on GitHub , ready to be included in your project: Provide two values to set up connectivity to Atlan: Provide your Atlan tenant URL to the assets.Context() method. If you prefer using the value from an environment variable, you can use assets.NewContext() without any parameters. Provide your API token as the second parameter to the assets.Context() method. (Or again, have it picked up automatically by the assets.NewContext() method.) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. Don't forget to give permissions If you want to be able to access existing metadata with an API token, don't forget that you need to assign one or more personas to the API token that grant it access to metadata. Now that you have an SDK installed and configured, you are ready to code! Before we jump straight to code, though, let's first introduce some key concepts in Atlan: What is an asset? In Atlan, we refer to all objects that provide context to your data as assets . Each type of asset in Atlan has a set of: Properties , such as: Certificates Announcements Properties , such as: Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table In an object-oriented programming sense, think of an asset as an instance of a class. The structure of an asset (the class itself, in this analogy) is defined by something called a type definition , but that's for another day. So as you can see: There are many different kinds of assets: tables, columns, schemas, databases, business intelligence dashboards, reports, and so on. Assets inter-relate with each other: a table has a parent schema and child columns, a schema has a parent database and child tables, and so on. Different kinds of assets have some common properties (like certificates) and other properties that are unique to that kind of asset (like a columnCount that only exists on tables, not on schemas or databases). When you know the asset When you already know which asset you want to retrieve, you can read it from Atlan using one of its identifiers . We'll discuss these in more detail as part of updates, but for now you can think of them as: is a primary key for an asset: completely unique, but meaningless by itself is a business key for an asset: unique for a given kind of asset, and interpretable You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the asset.get_by_guid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the asset.get_by_qualified_name() method on the Atlan client, providing the type of asset you expect to retrieve and its qualified_name . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the assets.GetByGuid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the assets.GetByQualifiedName() method on the Atlan client, providing the type of asset you expect to retrieve and its qualifiedName . (Each asset type is its own unique class in the SDK.) Note that the response is strongly typed: If you are retrieving a table, you will get a table back (as long as it exists). You do not need to figure out what properties or relationships exist on a table - the Table class defines them for for you already. In any modern IDE, this means you have type-ahead support for retrieving the properties and relationships from the table variable. You can also refer to the types reference in this portal for full details of every kind of asset. Retrieval by identifier can be more costly than you might expect Even though you are retrieving an asset by an identifier, this can be more costly than you might expect. Retrieving an asset in this way will: Retrieve all its properties and their values Retrieve all its relationships Imagine the asset you are retrieving has 100's or 1000's of these. If you only care about its certificate and any owners, you will be retrieving far more information than you need. When you need to find it first For example, imagine you want to find all tables named MY_TABLE : You can then run the request using Execute() . For example, if you want to know the certificate of the asset you only need to tack that onto the query: Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many include_on_results calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can include as many attributes in IncludeOnResults as you want to specify the properties and relationships you want to retrieve for matching assets. Also gives the best performance If all you want to do is check or report on metadata, you should have a starting point from the information above. Or, now that you've found an asset of interest, maybe you want to update the asset with additional metadata ? Once again, before we jump to code, let's first understand some key concepts about how Atlan handles updates: Importance of identifiers Most operations on assets are upserts , that is, they could either create (insert) or update a given asset. How do you know which is going to happen? To answer this question, you need to understand how Atlan uniquely identifies each asset. Recall earlier we discussed asset's different identifiers in Atlan . Every asset in Atlan has at least the following two unique identifiers. These are both mandatory for every asset, so no asset can exist without these: Atlan uses globally-unique identifiers (GUIDs) to uniquely identify each asset, globally . They look something like this: As the name implies, GUIDs are: Globally unique (across all systems). Generated in a way that makes it nearly impossible for anything else to ever generate that same ID. 2 Note that this means the GUID itself is not : Meaningful or capable of being interpreted in any way Atlan uses qualifiedName s to uniquely identify assets based on their characteristics. They look something like this: Qualified names are not : Globally unique (across all systems). Instead, they are: Consistently constructed in a meaningful way, making it possible for them to be reconstructed. Note that this means the qualifiedName is: Meaningful and capable of being interpreted How these impact updates Since they are truly unique, operations that include a GUID will only update an asset, not create one. Conversely, operations that take a qualifiedName can: Create an asset, if no exactly-matching qualifiedName is found in Atlan. Update an asset, if an exact-match for the qualifiedName is found in Atlan. These operations also require a typeName , so that if creation does occur the correct type of asset is created. Unintended consequences of this behavior Be careful when using operations with only the qualifiedName . You may end up creating assets when you were only expecting them to be updated or to fail if they did not already exist. This is particularly true when you do not give the exact, case-sensitive qualifiedName of an asset. a\/b\/c\/d is not the same as a\/B\/c\/d when it comes to qualifiedName s. Perhaps this leaves you wondering: why have a qualifiedName at all? The qualifiedName 's purpose is to identify what is a unique asset. Many different tools might all have information about that asset. Having a common \"identity\" means that many different systems can each independently construct its identifier the same way. If a crawler gets table details from Snowflake it can upsert based on those identity characteristics in Atlan. The crawler will not create duplicate tables every time it runs. This gives idempotency. Looker knows the same identity characteristics for the Snowflake tables and columns. So if you get details from Looker about the tables it uses for reporting, you can link them together in lineage. (Looker can construct the same identifier for the table as Snowflake itself.) These characteristics are not possible using GUIDs alone. Limit to changes only Now that you understand the nuances of identifiers, let's look at how you can update metadata in Atlan. In general, you only need to send changes to Atlan. You do not need to send an entire asset each time you want to make changes to it. For example, imagine you want to mark a table as certified but do not want to change anything else (its name, description, owner details, and so on): You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualified_name . Using the updater() class method on any asset type, you pass in (typically) the qualified_name and name of the asset. You can then add onto the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to client.asset.save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the Updater() method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns an object into which you can then place any updates. You can place into the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to .Save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. Atlan will handle idempotency By sending only the changes you want to apply, Atlan can make idempotent updates. Atlan will only attempt to update the asset with the changes you send. Atlan leaves any existing metadata on the asset as-is. If the asset already has the metadata values you are sending, Atlan does nothing. It will not even update audit details like the last update timestamp, and is thus idempotent. What if you want to make changes to many assets, as efficiently as possible? Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Where to go from here Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Real code samples our customers use to solve particular use cases. Review live samples Events Delve deep into the details of the events Atlan triggers. Learn more about events Delve deep into the details of the events Atlan triggers. Learn more about events Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†©"}
{"url":"https:\/\/developer.atlan.com\/getting-started\/#qualifiedname","title":"Introductory walkthrough - Developer","text":"An introductory walkthrough You might also like our Atlan Platform Essentials certification . Not sure where to start? Allow us to introduce Atlan development through example. 1 We strongly recommend using one of our SDKs to simplify the development process. As a first step, set one up: The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include logback as a simple binding mechanism to send any logging information out to your console (standard out). Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on PyPI . You can use pip to install it as follows: Provide two values to create an Atlan client: Provide your Atlan tenant URL to the base_url parameter. (You can also do this through environment variables .) Provide your API token to the api_key parameter. (You can also do this through environment variables .) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include slf4j-simple as a simple binding mechanism to send any logging information out to your console (standard out), along with the kotlin-logging-jvm microutil. Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on GitHub , ready to be included in your project: Provide two values to set up connectivity to Atlan: Provide your Atlan tenant URL to the assets.Context() method. If you prefer using the value from an environment variable, you can use assets.NewContext() without any parameters. Provide your API token as the second parameter to the assets.Context() method. (Or again, have it picked up automatically by the assets.NewContext() method.) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. Don't forget to give permissions If you want to be able to access existing metadata with an API token, don't forget that you need to assign one or more personas to the API token that grant it access to metadata. Now that you have an SDK installed and configured, you are ready to code! Before we jump straight to code, though, let's first introduce some key concepts in Atlan: What is an asset? In Atlan, we refer to all objects that provide context to your data as assets . Each type of asset in Atlan has a set of: Properties , such as: Certificates Announcements Properties , such as: Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table In an object-oriented programming sense, think of an asset as an instance of a class. The structure of an asset (the class itself, in this analogy) is defined by something called a type definition , but that's for another day. So as you can see: There are many different kinds of assets: tables, columns, schemas, databases, business intelligence dashboards, reports, and so on. Assets inter-relate with each other: a table has a parent schema and child columns, a schema has a parent database and child tables, and so on. Different kinds of assets have some common properties (like certificates) and other properties that are unique to that kind of asset (like a columnCount that only exists on tables, not on schemas or databases). When you know the asset When you already know which asset you want to retrieve, you can read it from Atlan using one of its identifiers . We'll discuss these in more detail as part of updates, but for now you can think of them as: is a primary key for an asset: completely unique, but meaningless by itself is a business key for an asset: unique for a given kind of asset, and interpretable You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the asset.get_by_guid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the asset.get_by_qualified_name() method on the Atlan client, providing the type of asset you expect to retrieve and its qualified_name . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the assets.GetByGuid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the assets.GetByQualifiedName() method on the Atlan client, providing the type of asset you expect to retrieve and its qualifiedName . (Each asset type is its own unique class in the SDK.) Note that the response is strongly typed: If you are retrieving a table, you will get a table back (as long as it exists). You do not need to figure out what properties or relationships exist on a table - the Table class defines them for for you already. In any modern IDE, this means you have type-ahead support for retrieving the properties and relationships from the table variable. You can also refer to the types reference in this portal for full details of every kind of asset. Retrieval by identifier can be more costly than you might expect Even though you are retrieving an asset by an identifier, this can be more costly than you might expect. Retrieving an asset in this way will: Retrieve all its properties and their values Retrieve all its relationships Imagine the asset you are retrieving has 100's or 1000's of these. If you only care about its certificate and any owners, you will be retrieving far more information than you need. When you need to find it first For example, imagine you want to find all tables named MY_TABLE : You can then run the request using Execute() . For example, if you want to know the certificate of the asset you only need to tack that onto the query: Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many include_on_results calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can include as many attributes in IncludeOnResults as you want to specify the properties and relationships you want to retrieve for matching assets. Also gives the best performance If all you want to do is check or report on metadata, you should have a starting point from the information above. Or, now that you've found an asset of interest, maybe you want to update the asset with additional metadata ? Once again, before we jump to code, let's first understand some key concepts about how Atlan handles updates: Importance of identifiers Most operations on assets are upserts , that is, they could either create (insert) or update a given asset. How do you know which is going to happen? To answer this question, you need to understand how Atlan uniquely identifies each asset. Recall earlier we discussed asset's different identifiers in Atlan . Every asset in Atlan has at least the following two unique identifiers. These are both mandatory for every asset, so no asset can exist without these: Atlan uses globally-unique identifiers (GUIDs) to uniquely identify each asset, globally . They look something like this: As the name implies, GUIDs are: Globally unique (across all systems). Generated in a way that makes it nearly impossible for anything else to ever generate that same ID. 2 Note that this means the GUID itself is not : Meaningful or capable of being interpreted in any way Atlan uses qualifiedName s to uniquely identify assets based on their characteristics. They look something like this: Qualified names are not : Globally unique (across all systems). Instead, they are: Consistently constructed in a meaningful way, making it possible for them to be reconstructed. Note that this means the qualifiedName is: Meaningful and capable of being interpreted How these impact updates Since they are truly unique, operations that include a GUID will only update an asset, not create one. Conversely, operations that take a qualifiedName can: Create an asset, if no exactly-matching qualifiedName is found in Atlan. Update an asset, if an exact-match for the qualifiedName is found in Atlan. These operations also require a typeName , so that if creation does occur the correct type of asset is created. Unintended consequences of this behavior Be careful when using operations with only the qualifiedName . You may end up creating assets when you were only expecting them to be updated or to fail if they did not already exist. This is particularly true when you do not give the exact, case-sensitive qualifiedName of an asset. a\/b\/c\/d is not the same as a\/B\/c\/d when it comes to qualifiedName s. Perhaps this leaves you wondering: why have a qualifiedName at all? The qualifiedName 's purpose is to identify what is a unique asset. Many different tools might all have information about that asset. Having a common \"identity\" means that many different systems can each independently construct its identifier the same way. If a crawler gets table details from Snowflake it can upsert based on those identity characteristics in Atlan. The crawler will not create duplicate tables every time it runs. This gives idempotency. Looker knows the same identity characteristics for the Snowflake tables and columns. So if you get details from Looker about the tables it uses for reporting, you can link them together in lineage. (Looker can construct the same identifier for the table as Snowflake itself.) These characteristics are not possible using GUIDs alone. Limit to changes only Now that you understand the nuances of identifiers, let's look at how you can update metadata in Atlan. In general, you only need to send changes to Atlan. You do not need to send an entire asset each time you want to make changes to it. For example, imagine you want to mark a table as certified but do not want to change anything else (its name, description, owner details, and so on): You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualified_name . Using the updater() class method on any asset type, you pass in (typically) the qualified_name and name of the asset. You can then add onto the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to client.asset.save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the Updater() method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns an object into which you can then place any updates. You can place into the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to .Save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. Atlan will handle idempotency By sending only the changes you want to apply, Atlan can make idempotent updates. Atlan will only attempt to update the asset with the changes you send. Atlan leaves any existing metadata on the asset as-is. If the asset already has the metadata values you are sending, Atlan does nothing. It will not even update audit details like the last update timestamp, and is thus idempotent. What if you want to make changes to many assets, as efficiently as possible? Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Where to go from here Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Real code samples our customers use to solve particular use cases. Review live samples Events Delve deep into the details of the events Atlan triggers. Learn more about events Delve deep into the details of the events Atlan triggers. Learn more about events Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†©"}
{"url":"https:\/\/developer.atlan.com\/models\/","title":"Full model reference - Developer","text":"Full model reference Our SDKs are strongly-typed to represent every type 's attributes and relationships directly. This reduces the burden on you to understand how these are named, capitalized, spelled, or even the type of data they possess â€” since all of those details are encoded directly into the models the SDKs use. Here you will find a complete reference of all of these attributes and relationships. In certain rare cases, our strongly-typed model might not (yet) include a property you need to access. In these cases, you can always directly access the raw properties returned in every API response. For example, when retrieving workflow credentials the response object does not include extra details that are connector-specific. However, you can still access these extra properties: Use the getRawJsonObject() accessor to access the entire (raw) response object from the API. For this credential example getRawJsonObject() looks like: { \"id\" : \"e3d74922-e2e8-4dbe-a7ed-3937e5153a51\" , \"version\" : \"super-king-1886\" , \"isActive\" : true , \"createdAt\" : 1719240600149 , \"updatedAt\" : 1719240600149 , \"createdBy\" : \"service-account-apikey-5aca94d3-f0a3-4af5-b074-567e209c1b75\" , \"tenantId\" : \"default\" , \"name\" : \"default-snowflake-1719240599-0\" , \"description\" : null , \"connectorConfigName\" : \"atlan-connectors-snowflake\" , \"connector\" : \"snowflake\" , \"connectorType\" : \"jdbc\" , \"authType\" : \"basic\" , \"host\" : \"abc123.snowflakecomputing.com\" , \"port\" : 443 , \"metadata\" : null , \"level\" : null , \"connection\" : null , \"username\" : \"atlan-user\" , \"extra\" : { \"role\" : \"ADMIN\" , \"warehouse\" : \"DEV\" } } Use the getRawJsonObject() accessor to access the entire (raw) response object from the API. For this credential example getRawJsonObject() looks like: Use __atlan_extra__ dict field for any response model to access extra API response properties. For this credential example __atlan_extra__ looks like: { \"username\" : \"atlan-user\" , \"extra\" : { \"role\" : \"ADMIN\" , \"warehouse\" : \"DEV\" } } Use __atlan_extra__ dict field for any response model to access extra API response properties. For this credential example __atlan_extra__ looks like: Use the rawJsonObject field to access the entire (raw) response object from the API. For this credential example rawJsonObject looks like: { \"id\" : \"e3d74922-e2e8-4dbe-a7ed-3937e5153a51\" , \"version\" : \"super-king-1886\" , \"isActive\" : true , \"createdAt\" : 1719240600149 , \"updatedAt\" : 1719240600149 , \"createdBy\" : \"service-account-apikey-5aca94d3-f0a3-4af5-b074-567e209c1b75\" , \"tenantId\" : \"default\" , \"name\" : \"default-snowflake-1719240599-0\" , \"description\" : null , \"connectorConfigName\" : \"atlan-connectors-snowflake\" , \"connector\" : \"snowflake\" , \"connectorType\" : \"jdbc\" , \"authType\" : \"basic\" , \"host\" : \"abc123.snowflakecomputing.com\" , \"port\" : 443 , \"metadata\" : null , \"level\" : null , \"connection\" : null , \"username\" : \"atlan-user\" , \"extra\" : { \"role\" : \"ADMIN\" , \"warehouse\" : \"DEV\" } } Use the rawJsonObject field to access the entire (raw) response object from the API. For this credential example rawJsonObject looks like: These are all the metadata assets currently available through Atlan's SDKs:"}
{"url":"https:\/\/developer.atlan.com\/getting-started\/#when-you-need-to-find-it-first","title":"Introductory walkthrough - Developer","text":"An introductory walkthrough You might also like our Atlan Platform Essentials certification . Not sure where to start? Allow us to introduce Atlan development through example. 1 We strongly recommend using one of our SDKs to simplify the development process. As a first step, set one up: The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include logback as a simple binding mechanism to send any logging information out to your console (standard out). Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on PyPI . You can use pip to install it as follows: Provide two values to create an Atlan client: Provide your Atlan tenant URL to the base_url parameter. (You can also do this through environment variables .) Provide your API token to the api_key parameter. (You can also do this through environment variables .) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include slf4j-simple as a simple binding mechanism to send any logging information out to your console (standard out), along with the kotlin-logging-jvm microutil. Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on GitHub , ready to be included in your project: Provide two values to set up connectivity to Atlan: Provide your Atlan tenant URL to the assets.Context() method. If you prefer using the value from an environment variable, you can use assets.NewContext() without any parameters. Provide your API token as the second parameter to the assets.Context() method. (Or again, have it picked up automatically by the assets.NewContext() method.) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. Don't forget to give permissions If you want to be able to access existing metadata with an API token, don't forget that you need to assign one or more personas to the API token that grant it access to metadata. Now that you have an SDK installed and configured, you are ready to code! Before we jump straight to code, though, let's first introduce some key concepts in Atlan: What is an asset? In Atlan, we refer to all objects that provide context to your data as assets . Each type of asset in Atlan has a set of: Properties , such as: Certificates Announcements Properties , such as: Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table In an object-oriented programming sense, think of an asset as an instance of a class. The structure of an asset (the class itself, in this analogy) is defined by something called a type definition , but that's for another day. So as you can see: There are many different kinds of assets: tables, columns, schemas, databases, business intelligence dashboards, reports, and so on. Assets inter-relate with each other: a table has a parent schema and child columns, a schema has a parent database and child tables, and so on. Different kinds of assets have some common properties (like certificates) and other properties that are unique to that kind of asset (like a columnCount that only exists on tables, not on schemas or databases). When you know the asset When you already know which asset you want to retrieve, you can read it from Atlan using one of its identifiers . We'll discuss these in more detail as part of updates, but for now you can think of them as: is a primary key for an asset: completely unique, but meaningless by itself is a business key for an asset: unique for a given kind of asset, and interpretable You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the asset.get_by_guid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the asset.get_by_qualified_name() method on the Atlan client, providing the type of asset you expect to retrieve and its qualified_name . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the assets.GetByGuid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the assets.GetByQualifiedName() method on the Atlan client, providing the type of asset you expect to retrieve and its qualifiedName . (Each asset type is its own unique class in the SDK.) Note that the response is strongly typed: If you are retrieving a table, you will get a table back (as long as it exists). You do not need to figure out what properties or relationships exist on a table - the Table class defines them for for you already. In any modern IDE, this means you have type-ahead support for retrieving the properties and relationships from the table variable. You can also refer to the types reference in this portal for full details of every kind of asset. Retrieval by identifier can be more costly than you might expect Even though you are retrieving an asset by an identifier, this can be more costly than you might expect. Retrieving an asset in this way will: Retrieve all its properties and their values Retrieve all its relationships Imagine the asset you are retrieving has 100's or 1000's of these. If you only care about its certificate and any owners, you will be retrieving far more information than you need. When you need to find it first For example, imagine you want to find all tables named MY_TABLE : You can then run the request using Execute() . For example, if you want to know the certificate of the asset you only need to tack that onto the query: Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many include_on_results calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can include as many attributes in IncludeOnResults as you want to specify the properties and relationships you want to retrieve for matching assets. Also gives the best performance If all you want to do is check or report on metadata, you should have a starting point from the information above. Or, now that you've found an asset of interest, maybe you want to update the asset with additional metadata ? Once again, before we jump to code, let's first understand some key concepts about how Atlan handles updates: Importance of identifiers Most operations on assets are upserts , that is, they could either create (insert) or update a given asset. How do you know which is going to happen? To answer this question, you need to understand how Atlan uniquely identifies each asset. Recall earlier we discussed asset's different identifiers in Atlan . Every asset in Atlan has at least the following two unique identifiers. These are both mandatory for every asset, so no asset can exist without these: Atlan uses globally-unique identifiers (GUIDs) to uniquely identify each asset, globally . They look something like this: As the name implies, GUIDs are: Globally unique (across all systems). Generated in a way that makes it nearly impossible for anything else to ever generate that same ID. 2 Note that this means the GUID itself is not : Meaningful or capable of being interpreted in any way Atlan uses qualifiedName s to uniquely identify assets based on their characteristics. They look something like this: Qualified names are not : Globally unique (across all systems). Instead, they are: Consistently constructed in a meaningful way, making it possible for them to be reconstructed. Note that this means the qualifiedName is: Meaningful and capable of being interpreted How these impact updates Since they are truly unique, operations that include a GUID will only update an asset, not create one. Conversely, operations that take a qualifiedName can: Create an asset, if no exactly-matching qualifiedName is found in Atlan. Update an asset, if an exact-match for the qualifiedName is found in Atlan. These operations also require a typeName , so that if creation does occur the correct type of asset is created. Unintended consequences of this behavior Be careful when using operations with only the qualifiedName . You may end up creating assets when you were only expecting them to be updated or to fail if they did not already exist. This is particularly true when you do not give the exact, case-sensitive qualifiedName of an asset. a\/b\/c\/d is not the same as a\/B\/c\/d when it comes to qualifiedName s. Perhaps this leaves you wondering: why have a qualifiedName at all? The qualifiedName 's purpose is to identify what is a unique asset. Many different tools might all have information about that asset. Having a common \"identity\" means that many different systems can each independently construct its identifier the same way. If a crawler gets table details from Snowflake it can upsert based on those identity characteristics in Atlan. The crawler will not create duplicate tables every time it runs. This gives idempotency. Looker knows the same identity characteristics for the Snowflake tables and columns. So if you get details from Looker about the tables it uses for reporting, you can link them together in lineage. (Looker can construct the same identifier for the table as Snowflake itself.) These characteristics are not possible using GUIDs alone. Limit to changes only Now that you understand the nuances of identifiers, let's look at how you can update metadata in Atlan. In general, you only need to send changes to Atlan. You do not need to send an entire asset each time you want to make changes to it. For example, imagine you want to mark a table as certified but do not want to change anything else (its name, description, owner details, and so on): You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualified_name . Using the updater() class method on any asset type, you pass in (typically) the qualified_name and name of the asset. You can then add onto the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to client.asset.save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the Updater() method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns an object into which you can then place any updates. You can place into the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to .Save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. Atlan will handle idempotency By sending only the changes you want to apply, Atlan can make idempotent updates. Atlan will only attempt to update the asset with the changes you send. Atlan leaves any existing metadata on the asset as-is. If the asset already has the metadata values you are sending, Atlan does nothing. It will not even update audit details like the last update timestamp, and is thus idempotent. What if you want to make changes to many assets, as efficiently as possible? Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Where to go from here Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Real code samples our customers use to solve particular use cases. Review live samples Events Delve deep into the details of the events Atlan triggers. Learn more about events Delve deep into the details of the events Atlan triggers. Learn more about events Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†©"}
{"url":"https:\/\/developer.atlan.com\/snippets\/advanced-examples\/search\/","title":"Searching for assets - Developer","text":"Build the query 0.1.0 1.0.0 2.0.0 You can start building a query across all assets using the select() method on the assets member of any client. You can chain as many conditions as you want: where() is mandatory inclusion whereNot() is mandatory exclusion whereSome() for conditions where some of them must match You can start building a query across all assets using the select() method on the assets member of any client. You can chain as many conditions as you want: where() is mandatory inclusion whereNot() is mandatory exclusion whereSome() for conditions where some of them must match This helper provides a query that ensures results are active (not archived) assets. Equivalent Elastic query Query beActive = TermQuery . of ( m -> m . field ( \"__state\" ) . value ( AtlanStatus . ACTIVE . getValue ())) . _toQuery (); This helper provides a query that ensures results are active (not archived) assets. This helper provides a query that ensures results are active (not archived) assets. You can start building a query across all assets using the select() method on the assets member of any client. You can chain as many mandatory ( where() ) conditions, mandatory exclusion ( whereNot() ) conditions, and set of conditions some of which must match ( whereSome() ) as you want. This helper provides a query that ensures results are active (not archived) assets. Equivalent Elastic query val beActive = TermQuery . of ( m -> m . field ( \"__state\" ) . value ( AtlanStatus . ACTIVE . getValue ())) . _toQuery () This helper provides a query that ensures results are active (not archived) assets. This helper provides a query that ensures results are active (not archived) assets. A bool query combines together multiple conditions. A filter clause exactly matches all of the conditions, without scoring (so can be slightly faster than other scoring-based combination mechanisms). Term queries are generally used to exactly match values. The __state field will match the status of an asset in Atlan. So in this example you will only match assets that are currently ACTIVE (not archived or soft-deleted). You will also only match assets that are of a specific type, since __typeName.keyword will match the type of asset. Note these names do not exactly match attribute names Build the request 0.0.17 1.0.0 1.1.0 The number of results to include (per page). You can chain as many attributes to include on each related asset to each result. Since we are returning anchor relationships, this will ensure that the certificateStatus of those related glossaries is also included in each result. The number of results to include (per page). You can chain as many attributes to include on each related asset to each result. Since we are returning anchor relationships, this will ensure that the certificate_status of those related glossaries is also included in each result. The number of results to include (per page). You can chain as many attributes to include on each related asset to each result. Since we are returning anchor relationships, this will ensure that the certificateStatus of those related glossaries is also included in each result. The number of results to include (per page). You can chain as many attributes to include on each related asset to each result. Since we are returning anchor relationships, this will ensure that the certificateStatus of those related glossaries is also included in each result. A query should always be defined within the dsl portion of the request. In addition to the query, you can specify from and size parameters for pagination. The query itself should be provided within the query portion of the dsl . Here you would use the query body provided in the earlier step. You must set track_total_hits to true if you want an exact count of the number of results (in particular for pagination). The list of attributes to include on each relationship that is included in each result. Since we are returning anchor relationships, this will ensure that the certificateStatus of those related glossaries is also included in each result. (Optional) Build request directly from JSON A query should always be defined within the dsl portion of the request. In addition to the query, you can specify from and size parameters for pagination. The query itself should be provided within the query portion of the dsl . Here you would use the query body provided in the earlier step. You must set track_total_hits to true if you want an exact count of the number of results (in particular for pagination). The list of attributes to include on each relationship that is included in each result. Since we are returning anchor relationships, this will ensure that the certificateStatus of those related glossaries is also included in each result. 0.1.0 1.4.0 4.0.0 The getApproximateCount() method gives the total number of results overall (not restricted by page). The count property gives the total number of results overall (not restricted by page). The .approximateCount member gives the total number of results overall (not restricted by page). The ApproximateCount property gives the total number of results overall (not restricted by page). If an error occurs during iteration (e.g: a failed API request), it is sent to the errIter channel. The iteration stops, and you can handle the error accordingly. Implicit in the previous step Iterate through results One page of results 0.1.0 1.0.0 1.1.0 To iterate through one page of results, loop through the list of assets: The page of results itself can be accessed through the getAssets() method on the response. You can then iterate through these results from a single page. You can iterate through the results from a single page. The page of results itself can be accessed through the .assets member on the response. You can then iterate through these results from a single page. You can iterate through the results from a single page. You can filter the asset based on the typename and perform operations on the asset. Each object in entities is a matching asset Each item in the entities array of the response will give details about a matching asset. Multiple pages of results 0.0.17 1.0.0 1.1.0 To iterate through multiple pages of results: You can simply iterate over the reponse itself. This will lazily load and loop through each page of results until the loop finishes or you break out of it. (You could also use response.forEach() , which uses the same iteratable-based implementation behind-the-scenes.) Alternatively, you can also stream the results direct from the response. This will also lazily load and loop through each page of results. Can be chained without creating a request in-between You can actually chain the stream() method directly onto the end of your query and request construction, without creating a request or response object in-between. Alternatively, you can also stream the results direct from the response. This will also lazily load and loop through each page of results. Can be chained without creating a request in-between You can actually chain the stream() method directly onto the end of your query and request construction, without creating a request or response object in-between. With streaming, you can apply your own limits to the maximum number of results you want to process. Independent of page size Note that this is independent of page size. You could page through results 50 at a time, but only process a maximum of 100 total results this way. Since the results are lazily-loaded when streaming, only the first two pages of results would be retrieved in such a scenario. With streaming, you can apply your own limits to the maximum number of results you want to process. Independent of page size Note that this is independent of page size. You could page through results 50 at a time, but only process a maximum of 100 total results this way. Since the results are lazily-loaded when streaming, only the first two pages of results would be retrieved in such a scenario. You can also apply your own logical filters to the results. Push-down as much as you can to the query You should of course push-down as many of the filters as you can to the query itself, but if you have a particular complex check to apply that cannot be encoded in the query this can be a useful secondary filter over the results. You can also apply your own logical filters to the results. Push-down as much as you can to the query You should of course push-down as many of the filters as you can to the query itself, but if you have a particular complex check to apply that cannot be encoded in the query this can be a useful secondary filter over the results. The forEach() on the resulting stream will then apply whatever actions you want with the results that come through. The forEach() on the resulting stream will then apply whatever actions you want with the results that come through. The current_page() method returns a list of the assets for the current page. If there are none then an empty list will be returned. Iterate through the assets in the current page. The next_pages() method retrieves the next page of results and return True if more assets are available and False if they are not. Break out of the While loop if no more assets are available. This will iterate through all the results without the need to be concerned with pages. Iterating over results produces a Generator This means that results are retrieved from the backend a page at time. This also means that you can only iterate over the results once. This will iterate through all the results without the need to be concerned with pages. Iterating over results produces a Generator This means that results are retrieved from the backend a page at time. This also means that you can only iterate over the results once. You can simply iterate over the reponse itself. This will lazily load and loop through each page of results until the loop finishes or you break out of it. (You could also use response.forEach{ } , which uses the same iteratable-based implementation behind-the-scenes.) Alternatively, you can also stream the results direct from the response. This will also lazily load and loop through each page of results. Can be chained without creating a request in-between You can actually chain the stream() method directly onto the end of your query and request construction, without creating a request or response object in-between. Alternatively, you can also stream the results direct from the response. This will also lazily load and loop through each page of results. Can be chained without creating a request in-between You can actually chain the stream() method directly onto the end of your query and request construction, without creating a request or response object in-between. With streaming, you can apply your own limits to the maximum number of results you want to process. Independent of page size Note that this is independent of page size. You could page through results 50 at a time, but only process a maximum of 100 total results this way. Since the results are lazily-loaded when streaming, only the first two pages of results would be retrieved in such a scenario. With streaming, you can apply your own limits to the maximum number of results you want to process. Independent of page size Note that this is independent of page size. You could page through results 50 at a time, but only process a maximum of 100 total results this way. Since the results are lazily-loaded when streaming, only the first two pages of results would be retrieved in such a scenario. You can also apply your own logical filters to the results. Push-down as much as you can to the query You should of course push-down as many of the filters as you can to the query itself, but if you have a particular complex check to apply that cannot be encoded in the query this can be a useful secondary filter over the results. You can also apply your own logical filters to the results. Push-down as much as you can to the query You should of course push-down as many of the filters as you can to the query itself, but if you have a particular complex check to apply that cannot be encoded in the query this can be a useful secondary filter over the results. The forEach{ } on the resulting stream will then apply whatever actions you want with the results that come through. The forEach{ } on the resulting stream will then apply whatever actions you want with the results that come through. If an error occurs during iteration (e.g: a failed API request), it is sent to the errIter channel. The iteration stops, and you can handle the error accordingly. Use this query string from the response to start building a new query using the same logic. Add the page size to the from parameter embedded in that query string, to give the starting point for the next page of results. Re-include any attributes or relationAttributes from the query string into the new query. Send this new query to retrieve the next page of results."}
{"url":"https:\/\/developer.atlan.com\/getting-started\/#updating-metadata","title":"Introductory walkthrough - Developer","text":"An introductory walkthrough You might also like our Atlan Platform Essentials certification . Not sure where to start? Allow us to introduce Atlan development through example. 1 We strongly recommend using one of our SDKs to simplify the development process. As a first step, set one up: The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include logback as a simple binding mechanism to send any logging information out to your console (standard out). Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on PyPI . You can use pip to install it as follows: Provide two values to create an Atlan client: Provide your Atlan tenant URL to the base_url parameter. (You can also do this through environment variables .) Provide your API token to the api_key parameter. (You can also do this through environment variables .) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include slf4j-simple as a simple binding mechanism to send any logging information out to your console (standard out), along with the kotlin-logging-jvm microutil. Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on GitHub , ready to be included in your project: Provide two values to set up connectivity to Atlan: Provide your Atlan tenant URL to the assets.Context() method. If you prefer using the value from an environment variable, you can use assets.NewContext() without any parameters. Provide your API token as the second parameter to the assets.Context() method. (Or again, have it picked up automatically by the assets.NewContext() method.) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. Don't forget to give permissions If you want to be able to access existing metadata with an API token, don't forget that you need to assign one or more personas to the API token that grant it access to metadata. Now that you have an SDK installed and configured, you are ready to code! Before we jump straight to code, though, let's first introduce some key concepts in Atlan: What is an asset? In Atlan, we refer to all objects that provide context to your data as assets . Each type of asset in Atlan has a set of: Properties , such as: Certificates Announcements Properties , such as: Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table In an object-oriented programming sense, think of an asset as an instance of a class. The structure of an asset (the class itself, in this analogy) is defined by something called a type definition , but that's for another day. So as you can see: There are many different kinds of assets: tables, columns, schemas, databases, business intelligence dashboards, reports, and so on. Assets inter-relate with each other: a table has a parent schema and child columns, a schema has a parent database and child tables, and so on. Different kinds of assets have some common properties (like certificates) and other properties that are unique to that kind of asset (like a columnCount that only exists on tables, not on schemas or databases). When you know the asset When you already know which asset you want to retrieve, you can read it from Atlan using one of its identifiers . We'll discuss these in more detail as part of updates, but for now you can think of them as: is a primary key for an asset: completely unique, but meaningless by itself is a business key for an asset: unique for a given kind of asset, and interpretable You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the asset.get_by_guid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the asset.get_by_qualified_name() method on the Atlan client, providing the type of asset you expect to retrieve and its qualified_name . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the assets.GetByGuid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the assets.GetByQualifiedName() method on the Atlan client, providing the type of asset you expect to retrieve and its qualifiedName . (Each asset type is its own unique class in the SDK.) Note that the response is strongly typed: If you are retrieving a table, you will get a table back (as long as it exists). You do not need to figure out what properties or relationships exist on a table - the Table class defines them for for you already. In any modern IDE, this means you have type-ahead support for retrieving the properties and relationships from the table variable. You can also refer to the types reference in this portal for full details of every kind of asset. Retrieval by identifier can be more costly than you might expect Even though you are retrieving an asset by an identifier, this can be more costly than you might expect. Retrieving an asset in this way will: Retrieve all its properties and their values Retrieve all its relationships Imagine the asset you are retrieving has 100's or 1000's of these. If you only care about its certificate and any owners, you will be retrieving far more information than you need. When you need to find it first For example, imagine you want to find all tables named MY_TABLE : You can then run the request using Execute() . For example, if you want to know the certificate of the asset you only need to tack that onto the query: Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many include_on_results calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can include as many attributes in IncludeOnResults as you want to specify the properties and relationships you want to retrieve for matching assets. Also gives the best performance If all you want to do is check or report on metadata, you should have a starting point from the information above. Or, now that you've found an asset of interest, maybe you want to update the asset with additional metadata ? Once again, before we jump to code, let's first understand some key concepts about how Atlan handles updates: Importance of identifiers Most operations on assets are upserts , that is, they could either create (insert) or update a given asset. How do you know which is going to happen? To answer this question, you need to understand how Atlan uniquely identifies each asset. Recall earlier we discussed asset's different identifiers in Atlan . Every asset in Atlan has at least the following two unique identifiers. These are both mandatory for every asset, so no asset can exist without these: Atlan uses globally-unique identifiers (GUIDs) to uniquely identify each asset, globally . They look something like this: As the name implies, GUIDs are: Globally unique (across all systems). Generated in a way that makes it nearly impossible for anything else to ever generate that same ID. 2 Note that this means the GUID itself is not : Meaningful or capable of being interpreted in any way Atlan uses qualifiedName s to uniquely identify assets based on their characteristics. They look something like this: Qualified names are not : Globally unique (across all systems). Instead, they are: Consistently constructed in a meaningful way, making it possible for them to be reconstructed. Note that this means the qualifiedName is: Meaningful and capable of being interpreted How these impact updates Since they are truly unique, operations that include a GUID will only update an asset, not create one. Conversely, operations that take a qualifiedName can: Create an asset, if no exactly-matching qualifiedName is found in Atlan. Update an asset, if an exact-match for the qualifiedName is found in Atlan. These operations also require a typeName , so that if creation does occur the correct type of asset is created. Unintended consequences of this behavior Be careful when using operations with only the qualifiedName . You may end up creating assets when you were only expecting them to be updated or to fail if they did not already exist. This is particularly true when you do not give the exact, case-sensitive qualifiedName of an asset. a\/b\/c\/d is not the same as a\/B\/c\/d when it comes to qualifiedName s. Perhaps this leaves you wondering: why have a qualifiedName at all? The qualifiedName 's purpose is to identify what is a unique asset. Many different tools might all have information about that asset. Having a common \"identity\" means that many different systems can each independently construct its identifier the same way. If a crawler gets table details from Snowflake it can upsert based on those identity characteristics in Atlan. The crawler will not create duplicate tables every time it runs. This gives idempotency. Looker knows the same identity characteristics for the Snowflake tables and columns. So if you get details from Looker about the tables it uses for reporting, you can link them together in lineage. (Looker can construct the same identifier for the table as Snowflake itself.) These characteristics are not possible using GUIDs alone. Limit to changes only Now that you understand the nuances of identifiers, let's look at how you can update metadata in Atlan. In general, you only need to send changes to Atlan. You do not need to send an entire asset each time you want to make changes to it. For example, imagine you want to mark a table as certified but do not want to change anything else (its name, description, owner details, and so on): You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualified_name . Using the updater() class method on any asset type, you pass in (typically) the qualified_name and name of the asset. You can then add onto the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to client.asset.save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the Updater() method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns an object into which you can then place any updates. You can place into the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to .Save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. Atlan will handle idempotency By sending only the changes you want to apply, Atlan can make idempotent updates. Atlan will only attempt to update the asset with the changes you send. Atlan leaves any existing metadata on the asset as-is. If the asset already has the metadata values you are sending, Atlan does nothing. It will not even update audit details like the last update timestamp, and is thus idempotent. What if you want to make changes to many assets, as efficiently as possible? Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Where to go from here Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Real code samples our customers use to solve particular use cases. Review live samples Events Delve deep into the details of the events Atlan triggers. Learn more about events Delve deep into the details of the events Atlan triggers. Learn more about events Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†©"}
{"url":"https:\/\/developer.atlan.com\/snippets\/advanced-examples\/update\/","title":"Updating an asset - Developer","text":"Updating an asset All objects in the SDK that you can update within Atlan implement the builder pattern. This allows you to progressively build-up the object you want to update. In addition, each object provides a method that takes the minimal set of required fields to update that asset , when it already exists in Atlan. Include only your intended changes, nothing more When enriching an asset in Atlan, you only need to specify the information you want to change. Any information you do not include in your update will be left untouched on the asset in Atlan. This way you do not need to: try to reproduce the complete asset in your request to do targeted updates to specific attributes worry about other changes that may be made to the asset in parallel to the changes you will be making to the asset Build minimal object needed For example, to update a glossary term we need to provide the qualifiedName and name of the term, and the GUID of the glossary in which it exists: The name of the existing term. This must match exactly (case-sensitive). The GUID of the glossary in which the term exists. The name of the existing term. This must match exactly (case-sensitive). The GUID of the glossary in which the term exists. The name of the existing term. This must match exactly (case-sensitive). The GUID of the glossary in which the term exists. Implicit in the API calls below There is nothing specific to do for this step when using the raw APIs â€” constructing the object is simply what you place in the payload of the API calls in the steps below. Since you should only include your intended changes, and nothing more, the SDKs provide a convenience method to reduce an asset down to its minimal properties. You should use this method to trim an object in your code down to a starting point for the changes you want to make to that asset: Assuming you have an existing asset in a variable called existing , you can call trimToRequired() to reduce it to a builder with the minimal properties needed to update that asset. Assuming you have an existing asset in a variable called existing , you can call trim_to_required() to reduce it to an object with the minimal properties needed to update that asset. Enrich before updating The term object now has the minimal required information for Atlan to update it. Without any additional enrichment, though, there isn't really anything to update. So first you should enrich the object: We'll create an object we can take actions on from this updater. In this example, we're adding a certificate to the object. Note that you can chain any number of enrichments together. Here we are also adding an announcement to the asset. To persist the enrichment back to the object, we must build() the builder. Assign the result back Remember to assign the result of the build() operation back to your original object. Otherwise the result is not persisted back into any variable! (In this case we're assigning to the term variable back on line 5.) To persist the enrichment back to the object, we must build() the builder. Assign the result back Remember to assign the result of the build() operation back to your original object. Otherwise the result is not persisted back into any variable! (In this case we're assigning to the term variable back on line 5.) In this example, we're adding a certificate to the object. In this example, we're adding an announcement to the object. We'll create an object we can take actions on from this updater. In this example, we're adding a certificate to the object. Note that you can chain any number of enrichments together. Here we are also adding an announcement to the asset. To persist the enrichment back to the object, we must build() the builder. Assign the result back Remember to assign the result of the build() operation back to your original object. Otherwise the result is not persisted back into any variable! (In this case we're assigning to the term variable back on line 5.) To persist the enrichment back to the object, we must build() the builder. Assign the result back Remember to assign the result of the build() operation back to your original object. Otherwise the result is not persisted back into any variable! (In this case we're assigning to the term variable back on line 5.) Implicit in the API calls below There is nothing specific to do for this step when using the raw APIs â€” constructing the object is simply what you place in the payload of the API calls in the steps below. Update the asset from the object You can then actually update the object in Atlan 1 : The save() method will either update an existing asset (if Atlan already has a term with the same name and qualifiedName in the same glossary) or create a new asset (if Atlan does not have a term with the same name in the same glossary). Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can distinguish what was created or updated: getCreatedAssets() lists assets that were created getUpdatedAssets() lists assets that were updated Note that the save() method always returns objects of type Asset , though. You can distinguish what was created or updated: getCreatedAssets() lists assets that were created getUpdatedAssets() lists assets that were updated Note that the save() method always returns objects of type Asset , though. The Asset class is a superclass of all assets. So we need to cast to more specific types (like GlossaryTerm ) after verifying the object that was actually returned. The Asset class is a superclass of all assets. So we need to cast to more specific types (like GlossaryTerm ) after verifying the object that was actually returned. The updateMergingCM() method will only update an existing asset (if Atlan already has an asset of the same type with the same name qualifiedName ). Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Depending on the update behavior you want, you could also use: updateMergingCM(false) to only overwrite any custom metadata provided in your update (leaving anything you don't provide untouched on the existing asset), while leaving any Atlan tags on the existing asset untouched updateMergingCM(true) to only overwrite any custom metadata provided in your update (leaving anything you don't provide untouched on the existing asset), while replacing any Atlan tags on the existing asset updateReplacingCM(false) to overwrite all custom metadata on the existing asset with what you're providing in your update, while leaving any Atlan tags on the existing asset untouched updateReplacingCM(true) to overwrite all custom metadata on the existing asset with what you're providing in your update, while replacing any Atlan tags on the existing asset The updateMergingCM() method will only update an existing asset (if Atlan already has an asset of the same type with the same name qualifiedName ). Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Depending on the update behavior you want, you could also use: updateMergingCM(false) to only overwrite any custom metadata provided in your update (leaving anything you don't provide untouched on the existing asset), while leaving any Atlan tags on the existing asset untouched updateMergingCM(true) to only overwrite any custom metadata provided in your update (leaving anything you don't provide untouched on the existing asset), while replacing any Atlan tags on the existing asset updateReplacingCM(false) to overwrite all custom metadata on the existing asset with what you're providing in your update, while leaving any Atlan tags on the existing asset untouched updateReplacingCM(true) to overwrite all custom metadata on the existing asset with what you're providing in your update, while replacing any Atlan tags on the existing asset You can distinguish what was created or updated: getUpdatedAssets() lists assets that were updated Note that the update. () methods always returns objects of type Asset , though. You can distinguish what was created or updated: getUpdatedAssets() lists assets that were updated Note that the update. () methods always returns objects of type Asset , though. The Asset class is a superclass of all assets. So we need to cast to more specific types (like GlossaryTerm ) after verifying the object that was actually returned. The Asset class is a superclass of all assets. So we need to cast to more specific types (like GlossaryTerm ) after verifying the object that was actually returned. Since the update. () methods strictly update (and never create) an asset, if the asset you are trying to update does not exist the operation will throw a NotFoundException . Since the update. () methods strictly update (and never create) an asset, if the asset you are trying to update does not exist the operation will throw a NotFoundException . The save(term) method will either update an existing asset (if Atlan already has a term with the same name and qualifiedName in the same glossary) or (create a new asset, if Atlan does not have a term with the same name in the same glossary). You can distinguish what was created or updated: assets_created(asset_type = AtlasGlossaryType) returns a list assets of the specified type that were created. assets_updated(asset_type = AtlasGlossaryType) returns a list assets of the specified type that were updated. You can distinguish what was created or updated: assets_created(asset_type = AtlasGlossaryType) returns a list assets of the specified type that were created. assets_updated(asset_type = AtlasGlossaryType) returns a list assets of the specified type that were updated. If the returned list is not empty, get the term that was updated. If the returned list is not empty, get the term that was updated. The update_merging_cm() method will only update an existing asset (if Atlan already has an asset of the same type with the same name qualified_name ). Depending on the update behavior you want, you could also use: update_merging_cm(replace_atlan_tags=False) to only overwrite any custom metadata provided in your update (leaving anything you don't provide untouched on the existing asset), while leaving any Atlan tags on the existing asset untouched update_merging_cm(replace_atlan_tags=True) to only overwrite any custom metadata provided in your update (leaving anything you don't provide untouched on the existing asset), while replacing any Atlan tags on the existing asset update_replacing_cm(replace_atlan_tags=False) to overwrite all custom metadata on the existing asset with what you're providing in your update, while leaving any Atlan tags on the existing asset untouched update_replacing_cm(replace_atlan_tags=True) to overwrite all custom metadata on the existing asset with what you're providing in your update, while replacing any Atlan tags on the existing asset The update_merging_cm() method will only update an existing asset (if Atlan already has an asset of the same type with the same name qualified_name ). Depending on the update behavior you want, you could also use: update_merging_cm(replace_atlan_tags=False) to only overwrite any custom metadata provided in your update (leaving anything you don't provide untouched on the existing asset), while leaving any Atlan tags on the existing asset untouched update_merging_cm(replace_atlan_tags=True) to only overwrite any custom metadata provided in your update (leaving anything you don't provide untouched on the existing asset), while replacing any Atlan tags on the existing asset update_replacing_cm(replace_atlan_tags=False) to overwrite all custom metadata on the existing asset with what you're providing in your update, while leaving any Atlan tags on the existing asset untouched update_replacing_cm(replace_atlan_tags=True) to overwrite all custom metadata on the existing asset with what you're providing in your update, while replacing any Atlan tags on the existing asset You can distinguish what was created or updated: assets_updated(asset_type = AtlasGlossaryType) returns a list assets of the specified type that were updated. You can distinguish what was created or updated: assets_updated(asset_type = AtlasGlossaryType) returns a list assets of the specified type that were updated. Since the update. () methods strictly update (and never create) an asset, if the asset you are trying to update does not exist the operation will throw a NotFoundError . Since the update. () methods strictly update (and never create) an asset, if the asset you are trying to update does not exist the operation will throw a NotFoundError . The save() method will either update an existing asset (if Atlan already has a term with the same name and qualifiedName in the same glossary) or create a new asset (if Atlan does not have a term with the same name in the same glossary). Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can distinguish what was created or updated: createdAssets lists assets that were created updatedAssets lists assets that were updated Note that the save() method always returns objects of type Asset , though. You can distinguish what was created or updated: createdAssets lists assets that were created updatedAssets lists assets that were updated Note that the save() method always returns objects of type Asset , though. The Asset class is a superclass of all assets. So we need to cast to more specific types (like GlossaryTerm ) after verifying the object that was actually returned. The Asset class is a superclass of all assets. So we need to cast to more specific types (like GlossaryTerm ) after verifying the object that was actually returned. The updateMergingCM() method will only update an existing asset (if Atlan already has an asset of the same type with the same name qualifiedName ). Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Depending on the update behavior you want, you could also use: updateMergingCM(false) to only overwrite any custom metadata provided in your update (leaving anything you don't provide untouched on the existing asset), while leaving any Atlan tags on the existing asset untouched updateMergingCM(true) to only overwrite any custom metadata provided in your update (leaving anything you don't provide untouched on the existing asset), while replacing any Atlan tags on the existing asset updateReplacingCM(false) to overwrite all custom metadata on the existing asset with what you're providing in your update, while leaving any Atlan tags on the existing asset untouched updateReplacingCM(true) to overwrite all custom metadata on the existing asset with what you're providing in your update, while replacing any Atlan tags on the existing asset The updateMergingCM() method will only update an existing asset (if Atlan already has an asset of the same type with the same name qualifiedName ). Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Depending on the update behavior you want, you could also use: updateMergingCM(false) to only overwrite any custom metadata provided in your update (leaving anything you don't provide untouched on the existing asset), while leaving any Atlan tags on the existing asset untouched updateMergingCM(true) to only overwrite any custom metadata provided in your update (leaving anything you don't provide untouched on the existing asset), while replacing any Atlan tags on the existing asset updateReplacingCM(false) to overwrite all custom metadata on the existing asset with what you're providing in your update, while leaving any Atlan tags on the existing asset untouched updateReplacingCM(true) to overwrite all custom metadata on the existing asset with what you're providing in your update, while replacing any Atlan tags on the existing asset You can distinguish what was created or updated: getUpdatedAssets() lists assets that were updated Note that the update. () methods always returns objects of type Asset , though. You can distinguish what was created or updated: getUpdatedAssets() lists assets that were updated Note that the update. () methods always returns objects of type Asset , though. The Asset class is a superclass of all assets. So we need to cast to more specific types (like GlossaryTerm ) after verifying the object that was actually returned. The Asset class is a superclass of all assets. So we need to cast to more specific types (like GlossaryTerm ) after verifying the object that was actually returned. Since the update. () methods strictly update (and never create) an asset, if the asset you are trying to update does not exist the operation will throw a NotFoundException . Since the update. () methods strictly update (and never create) an asset, if the asset you are trying to update does not exist the operation will throw a NotFoundException . All assets must be wrapped in an entities array. You must provide the exact type name for the asset (case-sensitive). For a term, this is AtlasGlossaryTerm . You must provide the exact name of the existing asset (case-sensitive). (Unless you want to change its name, in which case you can provide the new name instead.) You must provide the exact qualifiedName of the existing asset (case-sensitive). Must exactly match the qualifiedName of an existing asset If this does not exactly match the qualifiedName of an existing asset, the API call will instead create a new asset rather than updating an existing one. You must provide the exact qualifiedName of the existing asset (case-sensitive). Must exactly match the qualifiedName of an existing asset If this does not exactly match the qualifiedName of an existing asset, the API call will instead create a new asset rather than updating an existing one. In this example, we're adding a certificate to the object. Note that you can include any number of enrichments together. Here we are also adding an announcement to the asset. Case-sensitive, exact match If you use a different capitalization or spelling for the qualifiedName , you may accidentally create a new asset rather than updating the existing one. 2 Remove information from an asset As mentioned in Enrich before updating section, only the information in your request will be updated on the object. But what if you want to remove some information that already exists on the asset in Atlan? We'll create an object we can take actions on from this updater. In this example, we'll remove any existing certificate from the object in Atlan. Note that you can chain any number of enrichments together. Here we are also removing any announcement from the asset. To persist the enrichment back to the object, we must build() the builder. Assign the result back Remember to assign the result of the build() operation back to your original object. Otherwise the result is not persisted back into any variable! (In this case we're assigning to the term variable back on line 5.) To persist the enrichment back to the object, we must build() the builder. Assign the result back Remember to assign the result of the build() operation back to your original object. Otherwise the result is not persisted back into any variable! (In this case we're assigning to the term variable back on line 5.) The save() method will either: Update an existing asset, if Atlan already has a term with the same name and qualifiedName in the same glossary. Create a new asset, if Atlan does not have a term with the same name in the same glossary. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The save() method will either: Update an existing asset, if Atlan already has a term with the same name and qualifiedName in the same glossary. Create a new asset, if Atlan does not have a term with the same name in the same glossary. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can distinguish what was created or updated: getCreatedAssets() lists assets that were created getUpdatedAssets() lists assets that were updated Note that the save() method always returns objects of type Asset , though. You can distinguish what was created or updated: getCreatedAssets() lists assets that were created getUpdatedAssets() lists assets that were updated Note that the save() method always returns objects of type Asset , though. The Asset class is a superclass of all assets. So we need to cast to more specific types (like GlossaryTerm ) after verifying the object that was actually returned. The Asset class is a superclass of all assets. So we need to cast to more specific types (like GlossaryTerm ) after verifying the object that was actually returned. In this example we will remove an existing certificate from any existing certificate from the object. In this example we will remove any existing announcement from the object. We'll create an object we can take actions on from this updater. In this example, we'll remove any existing certificate from the object in Atlan. Note that you can chain any number of enrichments together. Here we are also removing any announcement from the asset. To persist the enrichment back to the object, we must build() the builder. Assign the result back Remember to assign the result of the build() operation back to your original object. Otherwise the result is not persisted back into any variable! (In this case we're assigning to the term variable back on line 5.) To persist the enrichment back to the object, we must build() the builder. Assign the result back Remember to assign the result of the build() operation back to your original object. Otherwise the result is not persisted back into any variable! (In this case we're assigning to the term variable back on line 5.) The save() method will either: Update an existing asset, if Atlan already has a term with the same name and qualifiedName in the same glossary. Create a new asset, if Atlan does not have a term with the same name in the same glossary. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The save() method will either: Update an existing asset, if Atlan already has a term with the same name and qualifiedName in the same glossary. Create a new asset, if Atlan does not have a term with the same name in the same glossary. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can distinguish what was created or updated: createdAssets lists assets that were created updatedAssets lists assets that were updated Note that the save() method always returns objects of type Asset , though. You can distinguish what was created or updated: createdAssets lists assets that were created updatedAssets lists assets that were updated Note that the save() method always returns objects of type Asset , though. The Asset class is a superclass of all assets. So we need to cast to more specific types (like GlossaryTerm ) after verifying the object that was actually returned. The Asset class is a superclass of all assets. So we need to cast to more specific types (like GlossaryTerm ) after verifying the object that was actually returned. All assets must be wrapped in an entities array. You must provide the exact type name for the asset (case-sensitive). For a term, this is AtlasGlossaryTerm . You must provide the exact name of the existing asset (case-sensitive). (Unless you want to change its name, in which case you can provide the new name instead.) You must provide the exact qualifiedName of the existing asset (case-sensitive). Must exactly match the qualifiedName of an existing asset If this does not exactly match the qualifiedName of an existing asset, the API call will instead create a new asset rather than updating an existing one. You must provide the exact qualifiedName of the existing asset (case-sensitive). Must exactly match the qualifiedName of an existing asset If this does not exactly match the qualifiedName of an existing asset, the API call will instead create a new asset rather than updating an existing one. In this example, we're removing any existing certificate information from the object in Atlan (by sending null ). Note that you can include any number of enrichments together. Here we are also removing any announcement from the asset. Atlan automatically detects changes to determine whether to create or update an asset â€” see the Importance of identifiers for a more detailed explanation. To strictly update (and avoid creating) an asset, you must first look for the existing asset and only if found proceed with your update. When the SDKs provide such strict update functionality, this is what they are doing behind-the-scenes. Be aware that this will impact performance, so you should only do this where strictly necessary for your logic. â†© Atlan automatically detects changes to determine whether to create or update an asset â€” see the Importance of identifiers for a more detailed explanation. To strictly update (and avoid creating) an asset, you must first look for the existing asset and only if found proceed with your update. When the SDKs provide such strict update functionality, this is what they are doing behind-the-scenes. Be aware that this will impact performance, so you should only do this where strictly necessary for your logic. â†© This is because Atlan uses the exact qualifiedName to determine whether it should do an update. For a more detailed explanation, see the Importance of identifiers . â†© This is because Atlan uses the exact qualifiedName to determine whether it should do an update. For a more detailed explanation, see the Importance of identifiers . â†©"}
{"url":"https:\/\/developer.atlan.com\/getting-started\/#importance-of-identifiers","title":"Introductory walkthrough - Developer","text":"An introductory walkthrough You might also like our Atlan Platform Essentials certification . Not sure where to start? Allow us to introduce Atlan development through example. 1 We strongly recommend using one of our SDKs to simplify the development process. As a first step, set one up: The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include logback as a simple binding mechanism to send any logging information out to your console (standard out). Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on PyPI . You can use pip to install it as follows: Provide two values to create an Atlan client: Provide your Atlan tenant URL to the base_url parameter. (You can also do this through environment variables .) Provide your API token to the api_key parameter. (You can also do this through environment variables .) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include slf4j-simple as a simple binding mechanism to send any logging information out to your console (standard out), along with the kotlin-logging-jvm microutil. Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on GitHub , ready to be included in your project: Provide two values to set up connectivity to Atlan: Provide your Atlan tenant URL to the assets.Context() method. If you prefer using the value from an environment variable, you can use assets.NewContext() without any parameters. Provide your API token as the second parameter to the assets.Context() method. (Or again, have it picked up automatically by the assets.NewContext() method.) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. Don't forget to give permissions If you want to be able to access existing metadata with an API token, don't forget that you need to assign one or more personas to the API token that grant it access to metadata. Now that you have an SDK installed and configured, you are ready to code! Before we jump straight to code, though, let's first introduce some key concepts in Atlan: What is an asset? In Atlan, we refer to all objects that provide context to your data as assets . Each type of asset in Atlan has a set of: Properties , such as: Certificates Announcements Properties , such as: Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table In an object-oriented programming sense, think of an asset as an instance of a class. The structure of an asset (the class itself, in this analogy) is defined by something called a type definition , but that's for another day. So as you can see: There are many different kinds of assets: tables, columns, schemas, databases, business intelligence dashboards, reports, and so on. Assets inter-relate with each other: a table has a parent schema and child columns, a schema has a parent database and child tables, and so on. Different kinds of assets have some common properties (like certificates) and other properties that are unique to that kind of asset (like a columnCount that only exists on tables, not on schemas or databases). When you know the asset When you already know which asset you want to retrieve, you can read it from Atlan using one of its identifiers . We'll discuss these in more detail as part of updates, but for now you can think of them as: is a primary key for an asset: completely unique, but meaningless by itself is a business key for an asset: unique for a given kind of asset, and interpretable You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the asset.get_by_guid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the asset.get_by_qualified_name() method on the Atlan client, providing the type of asset you expect to retrieve and its qualified_name . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the assets.GetByGuid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the assets.GetByQualifiedName() method on the Atlan client, providing the type of asset you expect to retrieve and its qualifiedName . (Each asset type is its own unique class in the SDK.) Note that the response is strongly typed: If you are retrieving a table, you will get a table back (as long as it exists). You do not need to figure out what properties or relationships exist on a table - the Table class defines them for for you already. In any modern IDE, this means you have type-ahead support for retrieving the properties and relationships from the table variable. You can also refer to the types reference in this portal for full details of every kind of asset. Retrieval by identifier can be more costly than you might expect Even though you are retrieving an asset by an identifier, this can be more costly than you might expect. Retrieving an asset in this way will: Retrieve all its properties and their values Retrieve all its relationships Imagine the asset you are retrieving has 100's or 1000's of these. If you only care about its certificate and any owners, you will be retrieving far more information than you need. When you need to find it first For example, imagine you want to find all tables named MY_TABLE : You can then run the request using Execute() . For example, if you want to know the certificate of the asset you only need to tack that onto the query: Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many include_on_results calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can include as many attributes in IncludeOnResults as you want to specify the properties and relationships you want to retrieve for matching assets. Also gives the best performance If all you want to do is check or report on metadata, you should have a starting point from the information above. Or, now that you've found an asset of interest, maybe you want to update the asset with additional metadata ? Once again, before we jump to code, let's first understand some key concepts about how Atlan handles updates: Importance of identifiers Most operations on assets are upserts , that is, they could either create (insert) or update a given asset. How do you know which is going to happen? To answer this question, you need to understand how Atlan uniquely identifies each asset. Recall earlier we discussed asset's different identifiers in Atlan . Every asset in Atlan has at least the following two unique identifiers. These are both mandatory for every asset, so no asset can exist without these: Atlan uses globally-unique identifiers (GUIDs) to uniquely identify each asset, globally . They look something like this: As the name implies, GUIDs are: Globally unique (across all systems). Generated in a way that makes it nearly impossible for anything else to ever generate that same ID. 2 Note that this means the GUID itself is not : Meaningful or capable of being interpreted in any way Atlan uses qualifiedName s to uniquely identify assets based on their characteristics. They look something like this: Qualified names are not : Globally unique (across all systems). Instead, they are: Consistently constructed in a meaningful way, making it possible for them to be reconstructed. Note that this means the qualifiedName is: Meaningful and capable of being interpreted How these impact updates Since they are truly unique, operations that include a GUID will only update an asset, not create one. Conversely, operations that take a qualifiedName can: Create an asset, if no exactly-matching qualifiedName is found in Atlan. Update an asset, if an exact-match for the qualifiedName is found in Atlan. These operations also require a typeName , so that if creation does occur the correct type of asset is created. Unintended consequences of this behavior Be careful when using operations with only the qualifiedName . You may end up creating assets when you were only expecting them to be updated or to fail if they did not already exist. This is particularly true when you do not give the exact, case-sensitive qualifiedName of an asset. a\/b\/c\/d is not the same as a\/B\/c\/d when it comes to qualifiedName s. Perhaps this leaves you wondering: why have a qualifiedName at all? The qualifiedName 's purpose is to identify what is a unique asset. Many different tools might all have information about that asset. Having a common \"identity\" means that many different systems can each independently construct its identifier the same way. If a crawler gets table details from Snowflake it can upsert based on those identity characteristics in Atlan. The crawler will not create duplicate tables every time it runs. This gives idempotency. Looker knows the same identity characteristics for the Snowflake tables and columns. So if you get details from Looker about the tables it uses for reporting, you can link them together in lineage. (Looker can construct the same identifier for the table as Snowflake itself.) These characteristics are not possible using GUIDs alone. Limit to changes only Now that you understand the nuances of identifiers, let's look at how you can update metadata in Atlan. In general, you only need to send changes to Atlan. You do not need to send an entire asset each time you want to make changes to it. For example, imagine you want to mark a table as certified but do not want to change anything else (its name, description, owner details, and so on): You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualified_name . Using the updater() class method on any asset type, you pass in (typically) the qualified_name and name of the asset. You can then add onto the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to client.asset.save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the Updater() method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns an object into which you can then place any updates. You can place into the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to .Save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. Atlan will handle idempotency By sending only the changes you want to apply, Atlan can make idempotent updates. Atlan will only attempt to update the asset with the changes you send. Atlan leaves any existing metadata on the asset as-is. If the asset already has the metadata values you are sending, Atlan does nothing. It will not even update audit details like the last update timestamp, and is thus idempotent. What if you want to make changes to many assets, as efficiently as possible? Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Where to go from here Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Real code samples our customers use to solve particular use cases. Review live samples Events Delve deep into the details of the events Atlan triggers. Learn more about events Delve deep into the details of the events Atlan triggers. Learn more about events Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†©"}
{"url":"https:\/\/developer.atlan.com\/getting-started\/#guid_1","title":"Introductory walkthrough - Developer","text":"An introductory walkthrough You might also like our Atlan Platform Essentials certification . Not sure where to start? Allow us to introduce Atlan development through example. 1 We strongly recommend using one of our SDKs to simplify the development process. As a first step, set one up: The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include logback as a simple binding mechanism to send any logging information out to your console (standard out). Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on PyPI . You can use pip to install it as follows: Provide two values to create an Atlan client: Provide your Atlan tenant URL to the base_url parameter. (You can also do this through environment variables .) Provide your API token to the api_key parameter. (You can also do this through environment variables .) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include slf4j-simple as a simple binding mechanism to send any logging information out to your console (standard out), along with the kotlin-logging-jvm microutil. Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on GitHub , ready to be included in your project: Provide two values to set up connectivity to Atlan: Provide your Atlan tenant URL to the assets.Context() method. If you prefer using the value from an environment variable, you can use assets.NewContext() without any parameters. Provide your API token as the second parameter to the assets.Context() method. (Or again, have it picked up automatically by the assets.NewContext() method.) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. Don't forget to give permissions If you want to be able to access existing metadata with an API token, don't forget that you need to assign one or more personas to the API token that grant it access to metadata. Now that you have an SDK installed and configured, you are ready to code! Before we jump straight to code, though, let's first introduce some key concepts in Atlan: What is an asset? In Atlan, we refer to all objects that provide context to your data as assets . Each type of asset in Atlan has a set of: Properties , such as: Certificates Announcements Properties , such as: Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table In an object-oriented programming sense, think of an asset as an instance of a class. The structure of an asset (the class itself, in this analogy) is defined by something called a type definition , but that's for another day. So as you can see: There are many different kinds of assets: tables, columns, schemas, databases, business intelligence dashboards, reports, and so on. Assets inter-relate with each other: a table has a parent schema and child columns, a schema has a parent database and child tables, and so on. Different kinds of assets have some common properties (like certificates) and other properties that are unique to that kind of asset (like a columnCount that only exists on tables, not on schemas or databases). When you know the asset When you already know which asset you want to retrieve, you can read it from Atlan using one of its identifiers . We'll discuss these in more detail as part of updates, but for now you can think of them as: is a primary key for an asset: completely unique, but meaningless by itself is a business key for an asset: unique for a given kind of asset, and interpretable You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the asset.get_by_guid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the asset.get_by_qualified_name() method on the Atlan client, providing the type of asset you expect to retrieve and its qualified_name . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the assets.GetByGuid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the assets.GetByQualifiedName() method on the Atlan client, providing the type of asset you expect to retrieve and its qualifiedName . (Each asset type is its own unique class in the SDK.) Note that the response is strongly typed: If you are retrieving a table, you will get a table back (as long as it exists). You do not need to figure out what properties or relationships exist on a table - the Table class defines them for for you already. In any modern IDE, this means you have type-ahead support for retrieving the properties and relationships from the table variable. You can also refer to the types reference in this portal for full details of every kind of asset. Retrieval by identifier can be more costly than you might expect Even though you are retrieving an asset by an identifier, this can be more costly than you might expect. Retrieving an asset in this way will: Retrieve all its properties and their values Retrieve all its relationships Imagine the asset you are retrieving has 100's or 1000's of these. If you only care about its certificate and any owners, you will be retrieving far more information than you need. When you need to find it first For example, imagine you want to find all tables named MY_TABLE : You can then run the request using Execute() . For example, if you want to know the certificate of the asset you only need to tack that onto the query: Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many include_on_results calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can include as many attributes in IncludeOnResults as you want to specify the properties and relationships you want to retrieve for matching assets. Also gives the best performance If all you want to do is check or report on metadata, you should have a starting point from the information above. Or, now that you've found an asset of interest, maybe you want to update the asset with additional metadata ? Once again, before we jump to code, let's first understand some key concepts about how Atlan handles updates: Importance of identifiers Most operations on assets are upserts , that is, they could either create (insert) or update a given asset. How do you know which is going to happen? To answer this question, you need to understand how Atlan uniquely identifies each asset. Recall earlier we discussed asset's different identifiers in Atlan . Every asset in Atlan has at least the following two unique identifiers. These are both mandatory for every asset, so no asset can exist without these: Atlan uses globally-unique identifiers (GUIDs) to uniquely identify each asset, globally . They look something like this: As the name implies, GUIDs are: Globally unique (across all systems). Generated in a way that makes it nearly impossible for anything else to ever generate that same ID. 2 Note that this means the GUID itself is not : Meaningful or capable of being interpreted in any way Atlan uses qualifiedName s to uniquely identify assets based on their characteristics. They look something like this: Qualified names are not : Globally unique (across all systems). Instead, they are: Consistently constructed in a meaningful way, making it possible for them to be reconstructed. Note that this means the qualifiedName is: Meaningful and capable of being interpreted How these impact updates Since they are truly unique, operations that include a GUID will only update an asset, not create one. Conversely, operations that take a qualifiedName can: Create an asset, if no exactly-matching qualifiedName is found in Atlan. Update an asset, if an exact-match for the qualifiedName is found in Atlan. These operations also require a typeName , so that if creation does occur the correct type of asset is created. Unintended consequences of this behavior Be careful when using operations with only the qualifiedName . You may end up creating assets when you were only expecting them to be updated or to fail if they did not already exist. This is particularly true when you do not give the exact, case-sensitive qualifiedName of an asset. a\/b\/c\/d is not the same as a\/B\/c\/d when it comes to qualifiedName s. Perhaps this leaves you wondering: why have a qualifiedName at all? The qualifiedName 's purpose is to identify what is a unique asset. Many different tools might all have information about that asset. Having a common \"identity\" means that many different systems can each independently construct its identifier the same way. If a crawler gets table details from Snowflake it can upsert based on those identity characteristics in Atlan. The crawler will not create duplicate tables every time it runs. This gives idempotency. Looker knows the same identity characteristics for the Snowflake tables and columns. So if you get details from Looker about the tables it uses for reporting, you can link them together in lineage. (Looker can construct the same identifier for the table as Snowflake itself.) These characteristics are not possible using GUIDs alone. Limit to changes only Now that you understand the nuances of identifiers, let's look at how you can update metadata in Atlan. In general, you only need to send changes to Atlan. You do not need to send an entire asset each time you want to make changes to it. For example, imagine you want to mark a table as certified but do not want to change anything else (its name, description, owner details, and so on): You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualified_name . Using the updater() class method on any asset type, you pass in (typically) the qualified_name and name of the asset. You can then add onto the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to client.asset.save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the Updater() method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns an object into which you can then place any updates. You can place into the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to .Save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. Atlan will handle idempotency By sending only the changes you want to apply, Atlan can make idempotent updates. Atlan will only attempt to update the asset with the changes you send. Atlan leaves any existing metadata on the asset as-is. If the asset already has the metadata values you are sending, Atlan does nothing. It will not even update audit details like the last update timestamp, and is thus idempotent. What if you want to make changes to many assets, as efficiently as possible? Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Where to go from here Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Real code samples our customers use to solve particular use cases. Review live samples Events Delve deep into the details of the events Atlan triggers. Learn more about events Delve deep into the details of the events Atlan triggers. Learn more about events Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†©"}
{"url":"https:\/\/developer.atlan.com\/getting-started\/#fn%3A2","title":"Introductory walkthrough - Developer","text":"An introductory walkthrough You might also like our Atlan Platform Essentials certification . Not sure where to start? Allow us to introduce Atlan development through example. 1 We strongly recommend using one of our SDKs to simplify the development process. As a first step, set one up: The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include logback as a simple binding mechanism to send any logging information out to your console (standard out). Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on PyPI . You can use pip to install it as follows: Provide two values to create an Atlan client: Provide your Atlan tenant URL to the base_url parameter. (You can also do this through environment variables .) Provide your API token to the api_key parameter. (You can also do this through environment variables .) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include slf4j-simple as a simple binding mechanism to send any logging information out to your console (standard out), along with the kotlin-logging-jvm microutil. Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on GitHub , ready to be included in your project: Provide two values to set up connectivity to Atlan: Provide your Atlan tenant URL to the assets.Context() method. If you prefer using the value from an environment variable, you can use assets.NewContext() without any parameters. Provide your API token as the second parameter to the assets.Context() method. (Or again, have it picked up automatically by the assets.NewContext() method.) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. Don't forget to give permissions If you want to be able to access existing metadata with an API token, don't forget that you need to assign one or more personas to the API token that grant it access to metadata. Now that you have an SDK installed and configured, you are ready to code! Before we jump straight to code, though, let's first introduce some key concepts in Atlan: What is an asset? In Atlan, we refer to all objects that provide context to your data as assets . Each type of asset in Atlan has a set of: Properties , such as: Certificates Announcements Properties , such as: Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table In an object-oriented programming sense, think of an asset as an instance of a class. The structure of an asset (the class itself, in this analogy) is defined by something called a type definition , but that's for another day. So as you can see: There are many different kinds of assets: tables, columns, schemas, databases, business intelligence dashboards, reports, and so on. Assets inter-relate with each other: a table has a parent schema and child columns, a schema has a parent database and child tables, and so on. Different kinds of assets have some common properties (like certificates) and other properties that are unique to that kind of asset (like a columnCount that only exists on tables, not on schemas or databases). When you know the asset When you already know which asset you want to retrieve, you can read it from Atlan using one of its identifiers . We'll discuss these in more detail as part of updates, but for now you can think of them as: is a primary key for an asset: completely unique, but meaningless by itself is a business key for an asset: unique for a given kind of asset, and interpretable You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the asset.get_by_guid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the asset.get_by_qualified_name() method on the Atlan client, providing the type of asset you expect to retrieve and its qualified_name . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the assets.GetByGuid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the assets.GetByQualifiedName() method on the Atlan client, providing the type of asset you expect to retrieve and its qualifiedName . (Each asset type is its own unique class in the SDK.) Note that the response is strongly typed: If you are retrieving a table, you will get a table back (as long as it exists). You do not need to figure out what properties or relationships exist on a table - the Table class defines them for for you already. In any modern IDE, this means you have type-ahead support for retrieving the properties and relationships from the table variable. You can also refer to the types reference in this portal for full details of every kind of asset. Retrieval by identifier can be more costly than you might expect Even though you are retrieving an asset by an identifier, this can be more costly than you might expect. Retrieving an asset in this way will: Retrieve all its properties and their values Retrieve all its relationships Imagine the asset you are retrieving has 100's or 1000's of these. If you only care about its certificate and any owners, you will be retrieving far more information than you need. When you need to find it first For example, imagine you want to find all tables named MY_TABLE : You can then run the request using Execute() . For example, if you want to know the certificate of the asset you only need to tack that onto the query: Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many include_on_results calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can include as many attributes in IncludeOnResults as you want to specify the properties and relationships you want to retrieve for matching assets. Also gives the best performance If all you want to do is check or report on metadata, you should have a starting point from the information above. Or, now that you've found an asset of interest, maybe you want to update the asset with additional metadata ? Once again, before we jump to code, let's first understand some key concepts about how Atlan handles updates: Importance of identifiers Most operations on assets are upserts , that is, they could either create (insert) or update a given asset. How do you know which is going to happen? To answer this question, you need to understand how Atlan uniquely identifies each asset. Recall earlier we discussed asset's different identifiers in Atlan . Every asset in Atlan has at least the following two unique identifiers. These are both mandatory for every asset, so no asset can exist without these: Atlan uses globally-unique identifiers (GUIDs) to uniquely identify each asset, globally . They look something like this: As the name implies, GUIDs are: Globally unique (across all systems). Generated in a way that makes it nearly impossible for anything else to ever generate that same ID. 2 Note that this means the GUID itself is not : Meaningful or capable of being interpreted in any way Atlan uses qualifiedName s to uniquely identify assets based on their characteristics. They look something like this: Qualified names are not : Globally unique (across all systems). Instead, they are: Consistently constructed in a meaningful way, making it possible for them to be reconstructed. Note that this means the qualifiedName is: Meaningful and capable of being interpreted How these impact updates Since they are truly unique, operations that include a GUID will only update an asset, not create one. Conversely, operations that take a qualifiedName can: Create an asset, if no exactly-matching qualifiedName is found in Atlan. Update an asset, if an exact-match for the qualifiedName is found in Atlan. These operations also require a typeName , so that if creation does occur the correct type of asset is created. Unintended consequences of this behavior Be careful when using operations with only the qualifiedName . You may end up creating assets when you were only expecting them to be updated or to fail if they did not already exist. This is particularly true when you do not give the exact, case-sensitive qualifiedName of an asset. a\/b\/c\/d is not the same as a\/B\/c\/d when it comes to qualifiedName s. Perhaps this leaves you wondering: why have a qualifiedName at all? The qualifiedName 's purpose is to identify what is a unique asset. Many different tools might all have information about that asset. Having a common \"identity\" means that many different systems can each independently construct its identifier the same way. If a crawler gets table details from Snowflake it can upsert based on those identity characteristics in Atlan. The crawler will not create duplicate tables every time it runs. This gives idempotency. Looker knows the same identity characteristics for the Snowflake tables and columns. So if you get details from Looker about the tables it uses for reporting, you can link them together in lineage. (Looker can construct the same identifier for the table as Snowflake itself.) These characteristics are not possible using GUIDs alone. Limit to changes only Now that you understand the nuances of identifiers, let's look at how you can update metadata in Atlan. In general, you only need to send changes to Atlan. You do not need to send an entire asset each time you want to make changes to it. For example, imagine you want to mark a table as certified but do not want to change anything else (its name, description, owner details, and so on): You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualified_name . Using the updater() class method on any asset type, you pass in (typically) the qualified_name and name of the asset. You can then add onto the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to client.asset.save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the Updater() method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns an object into which you can then place any updates. You can place into the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to .Save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. Atlan will handle idempotency By sending only the changes you want to apply, Atlan can make idempotent updates. Atlan will only attempt to update the asset with the changes you send. Atlan leaves any existing metadata on the asset as-is. If the asset already has the metadata values you are sending, Atlan does nothing. It will not even update audit details like the last update timestamp, and is thus idempotent. What if you want to make changes to many assets, as efficiently as possible? Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Where to go from here Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Real code samples our customers use to solve particular use cases. Review live samples Events Delve deep into the details of the events Atlan triggers. Learn more about events Delve deep into the details of the events Atlan triggers. Learn more about events Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†©"}
{"url":"https:\/\/developer.atlan.com\/getting-started\/#qualifiedname_1","title":"Introductory walkthrough - Developer","text":"An introductory walkthrough You might also like our Atlan Platform Essentials certification . Not sure where to start? Allow us to introduce Atlan development through example. 1 We strongly recommend using one of our SDKs to simplify the development process. As a first step, set one up: The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include logback as a simple binding mechanism to send any logging information out to your console (standard out). Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on PyPI . You can use pip to install it as follows: Provide two values to create an Atlan client: Provide your Atlan tenant URL to the base_url parameter. (You can also do this through environment variables .) Provide your API token to the api_key parameter. (You can also do this through environment variables .) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include slf4j-simple as a simple binding mechanism to send any logging information out to your console (standard out), along with the kotlin-logging-jvm microutil. Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on GitHub , ready to be included in your project: Provide two values to set up connectivity to Atlan: Provide your Atlan tenant URL to the assets.Context() method. If you prefer using the value from an environment variable, you can use assets.NewContext() without any parameters. Provide your API token as the second parameter to the assets.Context() method. (Or again, have it picked up automatically by the assets.NewContext() method.) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. Don't forget to give permissions If you want to be able to access existing metadata with an API token, don't forget that you need to assign one or more personas to the API token that grant it access to metadata. Now that you have an SDK installed and configured, you are ready to code! Before we jump straight to code, though, let's first introduce some key concepts in Atlan: What is an asset? In Atlan, we refer to all objects that provide context to your data as assets . Each type of asset in Atlan has a set of: Properties , such as: Certificates Announcements Properties , such as: Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table In an object-oriented programming sense, think of an asset as an instance of a class. The structure of an asset (the class itself, in this analogy) is defined by something called a type definition , but that's for another day. So as you can see: There are many different kinds of assets: tables, columns, schemas, databases, business intelligence dashboards, reports, and so on. Assets inter-relate with each other: a table has a parent schema and child columns, a schema has a parent database and child tables, and so on. Different kinds of assets have some common properties (like certificates) and other properties that are unique to that kind of asset (like a columnCount that only exists on tables, not on schemas or databases). When you know the asset When you already know which asset you want to retrieve, you can read it from Atlan using one of its identifiers . We'll discuss these in more detail as part of updates, but for now you can think of them as: is a primary key for an asset: completely unique, but meaningless by itself is a business key for an asset: unique for a given kind of asset, and interpretable You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the asset.get_by_guid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the asset.get_by_qualified_name() method on the Atlan client, providing the type of asset you expect to retrieve and its qualified_name . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the assets.GetByGuid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the assets.GetByQualifiedName() method on the Atlan client, providing the type of asset you expect to retrieve and its qualifiedName . (Each asset type is its own unique class in the SDK.) Note that the response is strongly typed: If you are retrieving a table, you will get a table back (as long as it exists). You do not need to figure out what properties or relationships exist on a table - the Table class defines them for for you already. In any modern IDE, this means you have type-ahead support for retrieving the properties and relationships from the table variable. You can also refer to the types reference in this portal for full details of every kind of asset. Retrieval by identifier can be more costly than you might expect Even though you are retrieving an asset by an identifier, this can be more costly than you might expect. Retrieving an asset in this way will: Retrieve all its properties and their values Retrieve all its relationships Imagine the asset you are retrieving has 100's or 1000's of these. If you only care about its certificate and any owners, you will be retrieving far more information than you need. When you need to find it first For example, imagine you want to find all tables named MY_TABLE : You can then run the request using Execute() . For example, if you want to know the certificate of the asset you only need to tack that onto the query: Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many include_on_results calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can include as many attributes in IncludeOnResults as you want to specify the properties and relationships you want to retrieve for matching assets. Also gives the best performance If all you want to do is check or report on metadata, you should have a starting point from the information above. Or, now that you've found an asset of interest, maybe you want to update the asset with additional metadata ? Once again, before we jump to code, let's first understand some key concepts about how Atlan handles updates: Importance of identifiers Most operations on assets are upserts , that is, they could either create (insert) or update a given asset. How do you know which is going to happen? To answer this question, you need to understand how Atlan uniquely identifies each asset. Recall earlier we discussed asset's different identifiers in Atlan . Every asset in Atlan has at least the following two unique identifiers. These are both mandatory for every asset, so no asset can exist without these: Atlan uses globally-unique identifiers (GUIDs) to uniquely identify each asset, globally . They look something like this: As the name implies, GUIDs are: Globally unique (across all systems). Generated in a way that makes it nearly impossible for anything else to ever generate that same ID. 2 Note that this means the GUID itself is not : Meaningful or capable of being interpreted in any way Atlan uses qualifiedName s to uniquely identify assets based on their characteristics. They look something like this: Qualified names are not : Globally unique (across all systems). Instead, they are: Consistently constructed in a meaningful way, making it possible for them to be reconstructed. Note that this means the qualifiedName is: Meaningful and capable of being interpreted How these impact updates Since they are truly unique, operations that include a GUID will only update an asset, not create one. Conversely, operations that take a qualifiedName can: Create an asset, if no exactly-matching qualifiedName is found in Atlan. Update an asset, if an exact-match for the qualifiedName is found in Atlan. These operations also require a typeName , so that if creation does occur the correct type of asset is created. Unintended consequences of this behavior Be careful when using operations with only the qualifiedName . You may end up creating assets when you were only expecting them to be updated or to fail if they did not already exist. This is particularly true when you do not give the exact, case-sensitive qualifiedName of an asset. a\/b\/c\/d is not the same as a\/B\/c\/d when it comes to qualifiedName s. Perhaps this leaves you wondering: why have a qualifiedName at all? The qualifiedName 's purpose is to identify what is a unique asset. Many different tools might all have information about that asset. Having a common \"identity\" means that many different systems can each independently construct its identifier the same way. If a crawler gets table details from Snowflake it can upsert based on those identity characteristics in Atlan. The crawler will not create duplicate tables every time it runs. This gives idempotency. Looker knows the same identity characteristics for the Snowflake tables and columns. So if you get details from Looker about the tables it uses for reporting, you can link them together in lineage. (Looker can construct the same identifier for the table as Snowflake itself.) These characteristics are not possible using GUIDs alone. Limit to changes only Now that you understand the nuances of identifiers, let's look at how you can update metadata in Atlan. In general, you only need to send changes to Atlan. You do not need to send an entire asset each time you want to make changes to it. For example, imagine you want to mark a table as certified but do not want to change anything else (its name, description, owner details, and so on): You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualified_name . Using the updater() class method on any asset type, you pass in (typically) the qualified_name and name of the asset. You can then add onto the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to client.asset.save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the Updater() method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns an object into which you can then place any updates. You can place into the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to .Save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. Atlan will handle idempotency By sending only the changes you want to apply, Atlan can make idempotent updates. Atlan will only attempt to update the asset with the changes you send. Atlan leaves any existing metadata on the asset as-is. If the asset already has the metadata values you are sending, Atlan does nothing. It will not even update audit details like the last update timestamp, and is thus idempotent. What if you want to make changes to many assets, as efficiently as possible? Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Where to go from here Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Real code samples our customers use to solve particular use cases. Review live samples Events Delve deep into the details of the events Atlan triggers. Learn more about events Delve deep into the details of the events Atlan triggers. Learn more about events Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†©"}
{"url":"https:\/\/developer.atlan.com\/getting-started\/#limit-to-changes-only","title":"Introductory walkthrough - Developer","text":"An introductory walkthrough You might also like our Atlan Platform Essentials certification . Not sure where to start? Allow us to introduce Atlan development through example. 1 We strongly recommend using one of our SDKs to simplify the development process. As a first step, set one up: The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include logback as a simple binding mechanism to send any logging information out to your console (standard out). Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on PyPI . You can use pip to install it as follows: Provide two values to create an Atlan client: Provide your Atlan tenant URL to the base_url parameter. (You can also do this through environment variables .) Provide your API token to the api_key parameter. (You can also do this through environment variables .) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include slf4j-simple as a simple binding mechanism to send any logging information out to your console (standard out), along with the kotlin-logging-jvm microutil. Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on GitHub , ready to be included in your project: Provide two values to set up connectivity to Atlan: Provide your Atlan tenant URL to the assets.Context() method. If you prefer using the value from an environment variable, you can use assets.NewContext() without any parameters. Provide your API token as the second parameter to the assets.Context() method. (Or again, have it picked up automatically by the assets.NewContext() method.) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. Don't forget to give permissions If you want to be able to access existing metadata with an API token, don't forget that you need to assign one or more personas to the API token that grant it access to metadata. Now that you have an SDK installed and configured, you are ready to code! Before we jump straight to code, though, let's first introduce some key concepts in Atlan: What is an asset? In Atlan, we refer to all objects that provide context to your data as assets . Each type of asset in Atlan has a set of: Properties , such as: Certificates Announcements Properties , such as: Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table In an object-oriented programming sense, think of an asset as an instance of a class. The structure of an asset (the class itself, in this analogy) is defined by something called a type definition , but that's for another day. So as you can see: There are many different kinds of assets: tables, columns, schemas, databases, business intelligence dashboards, reports, and so on. Assets inter-relate with each other: a table has a parent schema and child columns, a schema has a parent database and child tables, and so on. Different kinds of assets have some common properties (like certificates) and other properties that are unique to that kind of asset (like a columnCount that only exists on tables, not on schemas or databases). When you know the asset When you already know which asset you want to retrieve, you can read it from Atlan using one of its identifiers . We'll discuss these in more detail as part of updates, but for now you can think of them as: is a primary key for an asset: completely unique, but meaningless by itself is a business key for an asset: unique for a given kind of asset, and interpretable You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the asset.get_by_guid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the asset.get_by_qualified_name() method on the Atlan client, providing the type of asset you expect to retrieve and its qualified_name . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the assets.GetByGuid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the assets.GetByQualifiedName() method on the Atlan client, providing the type of asset you expect to retrieve and its qualifiedName . (Each asset type is its own unique class in the SDK.) Note that the response is strongly typed: If you are retrieving a table, you will get a table back (as long as it exists). You do not need to figure out what properties or relationships exist on a table - the Table class defines them for for you already. In any modern IDE, this means you have type-ahead support for retrieving the properties and relationships from the table variable. You can also refer to the types reference in this portal for full details of every kind of asset. Retrieval by identifier can be more costly than you might expect Even though you are retrieving an asset by an identifier, this can be more costly than you might expect. Retrieving an asset in this way will: Retrieve all its properties and their values Retrieve all its relationships Imagine the asset you are retrieving has 100's or 1000's of these. If you only care about its certificate and any owners, you will be retrieving far more information than you need. When you need to find it first For example, imagine you want to find all tables named MY_TABLE : You can then run the request using Execute() . For example, if you want to know the certificate of the asset you only need to tack that onto the query: Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many include_on_results calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can include as many attributes in IncludeOnResults as you want to specify the properties and relationships you want to retrieve for matching assets. Also gives the best performance If all you want to do is check or report on metadata, you should have a starting point from the information above. Or, now that you've found an asset of interest, maybe you want to update the asset with additional metadata ? Once again, before we jump to code, let's first understand some key concepts about how Atlan handles updates: Importance of identifiers Most operations on assets are upserts , that is, they could either create (insert) or update a given asset. How do you know which is going to happen? To answer this question, you need to understand how Atlan uniquely identifies each asset. Recall earlier we discussed asset's different identifiers in Atlan . Every asset in Atlan has at least the following two unique identifiers. These are both mandatory for every asset, so no asset can exist without these: Atlan uses globally-unique identifiers (GUIDs) to uniquely identify each asset, globally . They look something like this: As the name implies, GUIDs are: Globally unique (across all systems). Generated in a way that makes it nearly impossible for anything else to ever generate that same ID. 2 Note that this means the GUID itself is not : Meaningful or capable of being interpreted in any way Atlan uses qualifiedName s to uniquely identify assets based on their characteristics. They look something like this: Qualified names are not : Globally unique (across all systems). Instead, they are: Consistently constructed in a meaningful way, making it possible for them to be reconstructed. Note that this means the qualifiedName is: Meaningful and capable of being interpreted How these impact updates Since they are truly unique, operations that include a GUID will only update an asset, not create one. Conversely, operations that take a qualifiedName can: Create an asset, if no exactly-matching qualifiedName is found in Atlan. Update an asset, if an exact-match for the qualifiedName is found in Atlan. These operations also require a typeName , so that if creation does occur the correct type of asset is created. Unintended consequences of this behavior Be careful when using operations with only the qualifiedName . You may end up creating assets when you were only expecting them to be updated or to fail if they did not already exist. This is particularly true when you do not give the exact, case-sensitive qualifiedName of an asset. a\/b\/c\/d is not the same as a\/B\/c\/d when it comes to qualifiedName s. Perhaps this leaves you wondering: why have a qualifiedName at all? The qualifiedName 's purpose is to identify what is a unique asset. Many different tools might all have information about that asset. Having a common \"identity\" means that many different systems can each independently construct its identifier the same way. If a crawler gets table details from Snowflake it can upsert based on those identity characteristics in Atlan. The crawler will not create duplicate tables every time it runs. This gives idempotency. Looker knows the same identity characteristics for the Snowflake tables and columns. So if you get details from Looker about the tables it uses for reporting, you can link them together in lineage. (Looker can construct the same identifier for the table as Snowflake itself.) These characteristics are not possible using GUIDs alone. Limit to changes only Now that you understand the nuances of identifiers, let's look at how you can update metadata in Atlan. In general, you only need to send changes to Atlan. You do not need to send an entire asset each time you want to make changes to it. For example, imagine you want to mark a table as certified but do not want to change anything else (its name, description, owner details, and so on): You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualified_name . Using the updater() class method on any asset type, you pass in (typically) the qualified_name and name of the asset. You can then add onto the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to client.asset.save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the Updater() method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns an object into which you can then place any updates. You can place into the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to .Save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. Atlan will handle idempotency By sending only the changes you want to apply, Atlan can make idempotent updates. Atlan will only attempt to update the asset with the changes you send. Atlan leaves any existing metadata on the asset as-is. If the asset already has the metadata values you are sending, Atlan does nothing. It will not even update audit details like the last update timestamp, and is thus idempotent. What if you want to make changes to many assets, as efficiently as possible? Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Where to go from here Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Real code samples our customers use to solve particular use cases. Review live samples Events Delve deep into the details of the events Atlan triggers. Learn more about events Delve deep into the details of the events Atlan triggers. Learn more about events Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†©"}
{"url":"https:\/\/developer.atlan.com\/getting-started\/#bulk-changes","title":"Introductory walkthrough - Developer","text":"An introductory walkthrough You might also like our Atlan Platform Essentials certification . Not sure where to start? Allow us to introduce Atlan development through example. 1 We strongly recommend using one of our SDKs to simplify the development process. As a first step, set one up: The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include logback as a simple binding mechanism to send any logging information out to your console (standard out). Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on PyPI . You can use pip to install it as follows: Provide two values to create an Atlan client: Provide your Atlan tenant URL to the base_url parameter. (You can also do this through environment variables .) Provide your API token to the api_key parameter. (You can also do this through environment variables .) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include slf4j-simple as a simple binding mechanism to send any logging information out to your console (standard out), along with the kotlin-logging-jvm microutil. Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on GitHub , ready to be included in your project: Provide two values to set up connectivity to Atlan: Provide your Atlan tenant URL to the assets.Context() method. If you prefer using the value from an environment variable, you can use assets.NewContext() without any parameters. Provide your API token as the second parameter to the assets.Context() method. (Or again, have it picked up automatically by the assets.NewContext() method.) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. Don't forget to give permissions If you want to be able to access existing metadata with an API token, don't forget that you need to assign one or more personas to the API token that grant it access to metadata. Now that you have an SDK installed and configured, you are ready to code! Before we jump straight to code, though, let's first introduce some key concepts in Atlan: What is an asset? In Atlan, we refer to all objects that provide context to your data as assets . Each type of asset in Atlan has a set of: Properties , such as: Certificates Announcements Properties , such as: Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table In an object-oriented programming sense, think of an asset as an instance of a class. The structure of an asset (the class itself, in this analogy) is defined by something called a type definition , but that's for another day. So as you can see: There are many different kinds of assets: tables, columns, schemas, databases, business intelligence dashboards, reports, and so on. Assets inter-relate with each other: a table has a parent schema and child columns, a schema has a parent database and child tables, and so on. Different kinds of assets have some common properties (like certificates) and other properties that are unique to that kind of asset (like a columnCount that only exists on tables, not on schemas or databases). When you know the asset When you already know which asset you want to retrieve, you can read it from Atlan using one of its identifiers . We'll discuss these in more detail as part of updates, but for now you can think of them as: is a primary key for an asset: completely unique, but meaningless by itself is a business key for an asset: unique for a given kind of asset, and interpretable You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the asset.get_by_guid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the asset.get_by_qualified_name() method on the Atlan client, providing the type of asset you expect to retrieve and its qualified_name . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the assets.GetByGuid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the assets.GetByQualifiedName() method on the Atlan client, providing the type of asset you expect to retrieve and its qualifiedName . (Each asset type is its own unique class in the SDK.) Note that the response is strongly typed: If you are retrieving a table, you will get a table back (as long as it exists). You do not need to figure out what properties or relationships exist on a table - the Table class defines them for for you already. In any modern IDE, this means you have type-ahead support for retrieving the properties and relationships from the table variable. You can also refer to the types reference in this portal for full details of every kind of asset. Retrieval by identifier can be more costly than you might expect Even though you are retrieving an asset by an identifier, this can be more costly than you might expect. Retrieving an asset in this way will: Retrieve all its properties and their values Retrieve all its relationships Imagine the asset you are retrieving has 100's or 1000's of these. If you only care about its certificate and any owners, you will be retrieving far more information than you need. When you need to find it first For example, imagine you want to find all tables named MY_TABLE : You can then run the request using Execute() . For example, if you want to know the certificate of the asset you only need to tack that onto the query: Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many include_on_results calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can include as many attributes in IncludeOnResults as you want to specify the properties and relationships you want to retrieve for matching assets. Also gives the best performance If all you want to do is check or report on metadata, you should have a starting point from the information above. Or, now that you've found an asset of interest, maybe you want to update the asset with additional metadata ? Once again, before we jump to code, let's first understand some key concepts about how Atlan handles updates: Importance of identifiers Most operations on assets are upserts , that is, they could either create (insert) or update a given asset. How do you know which is going to happen? To answer this question, you need to understand how Atlan uniquely identifies each asset. Recall earlier we discussed asset's different identifiers in Atlan . Every asset in Atlan has at least the following two unique identifiers. These are both mandatory for every asset, so no asset can exist without these: Atlan uses globally-unique identifiers (GUIDs) to uniquely identify each asset, globally . They look something like this: As the name implies, GUIDs are: Globally unique (across all systems). Generated in a way that makes it nearly impossible for anything else to ever generate that same ID. 2 Note that this means the GUID itself is not : Meaningful or capable of being interpreted in any way Atlan uses qualifiedName s to uniquely identify assets based on their characteristics. They look something like this: Qualified names are not : Globally unique (across all systems). Instead, they are: Consistently constructed in a meaningful way, making it possible for them to be reconstructed. Note that this means the qualifiedName is: Meaningful and capable of being interpreted How these impact updates Since they are truly unique, operations that include a GUID will only update an asset, not create one. Conversely, operations that take a qualifiedName can: Create an asset, if no exactly-matching qualifiedName is found in Atlan. Update an asset, if an exact-match for the qualifiedName is found in Atlan. These operations also require a typeName , so that if creation does occur the correct type of asset is created. Unintended consequences of this behavior Be careful when using operations with only the qualifiedName . You may end up creating assets when you were only expecting them to be updated or to fail if they did not already exist. This is particularly true when you do not give the exact, case-sensitive qualifiedName of an asset. a\/b\/c\/d is not the same as a\/B\/c\/d when it comes to qualifiedName s. Perhaps this leaves you wondering: why have a qualifiedName at all? The qualifiedName 's purpose is to identify what is a unique asset. Many different tools might all have information about that asset. Having a common \"identity\" means that many different systems can each independently construct its identifier the same way. If a crawler gets table details from Snowflake it can upsert based on those identity characteristics in Atlan. The crawler will not create duplicate tables every time it runs. This gives idempotency. Looker knows the same identity characteristics for the Snowflake tables and columns. So if you get details from Looker about the tables it uses for reporting, you can link them together in lineage. (Looker can construct the same identifier for the table as Snowflake itself.) These characteristics are not possible using GUIDs alone. Limit to changes only Now that you understand the nuances of identifiers, let's look at how you can update metadata in Atlan. In general, you only need to send changes to Atlan. You do not need to send an entire asset each time you want to make changes to it. For example, imagine you want to mark a table as certified but do not want to change anything else (its name, description, owner details, and so on): You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualified_name . Using the updater() class method on any asset type, you pass in (typically) the qualified_name and name of the asset. You can then add onto the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to client.asset.save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the Updater() method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns an object into which you can then place any updates. You can place into the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to .Save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. Atlan will handle idempotency By sending only the changes you want to apply, Atlan can make idempotent updates. Atlan will only attempt to update the asset with the changes you send. Atlan leaves any existing metadata on the asset as-is. If the asset already has the metadata values you are sending, Atlan does nothing. It will not even update audit details like the last update timestamp, and is thus idempotent. What if you want to make changes to many assets, as efficiently as possible? Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Where to go from here Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Real code samples our customers use to solve particular use cases. Review live samples Events Delve deep into the details of the events Atlan triggers. Learn more about events Delve deep into the details of the events Atlan triggers. Learn more about events Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†©"}
{"url":"https:\/\/developer.atlan.com\/patterns\/bulk\/end-to-end\/","title":"End-to-end bulk update - Developer","text":"End-to-end bulk update To walk through this using an example, and to compare and contrast the approaches, imagine you want to: Mark all views (including materialized views) in a particular schema as verified, unless they already have some certificate. Change the owner of the same views. The usual end-to-end pattern for updating many assets efficiently involves three steps: Finding the assets you want to update. Applying your updates to each asset (in-memory). Sending those changes to Atlan (in batches). You can do each of these steps in sequence, for example: 1. Find the assets The qualifiedName of every view starts with the qualifiedName of its parent (schema), so we can limit the results to a particular schema by using the qualifiedName . The qualifiedName of every view starts with the qualifiedName of its parent (schema), so we can limit the results to a particular schema by using the qualifiedName . To start building up a query with multiple conditions, you can use the select() helper on any client's assets member. To start building up a query with multiple conditions, you can use the select() helper on any client's assets member. Since there could be tables, views, materialized views and columns in this schema â€” but you only want views and materialized views â€” you can use the Asset.TYPE_NAME.in() method to restrict results to only views and materialized views. Since there could be tables, views, materialized views and columns in this schema â€” but you only want views and materialized views â€” you can use the Asset.TYPE_NAME.in() method to restrict results to only views and materialized views. Since you only want to update views that do not already have a certificate, you can further limit the results using the whereNot() method. This will exclude any assets where a certificate already hasAnyValue() . Since you only want to update views that do not already have a certificate, you can further limit the results using the whereNot() method. This will exclude any assets where a certificate already hasAnyValue() . Here you can play around with different page sizes, to further limit API calls by retrieving more results per page. Here you can play around with different page sizes, to further limit API calls by retrieving more results per page. The qualified_name of every view starts with the qualified_name of its parent (schema), so we can limit the results to a particular schema by using the qualified_name . The qualified_name of every view starts with the qualified_name of its parent (schema), so we can limit the results to a particular schema by using the qualified_name . Since there could be tables, views, materialized views and columns in this schema â€” but you only want views and materialized views â€” you can use the CompoundQuery.asset_types() helper method to restrict results to only views and materialized views. Since there could be tables, views, materialized views and columns in this schema â€” but you only want views and materialized views â€” you can use the CompoundQuery.asset_types() helper method to restrict results to only views and materialized views. Since you only want to update views that do not already have a certificate, you can further limit the results using the where_not() method. This will exclude any assets where a certificate already has_any_value() . Since you only want to update views that do not already have a certificate, you can further limit the results using the where_not() method. This will exclude any assets where a certificate already has_any_value() . Here you can play around with different page sizes, to further limit API calls by retrieving more results per page. Here you can play around with different page sizes, to further limit API calls by retrieving more results per page. The qualifiedName of every view starts with the qualifiedName of its parent (schema), so we can limit the results to a particular schema by using the qualifiedName . The qualifiedName of every view starts with the qualifiedName of its parent (schema), so we can limit the results to a particular schema by using the qualifiedName . To start building up a query with multiple conditions, you can use the select() helper on any client's assets member. To start building up a query with multiple conditions, you can use the select() helper on any client's assets member. Since there could be tables, views, materialized views and columns in this schema â€” but you only want views and materialized views â€” you can use the Asset.TYPE_NAME.in() helper method to restrict results to only views and materialized views. Since there could be tables, views, materialized views and columns in this schema â€” but you only want views and materialized views â€” you can use the Asset.TYPE_NAME.in() helper method to restrict results to only views and materialized views. Since you only want to update views that do not already have a certificate, you can further limit the results using the whereNot() method. This will exclude any assets where a certificate already hasAnyValue() . Since you only want to update views that do not already have a certificate, you can further limit the results using the whereNot() method. This will exclude any assets where a certificate already hasAnyValue() . Here you can play around with different page sizes, to further limit API calls by retrieving more results per page. Here you can play around with different page sizes, to further limit API calls by retrieving more results per page. The qualifiedName of every view starts with the qualifiedName of its parent (schema), so we can limit the results to a particular schema by using the qualifiedName . The qualifiedName of every view starts with the qualifiedName of its parent (schema), so we can limit the results to a particular schema by using the qualifiedName . Since there could be tables, views, materialized views and columns in this schema â€” but you only want views and materialized views â€” you can use an exact match on multiple types to restrict results to only views and materialized views. Since there could be tables, views, materialized views and columns in this schema â€” but you only want views and materialized views â€” you can use an exact match on multiple types to restrict results to only views and materialized views. Since you only want to update views that do not already have a certificate, you can further limit the results using the must_not clause. This will exclude any assets that already have a certificate present. Since you only want to update views that do not already have a certificate, you can further limit the results using the must_not clause. This will exclude any assets that already have a certificate present. When paging through results, you should specify a sort to give a stable set of results across pages. The most reliable sort will be by GUID of the asset, as this is guaranteed to be unique for every asset. When paging through results, you should specify a sort to give a stable set of results across pages. The most reliable sort will be by GUID of the asset, as this is guaranteed to be unique for every asset. Here you can play around with different page sizes, to further limit API calls by retrieving more results per page. Here you can play around with different page sizes, to further limit API calls by retrieving more results per page. 2. Build-up your changes Next, you iterate through those results and make the changes you want to each one. Use the multiple operations pattern to make multiple changes to each asset. Create a batch of assets to build-up the changes across multiple assets before applying those changes in Atlan itself. The first parameter defines the Atlan tenant on which the batch will be processed The second specifies the maximum number of assets to build-up before sending them across to Atlan Additional parameters By default (using only the options above) no classifications or custom metadata will be added or changed on the assets in each batch. To also include classifications and custom metadata, you need to use these additional parameters: A third parameter of true to replace all classifications on the assets in the batch, which would include removing classifications if none are provided for the assets in the batch itself (or false if you still want to ignore classifications) A fourth parameter to control how custom metadata should be handled for the assets: IGNORE any custom metadata changes in the batch, OVERWRITE to replace all custom metadata with what's provided in the batch (including removing custom metadata that already exists on an asset), or MERGE to only add or update custom metadata based on what's in the batch (leaving other existing custom metadata unchanged) a fifth parameter to control whether failures should be captured across batches ( true ) or ignored ( false ) a sixth parameter to control whether the batch should only attempt to update assets that already exist ( true ) or also create assets if they do not yet exist ( false ) a seventh parameter to control whether details about each created and updated asset across batches should be tracked ( true ) or ignored ( false ) â€” counts will always be kept an eighth parameter to control whether the matching for determining whether an asset already exists should be done in a case-insensitive way ( true ) or case-sensitively ( false ) a ninth parameter to control what kind of assets to create, if not running in updateOnly mode: partial assets (only available in lineage), or full assets a tenth parameter to control whether the matching for determining whether an asset already exists should be done strictly according to the data type specified ( false ), or if tables, views and materialized views should be treated interchangeably ( true ) Create a batch of assets to build-up the changes across multiple assets before applying those changes in Atlan itself. The first parameter defines the Atlan tenant on which the batch will be processed The second specifies the maximum number of assets to build-up before sending them across to Atlan By default (using only the options above) no classifications or custom metadata will be added or changed on the assets in each batch. To also include classifications and custom metadata, you need to use these additional parameters: A third parameter of true to replace all classifications on the assets in the batch, which would include removing classifications if none are provided for the assets in the batch itself (or false if you still want to ignore classifications) A fourth parameter to control how custom metadata should be handled for the assets: IGNORE any custom metadata changes in the batch, OVERWRITE to replace all custom metadata with what's provided in the batch (including removing custom metadata that already exists on an asset), or MERGE to only add or update custom metadata based on what's in the batch (leaving other existing custom metadata unchanged) a fifth parameter to control whether failures should be captured across batches ( true ) or ignored ( false ) a sixth parameter to control whether the batch should only attempt to update assets that already exist ( true ) or also create assets if they do not yet exist ( false ) a seventh parameter to control whether details about each created and updated asset across batches should be tracked ( true ) or ignored ( false ) â€” counts will always be kept an eighth parameter to control whether the matching for determining whether an asset already exists should be done in a case-insensitive way ( true ) or case-sensitively ( false ) a ninth parameter to control what kind of assets to create, if not running in updateOnly mode: partial assets (only available in lineage), or full assets a tenth parameter to control whether the matching for determining whether an asset already exists should be done strictly according to the data type specified ( false ), or if tables, views and materialized views should be treated interchangeably ( true ) Every asset implements the trimToRequired() method, which gives you a builder containing only the bare minimum information needed to update that asset. Limit your asset to only what you intend to update When you send an update to Atlan, it will only attempt to change the information you send in your request â€” leaving any information not in your request as-is (unchanged) on the asset in Atlan. By using trimToRequired() you can remove all information you do not want to update, and then chain on only the details you do want to update. Every asset implements the trimToRequired() method, which gives you a builder containing only the bare minimum information needed to update that asset. Limit your asset to only what you intend to update When you send an update to Atlan, it will only attempt to change the information you send in your request â€” leaving any information not in your request as-is (unchanged) on the asset in Atlan. By using trimToRequired() you can remove all information you do not want to update, and then chain on only the details you do want to update. In this running example, you are updating the certificate to verified and setting a new owner â€” so you simply chain those updates onto the trimmed builder. In this running example, you are updating the certificate to verified and setting a new owner â€” so you simply chain those updates onto the trimmed builder. You can then add your (in-memory) modified asset to the batch. Auto-saves as it goes As long as the number of assets built-up is below the maximum batch size specified when creating the batch, this will simply continue to build up the batch. As soon as you hit the size limit for the batch, though, this same method will call the save() operation to batch-update all of those assets in a single API call. Remember to flush Since your loop could finish before you reach another full batch, you must always remember to flush() the batch. This will send any remaining assets that were queued up, when the size of the queue did not yet reach the full batch size. You can then add your (in-memory) modified asset to the batch. Auto-saves as it goes As long as the number of assets built-up is below the maximum batch size specified when creating the batch, this will simply continue to build up the batch. As soon as you hit the size limit for the batch, though, this same method will call the save() operation to batch-update all of those assets in a single API call. Remember to flush Since your loop could finish before you reach another full batch, you must always remember to flush() the batch. This will send any remaining assets that were queued up, when the size of the queue did not yet reach the full batch size. Create a batch of assets to accumulate changes across multiple assets before applying those changes in Atlan itself. The Batch() takes the following parameters: client : an instance of AssetClient . max_size : the maximum size of each batch to be processed (per API call). Additional optional parameters By default (using only the options above) no classifications or custom metadata will be added or changed on the assets in each batch. To also include classifications and custom metadata, you need to use these additional parameters: replace_atlan_tags ( default: False ): If True replace all classifications (tags) on the assets in the batch, which would include removing classifications (tags) if none are provided for the assets in the batch itself (or False if you still want to ignore classifications) custom_metadata_handling ( default: CustomMetadataHandling.IGNORE ): control how custom metadata should be handled for the assets: IGNORE any custom metadata changes in the batch, OVERWRITE to replace all custom metadata with what's provided in the batch (including removing custom metadata that already exists on an asset), or MERGE to only add or update custom metadata based on what's in the batch (leaving other existing custom metadata unchanged) capture_failures ( default: False ): control whether failures should be captured across batches ( True ) or ignored ( False ) update_only ( default: False ): control whether the batch should only attempt to update assets that already exist ( True ) or also create assets if they do not yet exist ( False ) track ( default: False ): control whether details about each created and updated asset across batches should be tracked ( True ) or ignored ( False ) â€” counts will always be kept case_insensitive ( default: False ): control whether the matching for determining whether an asset already exists should be done in a case-insensitive way ( True ) or case-sensitively ( False ) creation_handling ( default: AssetCreationHandling.FULL ): control what kind of assets to create, if not running in update_only mode; PARTIAL assets (only available in lineage), or FULL assets table_view_agnostic ( default: False ): control whether the matching for determining whether an asset already exists should be done strictly according to the data type specified ( False ), or if tables, views and materialized views should be treated interchangeably ( True ) Create a batch of assets to accumulate changes across multiple assets before applying those changes in Atlan itself. The Batch() takes the following parameters: client : an instance of AssetClient . max_size : the maximum size of each batch to be processed (per API call). By default (using only the options above) no classifications or custom metadata will be added or changed on the assets in each batch. To also include classifications and custom metadata, you need to use these additional parameters: replace_atlan_tags ( default: False ): If True replace all classifications (tags) on the assets in the batch, which would include removing classifications (tags) if none are provided for the assets in the batch itself (or False if you still want to ignore classifications) custom_metadata_handling ( default: CustomMetadataHandling.IGNORE ): control how custom metadata should be handled for the assets: IGNORE any custom metadata changes in the batch, OVERWRITE to replace all custom metadata with what's provided in the batch (including removing custom metadata that already exists on an asset), or MERGE to only add or update custom metadata based on what's in the batch (leaving other existing custom metadata unchanged) capture_failures ( default: False ): control whether failures should be captured across batches ( True ) or ignored ( False ) update_only ( default: False ): control whether the batch should only attempt to update assets that already exist ( True ) or also create assets if they do not yet exist ( False ) track ( default: False ): control whether details about each created and updated asset across batches should be tracked ( True ) or ignored ( False ) â€” counts will always be kept case_insensitive ( default: False ): control whether the matching for determining whether an asset already exists should be done in a case-insensitive way ( True ) or case-sensitively ( False ) creation_handling ( default: AssetCreationHandling.FULL ): control what kind of assets to create, if not running in update_only mode; PARTIAL assets (only available in lineage), or FULL assets table_view_agnostic ( default: False ): control whether the matching for determining whether an asset already exists should be done strictly according to the data type specified ( False ), or if tables, views and materialized views should be treated interchangeably ( True ) Every asset implements the trim_to_required() method, which gives you an object containing only the bare minimum information needed to update that asset. Limit your asset to only what you intend to update When you send an update to Atlan, it will only attempt to change the information you send in your request â€” leaving any information not in your request as-is (unchanged) on the asset in Atlan. By using trimToRequired() you can remove all information you do not want to update, and then chain on only the details you do want to update. Every asset implements the trim_to_required() method, which gives you an object containing only the bare minimum information needed to update that asset. Limit your asset to only what you intend to update When you send an update to Atlan, it will only attempt to change the information you send in your request â€” leaving any information not in your request as-is (unchanged) on the asset in Atlan. By using trimToRequired() you can remove all information you do not want to update, and then chain on only the details you do want to update. In this running example, you are updating the certificate to verified and setting a new owner â€” so you simply add those updates onto the trimmed object. In this running example, you are updating the certificate to verified and setting a new owner â€” so you simply add those updates onto the trimmed object. You can then add your (in-memory) modified asset to the batch. Auto-saves as it goes As long as the number of assets built-up is below the maximum batch size specified when creating the batch, this will simply continue to build up the batch. As soon as you hit the size limit for the batch, though, this same method will call the save() operation to batch-update all of those assets in a single API call. Remember to flush Since your loop could finish before you reach another full batch, you must always remember to flush() the batch. This will send any remaining assets that were queued up, when the size of the queue did not yet reach the full batch size. You can then add your (in-memory) modified asset to the batch. Auto-saves as it goes As long as the number of assets built-up is below the maximum batch size specified when creating the batch, this will simply continue to build up the batch. As soon as you hit the size limit for the batch, though, this same method will call the save() operation to batch-update all of those assets in a single API call. Remember to flush Since your loop could finish before you reach another full batch, you must always remember to flush() the batch. This will send any remaining assets that were queued up, when the size of the queue did not yet reach the full batch size. Create a batch of assets to build-up the changes across multiple assets before applying those changes in Atlan itself. The first parameter defines the Atlan tenant on which the batch will be processed The second specifies the maximum number of assets to build-up before sending them across to Atlan Additional parameters By default (using only the options above) no classifications or custom metadata will be added or changed on the assets in each batch. To also include classifications and custom metadata, you need to use these additional parameters: A third parameter of true to replace all classifications on the assets in the batch, which would include removing classifications if none are provided for the assets in the batch itself (or false if you still want to ignore classifications) A fourth parameter to control how custom metadata should be handled for the assets: IGNORE any custom metadata changes in the batch, OVERWRITE to replace all custom metadata with what's provided in the batch (including removing custom metadata that already exists on an asset), or MERGE to only add or update custom metadata based on what's in the batch (leaving other existing custom metadata unchanged) a fifth parameter to control whether failures should be captured across batches ( true ) or ignored ( false ) a sixth parameter to control whether the batch should only attempt to update assets that already exist ( true ) or also create assets if they do not yet exist ( false ) a seventh parameter to control whether details about each created and updated asset across batches should be tracked ( true ) or ignored ( false ) â€” counts will always be kept an eighth parameter to control whether the matching for determining whether an asset already exists should be done in a case-insensitive way ( true ) or case-sensitively ( false ) a ninth parameter to control what kind of assets to create, if not running in updateOnly mode: partial assets (only available in lineage), or full assets a tenth parameter to control whether the matching for determining whether an asset already exists should be done strictly according to the data type specified ( false ), or if tables, views and materialized views should be treated interchangeably ( true ) Create a batch of assets to build-up the changes across multiple assets before applying those changes in Atlan itself. The first parameter defines the Atlan tenant on which the batch will be processed The second specifies the maximum number of assets to build-up before sending them across to Atlan By default (using only the options above) no classifications or custom metadata will be added or changed on the assets in each batch. To also include classifications and custom metadata, you need to use these additional parameters: A third parameter of true to replace all classifications on the assets in the batch, which would include removing classifications if none are provided for the assets in the batch itself (or false if you still want to ignore classifications) A fourth parameter to control how custom metadata should be handled for the assets: IGNORE any custom metadata changes in the batch, OVERWRITE to replace all custom metadata with what's provided in the batch (including removing custom metadata that already exists on an asset), or MERGE to only add or update custom metadata based on what's in the batch (leaving other existing custom metadata unchanged) a fifth parameter to control whether failures should be captured across batches ( true ) or ignored ( false ) a sixth parameter to control whether the batch should only attempt to update assets that already exist ( true ) or also create assets if they do not yet exist ( false ) a seventh parameter to control whether details about each created and updated asset across batches should be tracked ( true ) or ignored ( false ) â€” counts will always be kept an eighth parameter to control whether the matching for determining whether an asset already exists should be done in a case-insensitive way ( true ) or case-sensitively ( false ) a ninth parameter to control what kind of assets to create, if not running in updateOnly mode: partial assets (only available in lineage), or full assets a tenth parameter to control whether the matching for determining whether an asset already exists should be done strictly according to the data type specified ( false ), or if tables, views and materialized views should be treated interchangeably ( true ) Every asset implements the trimToRequired() method, which gives you a builder containing only the bare minimum information needed to update that asset. Limit your asset to only what you intend to update When you send an update to Atlan, it will only attempt to change the information you send in your request â€” leaving any information not in your request as-is (unchanged) on the asset in Atlan. By using trimToRequired() you can remove all information you do not want to update, and then chain on only the details you do want to update. Every asset implements the trimToRequired() method, which gives you a builder containing only the bare minimum information needed to update that asset. Limit your asset to only what you intend to update When you send an update to Atlan, it will only attempt to change the information you send in your request â€” leaving any information not in your request as-is (unchanged) on the asset in Atlan. By using trimToRequired() you can remove all information you do not want to update, and then chain on only the details you do want to update. In this running example, you are updating the certificate to verified and setting a new owner â€” so you simply chain those updates onto the trimmed builder. In this running example, you are updating the certificate to verified and setting a new owner â€” so you simply chain those updates onto the trimmed builder. You can then add your (in-memory) modified asset to the batch. Auto-saves as it goes As long as the number of assets built-up is below the maximum batch size specified when creating the batch, this will simply continue to build up the batch. As soon as you hit the size limit for the batch, though, this same method will call the save() operation to batch-update all of those assets in a single API call. Remember to flush Since your loop could finish before you reach another full batch, you must always remember to flush() the batch. This will send any remaining assets that were queued up, when the size of the queue did not yet reach the full batch size. You can then add your (in-memory) modified asset to the batch. Auto-saves as it goes As long as the number of assets built-up is below the maximum batch size specified when creating the batch, this will simply continue to build up the batch. As soon as you hit the size limit for the batch, though, this same method will call the save() operation to batch-update all of those assets in a single API call. Remember to flush Since your loop could finish before you reach another full batch, you must always remember to flush() the batch. This will send any remaining assets that were queued up, when the size of the queue did not yet reach the full batch size. Up to your own code There are no API calls to make to change the results in-memory. How you implement this will be entirely up to how you are writing your code. 3. Save them in batches Finally, send the changes you have queued up in batches. Use the multiple assets pattern to update multiple assets at the same time. The AssetBatch 's add() method used in the previous step will automatically save as its internal queue of assets reaches a full batch size. Remember to flush However, since your loop could finish before you reach another full batch, you must always remember to flush() the batch. This will send any remaining assets that were queued up. The AssetBatch 's add() method used in the previous step will automatically save as its internal queue of assets reaches a full batch size. Remember to flush However, since your loop could finish before you reach another full batch, you must always remember to flush() the batch. This will send any remaining assets that were queued up. Both the .add() and .flush() operations of the AssetBatch could send a request over to Atlan. Either can therefore also run into trouble and raise an error through an AtlanException . It is up to you to handle such potential errors as you see fit. Both the .add() and .flush() operations of the AssetBatch could send a request over to Atlan. Either can therefore also run into trouble and raise an error through an AtlanException . It is up to you to handle such potential errors as you see fit. The Batch 's add() method used in the previous step will automatically save as its internal queue of assets reaches a full batch size. Remember to flush However, since your loop could finish before you reach another full batch, you must always remember to flush() the batch. This will send any remaining assets that were queued up. The Batch 's add() method used in the previous step will automatically save as its internal queue of assets reaches a full batch size. Remember to flush However, since your loop could finish before you reach another full batch, you must always remember to flush() the batch. This will send any remaining assets that were queued up. Both the .add() and .flush() operations of the Batch could send a request over to Atlan. Either can therefore also run into trouble and raise an error through an AtlanError . It is up to you to handle such potential errors as you see fit. Both the .add() and .flush() operations of the Batch could send a request over to Atlan. Either can therefore also run into trouble and raise an error through an AtlanError . It is up to you to handle such potential errors as you see fit. The AssetBatch 's add() method used in the previous step will automatically save as its internal queue of assets reaches a full batch size. Remember to flush However, since your loop could finish before you reach another full batch, you must always remember to flush() the batch. This will send any remaining assets that were queued up. The AssetBatch 's add() method used in the previous step will automatically save as its internal queue of assets reaches a full batch size. Remember to flush However, since your loop could finish before you reach another full batch, you must always remember to flush() the batch. This will send any remaining assets that were queued up. Both the .add() and .flush() operations of the AssetBatch could send a request over to Atlan. Either can therefore also run into trouble and raise an error through an AtlanException . It is up to you to handle such potential errors as you see fit. Both the .add() and .flush() operations of the AssetBatch could send a request over to Atlan. Either can therefore also run into trouble and raise an error through an AtlanException . It is up to you to handle such potential errors as you see fit. All details must still be included in an outer entities array. You need to specify the type for each asset you are updating. You need to specify other required attributes for each asset, such as its name and qualifiedName. Add on any other attributes or relationships you want to set on the asset, such as in the running example a verified certificate and new individual owner. Add another object to the payload to represent another asset that should be updated by the same API call. Once again specify all the required information for that kind of asset, and any of the details for attributes or relationships you want to set. Alternatively, when using an SDK, you can pipeline these operations together. The pipeline will run just as efficiently as the step-by-step approach above: Lazily-fetching each page of results Batching up and bulk-saving changes The qualifiedName of every view starts with the qualifiedName of its parent (schema), so we can limit the results to a particular schema by using the qualifiedName . The qualifiedName of every view starts with the qualifiedName of its parent (schema), so we can limit the results to a particular schema by using the qualifiedName . Create a batch of assets to build-up the changes across multiple assets before applying those changes in Atlan itself. When parallel-processing (see further notes on the stream(true) ) you need to use a parallel-capable ParallelBatch : The first parameter defines the Atlan tenant on which the batch will be processed The second specifies the maximum number of assets to build-up before sending them across to Atlan Additional parameters By default (using only the options above) no classifications or custom metadata will be added or changed on the assets in each batch. To also include classifications and custom metadata, you need to use these additional parameters: A third parameter of true to replace all classifications on the assets in the batch, which would include removing classifications if none are provided for the assets in the batch itself (or false if you still want to ignore classifications) A fourth parameter to control how custom metadata should be handled for the assets: IGNORE any custom metadata changes in the batch, OVERWRITE to replace all custom metadata with what's provided in the batch (including removing custom metadata that already exists on an asset), or MERGE to only add or update custom metadata based on what's in the batch (leaving other existing custom metadata unchanged) a fifth parameter to control whether failures should be captured across batches ( true ) or ignored ( false ) a sixth parameter to control whether the batch should only attempt to update assets that already exist ( true ) or also create assets if they do not yet exist ( false ) a seventh parameter to control whether details about each created and updated asset across batches should be tracked ( true ) or ignored ( false ) â€” counts will always be kept an eighth parameter to control whether the matching for determining whether an asset already exists should be done in a case-insensitive way ( true ) or case-sensitively ( false ) a ninth parameter to control what kind of assets to create, if not running in updateOnly mode: partial assets (only available in lineage), or full assets a tenth parameter to control whether the matching for determining whether an asset already exists should be done strictly according to the data type specified ( false ), or if tables, views and materialized views should be treated interchangeably ( true ) Create a batch of assets to build-up the changes across multiple assets before applying those changes in Atlan itself. When parallel-processing (see further notes on the stream(true) ) you need to use a parallel-capable ParallelBatch : The first parameter defines the Atlan tenant on which the batch will be processed The second specifies the maximum number of assets to build-up before sending them across to Atlan By default (using only the options above) no classifications or custom metadata will be added or changed on the assets in each batch. To also include classifications and custom metadata, you need to use these additional parameters: A third parameter of true to replace all classifications on the assets in the batch, which would include removing classifications if none are provided for the assets in the batch itself (or false if you still want to ignore classifications) A fourth parameter to control how custom metadata should be handled for the assets: IGNORE any custom metadata changes in the batch, OVERWRITE to replace all custom metadata with what's provided in the batch (including removing custom metadata that already exists on an asset), or MERGE to only add or update custom metadata based on what's in the batch (leaving other existing custom metadata unchanged) a fifth parameter to control whether failures should be captured across batches ( true ) or ignored ( false ) a sixth parameter to control whether the batch should only attempt to update assets that already exist ( true ) or also create assets if they do not yet exist ( false ) a seventh parameter to control whether details about each created and updated asset across batches should be tracked ( true ) or ignored ( false ) â€” counts will always be kept an eighth parameter to control whether the matching for determining whether an asset already exists should be done in a case-insensitive way ( true ) or case-sensitively ( false ) a ninth parameter to control what kind of assets to create, if not running in updateOnly mode: partial assets (only available in lineage), or full assets a tenth parameter to control whether the matching for determining whether an asset already exists should be done strictly according to the data type specified ( false ), or if tables, views and materialized views should be treated interchangeably ( true ) You can then start defining a pipeline directly against the client's assets by using the select() method. Including archived (soft-deleted) assets Since there could be tables, views, materialized views and columns in this schema â€” but you only want views and materialized views â€” you can use the Asset.TYPE_NAME.in() method to restrict results to only views and materialized views. Since there could be tables, views, materialized views and columns in this schema â€” but you only want views and materialized views â€” you can use the Asset.TYPE_NAME.in() method to restrict results to only views and materialized views. Since you only want to update views that do not already have a certificate, you can further limit the results using the whereNot() method. This will exclude any assets where a certificate already hasAnyValue() . Since you only want to update views that do not already have a certificate, you can further limit the results using the whereNot() method. This will exclude any assets where a certificate already hasAnyValue() . (Optional) You can play around with different page sizes, to further limit API calls by retrieving more results per page. (Optional) You can play around with different page sizes, to further limit API calls by retrieving more results per page. Once you have defined the criteria for your pipeline, call the stream() method to push-down the pipeline to Atlan. This will: Page through the results by lazily fetching each subsequent page as you iterate through them. (So if you use a limit() on the stream, for example, you can break out before retrieving all pages.) Can also run in parallel threads You can also parallel-stream the results by passing true to the stream() method. This will spawn multiple threads that each independently process a page of results and combine the results in parallel. While this can be significantly faster for processing many results, keep in mind if you are collecting the results into any structure that structure must be thread-safe. (For example, you'll need to use things like ConcurrentHashMap rather than just HashMap , and to use ParallelBatch rather than AssetBatch if making changes.) For each result, you can then carry out your changes and submit them into the batch. For each result, you can then carry out your changes and submit them into the batch. Every asset implements the trimToRequired() method, which gives you a builder containing only the bare minimum information needed to update that asset. Limit your asset to only what you intend to update When you send an update to Atlan, it will only attempt to change the information you send in your request â€” leaving any information not in your request as-is (unchanged) on the asset in Atlan. By using trimToRequired() you can remove all information you do not want to update, and then chain on only the details you do want to update. Every asset implements the trimToRequired() method, which gives you a builder containing only the bare minimum information needed to update that asset. Limit your asset to only what you intend to update When you send an update to Atlan, it will only attempt to change the information you send in your request â€” leaving any information not in your request as-is (unchanged) on the asset in Atlan. By using trimToRequired() you can remove all information you do not want to update, and then chain on only the details you do want to update. In this running example, you are updating the certificate to verified and setting a new owner â€” so you simply chain those updates onto the trimmed builder. In this running example, you are updating the certificate to verified and setting a new owner â€” so you simply chain those updates onto the trimmed builder. You can then add your (in-memory) modified asset to the batch. Auto-saves as it goes As long as the number of assets built-up is below the maximum batch size specified when creating the batch, this will simply continue to build up the batch. As soon as you hit the size limit for the batch, though, this same method will call the save() operation to batch-update all of those assets in a single API call. Remember to flush Since your loop could finish before you reach another full batch, you must always remember to flush() the batch. This will send any remaining assets that were queued up, when the size of the queue did not yet reach the full batch size. You can then add your (in-memory) modified asset to the batch. Auto-saves as it goes As long as the number of assets built-up is below the maximum batch size specified when creating the batch, this will simply continue to build up the batch. As soon as you hit the size limit for the batch, though, this same method will call the save() operation to batch-update all of those assets in a single API call. Remember to flush Since your loop could finish before you reach another full batch, you must always remember to flush() the batch. This will send any remaining assets that were queued up, when the size of the queue did not yet reach the full batch size. Both the .add() and .flush() operations of the AssetBatch could send a request over to Atlan. Either can therefore also run into trouble and raise an error through an AtlanException . It is up to you to handle such potential errors as you see fit. Both the .add() and .flush() operations of the AssetBatch could send a request over to Atlan. Either can therefore also run into trouble and raise an error through an AtlanException . It is up to you to handle such potential errors as you see fit. The AssetBatch 's add() method used in the previous step will automatically save as its internal queue of assets reaches a full batch size. Remember to flush However, since your loop could finish before you reach another full batch, you must always remember to flush() the batch. This will send any remaining assets that were queued up. The AssetBatch 's add() method used in the previous step will automatically save as its internal queue of assets reaches a full batch size. Remember to flush However, since your loop could finish before you reach another full batch, you must always remember to flush() the batch. This will send any remaining assets that were queued up. The qualifiedName of every view starts with the qualifiedName of its parent (schema), so we can limit the results to a particular schema by using the qualifiedName . The qualifiedName of every view starts with the qualifiedName of its parent (schema), so we can limit the results to a particular schema by using the qualifiedName . Create a batch of assets to accumulate changes across multiple assets before applying those changes in Atlan itself. The Batch() takes the following parameters: client : an instance of AssetClient . max_size : the maximum size of each batch to be processed (per API call). Additional optional parameters By default (using only the options above) no classifications or custom metadata will be added or changed on the assets in each batch. To also include classifications and custom metadata, you need to use these additional parameters: replace_atlan_tags ( default: False ): If True replace all classifications (tags) on the assets in the batch, which would include removing classifications (tags) if none are provided for the assets in the batch itself (or False if you still want to ignore classifications) custom_metadata_handling ( default: CustomMetadataHandling.IGNORE ): control how custom metadata should be handled for the assets: IGNORE any custom metadata changes in the batch, OVERWRITE to replace all custom metadata with what's provided in the batch (including removing custom metadata that already exists on an asset), or MERGE to only add or update custom metadata based on what's in the batch (leaving other existing custom metadata unchanged) capture_failures ( default: False ): control whether failures should be captured across batches ( True ) or ignored ( False ) update_only ( default: False ): control whether the batch should only attempt to update assets that already exist ( True ) or also create assets if they do not yet exist ( False ) track ( default: False ): control whether details about each created and updated asset across batches should be tracked ( True ) or ignored ( False ) â€” counts will always be kept case_insensitive ( default: False ): control whether the matching for determining whether an asset already exists should be done in a case-insensitive way ( True ) or case-sensitively ( False ) creation_handling ( default: AssetCreationHandling.FULL ): control what kind of assets to create, if not running in update_only mode; PARTIAL assets (only available in lineage), or FULL assets table_view_agnostic ( default: False ): control whether the matching for determining whether an asset already exists should be done strictly according to the data type specified ( False ), or if tables, views and materialized views should be treated interchangeably ( True ) Create a batch of assets to accumulate changes across multiple assets before applying those changes in Atlan itself. The Batch() takes the following parameters: client : an instance of AssetClient . max_size : the maximum size of each batch to be processed (per API call). By default (using only the options above) no classifications or custom metadata will be added or changed on the assets in each batch. To also include classifications and custom metadata, you need to use these additional parameters: replace_atlan_tags ( default: False ): If True replace all classifications (tags) on the assets in the batch, which would include removing classifications (tags) if none are provided for the assets in the batch itself (or False if you still want to ignore classifications) custom_metadata_handling ( default: CustomMetadataHandling.IGNORE ): control how custom metadata should be handled for the assets: IGNORE any custom metadata changes in the batch, OVERWRITE to replace all custom metadata with what's provided in the batch (including removing custom metadata that already exists on an asset), or MERGE to only add or update custom metadata based on what's in the batch (leaving other existing custom metadata unchanged) capture_failures ( default: False ): control whether failures should be captured across batches ( True ) or ignored ( False ) update_only ( default: False ): control whether the batch should only attempt to update assets that already exist ( True ) or also create assets if they do not yet exist ( False ) track ( default: False ): control whether details about each created and updated asset across batches should be tracked ( True ) or ignored ( False ) â€” counts will always be kept case_insensitive ( default: False ): control whether the matching for determining whether an asset already exists should be done in a case-insensitive way ( True ) or case-sensitively ( False ) creation_handling ( default: AssetCreationHandling.FULL ): control what kind of assets to create, if not running in update_only mode; PARTIAL assets (only available in lineage), or FULL assets table_view_agnostic ( default: False ): control whether the matching for determining whether an asset already exists should be done strictly according to the data type specified ( False ), or if tables, views and materialized views should be treated interchangeably ( True ) Since there could be tables, views, materialized views and columns in this schema â€” but you only want views and materialized views â€” you can use the CompoundQuery.asset_types() helper method to restrict results to only views and materialized views. Since there could be tables, views, materialized views and columns in this schema â€” but you only want views and materialized views â€” you can use the CompoundQuery.asset_types() helper method to restrict results to only views and materialized views. Since you only want to update views that do not already have a certificate, you can further limit the results using the where_not() method. This will exclude any assets where a certificate already has_any_value() . Since you only want to update views that do not already have a certificate, you can further limit the results using the where_not() method. This will exclude any assets where a certificate already has_any_value() . (Optional) You can play around with different page sizes, to further limit API calls by retrieving more results per page. (Optional) You can play around with different page sizes, to further limit API calls by retrieving more results per page. For each result, you can then carry out your changes and submit them into the batch. For each result, you can then carry out your changes and submit them into the batch. Every asset implements the trim_to_required() method, which gives you a builder containing only the bare minimum information needed to update that asset. Limit your asset to only what you intend to update When you send an update to Atlan, it will only attempt to change the information you send in your request â€” leaving any information not in your request as-is (unchanged) on the asset in Atlan. By using trim_to_required() you can remove all information you do not want to update, and then chain on only the details you do want to update. Every asset implements the trim_to_required() method, which gives you a builder containing only the bare minimum information needed to update that asset. Limit your asset to only what you intend to update When you send an update to Atlan, it will only attempt to change the information you send in your request â€” leaving any information not in your request as-is (unchanged) on the asset in Atlan. By using trim_to_required() you can remove all information you do not want to update, and then chain on only the details you do want to update. In this running example, you are updating the certificate to verified and setting a new owner â€” so you simply set those updates on the trimmed object. In this running example, you are updating the certificate to verified and setting a new owner â€” so you simply set those updates on the trimmed object. You can then add your (in-memory) modified asset to the batch. You can then add your (in-memory) modified asset to the batch. The Batch 's add() method used in the previous step will automatically save as its internal queue of assets reaches a full batch size. Remember to flush However, since your loop could finish before you reach another full batch, you must always remember to flush() the batch. This will send any remaining assets that were queued up. The Batch 's add() method used in the previous step will automatically save as its internal queue of assets reaches a full batch size. Remember to flush However, since your loop could finish before you reach another full batch, you must always remember to flush() the batch. This will send any remaining assets that were queued up. The qualifiedName of every view starts with the qualifiedName of its parent (schema), so we can limit the results to a particular schema by using the qualifiedName . The qualifiedName of every view starts with the qualifiedName of its parent (schema), so we can limit the results to a particular schema by using the qualifiedName . Create a batch of assets to build-up the changes across multiple assets before applying those changes in Atlan itself. When parallel-processing (see further notes on the stream(true) ) you need to use a parallel-capable ParallelBatch : The first parameter defines the Atlan tenant on which the batch will be processed The second specifies the maximum number of assets to build-up before sending them across to Atlan Additional parameters By default (using only the options above) no classifications or custom metadata will be added or changed on the assets in each batch. To also include classifications and custom metadata, you need to use these additional parameters: A third parameter of true to replace all classifications on the assets in the batch, which would include removing classifications if none are provided for the assets in the batch itself (or false if you still want to ignore classifications) A fourth parameter to control how custom metadata should be handled for the assets: IGNORE any custom metadata changes in the batch, OVERWRITE to replace all custom metadata with what's provided in the batch (including removing custom metadata that already exists on an asset), or MERGE to only add or update custom metadata based on what's in the batch (leaving other existing custom metadata unchanged) a fifth parameter to control whether failures should be captured across batches ( true ) or ignored ( false ) a sixth parameter to control whether the batch should only attempt to update assets that already exist ( true ) or also create assets if they do not yet exist ( false ) a seventh parameter to control whether details about each created and updated asset across batches should be tracked ( true ) or ignored ( false ) â€” counts will always be kept an eighth parameter to control whether the matching for determining whether an asset already exists should be done in a case-insensitive way ( true ) or case-sensitively ( false ) a ninth parameter to control what kind of assets to create, if not running in updateOnly mode: partial assets (only available in lineage), or full assets a tenth parameter to control whether the matching for determining whether an asset already exists should be done strictly according to the data type specified ( false ), or if tables, views and materialized views should be treated interchangeably ( true ) Create a batch of assets to build-up the changes across multiple assets before applying those changes in Atlan itself. When parallel-processing (see further notes on the stream(true) ) you need to use a parallel-capable ParallelBatch : The first parameter defines the Atlan tenant on which the batch will be processed The second specifies the maximum number of assets to build-up before sending them across to Atlan By default (using only the options above) no classifications or custom metadata will be added or changed on the assets in each batch. To also include classifications and custom metadata, you need to use these additional parameters: A third parameter of true to replace all classifications on the assets in the batch, which would include removing classifications if none are provided for the assets in the batch itself (or false if you still want to ignore classifications) A fourth parameter to control how custom metadata should be handled for the assets: IGNORE any custom metadata changes in the batch, OVERWRITE to replace all custom metadata with what's provided in the batch (including removing custom metadata that already exists on an asset), or MERGE to only add or update custom metadata based on what's in the batch (leaving other existing custom metadata unchanged) a fifth parameter to control whether failures should be captured across batches ( true ) or ignored ( false ) a sixth parameter to control whether the batch should only attempt to update assets that already exist ( true ) or also create assets if they do not yet exist ( false ) a seventh parameter to control whether details about each created and updated asset across batches should be tracked ( true ) or ignored ( false ) â€” counts will always be kept an eighth parameter to control whether the matching for determining whether an asset already exists should be done in a case-insensitive way ( true ) or case-sensitively ( false ) a ninth parameter to control what kind of assets to create, if not running in updateOnly mode: partial assets (only available in lineage), or full assets a tenth parameter to control whether the matching for determining whether an asset already exists should be done strictly according to the data type specified ( false ), or if tables, views and materialized views should be treated interchangeably ( true ) You can then start defining a pipeline directly against the client's assets by using the select() method. Including archived (soft-deleted) assets Since there could be tables, views, materialized views and columns in this schema â€” but you only want views and materialized views â€” you can use the Asset.TYPE_NAME.in helper method to restrict results to only views and materialized views. Since there could be tables, views, materialized views and columns in this schema â€” but you only want views and materialized views â€” you can use the Asset.TYPE_NAME.in helper method to restrict results to only views and materialized views. Since you only want to update views that do not already have a certificate, you can further limit the results using the whereNot() method. This will exclude any assets where a certificate already hasAnyValue() . Since you only want to update views that do not already have a certificate, you can further limit the results using the whereNot() method. This will exclude any assets where a certificate already hasAnyValue() . (Optional) You can play around with different page sizes, to further limit API calls by retrieving more results per page. (Optional) You can play around with different page sizes, to further limit API calls by retrieving more results per page. Once you have defined the criteria for your pipeline, call the stream() method to push-down the pipeline to Atlan. This will: Page through the results by lazily fetching each subsequent page as you iterate through them. (So if you use a limit() on the stream, for example, you can break out before retrieving all pages.) Can also run in parallel threads You can also parallel-stream the results by passing true to the stream() method. This will spawn multiple threads that each independently process a page of results and combine the results in parallel. While this can be significantly faster for processing many results, keep in mind if you are collecting the results into any structure that structure must be thread-safe. (For example, you'll need to use things like ConcurrentHashMap rather than just HashMap , and to use ParallelBatch rather than AssetBatch if making changes.) For each result, you can then carry out your changes and submit them into the batch. For each result, you can then carry out your changes and submit them into the batch. Every asset implements the trimToRequired() method, which gives you a builder containing only the bare minimum information needed to update that asset. Limit your asset to only what you intend to update When you send an update to Atlan, it will only attempt to change the information you send in your request â€” leaving any information not in your request as-is (unchanged) on the asset in Atlan. By using trimToRequired() you can remove all information you do not want to update, and then chain on only the details you do want to update. Every asset implements the trimToRequired() method, which gives you a builder containing only the bare minimum information needed to update that asset. Limit your asset to only what you intend to update When you send an update to Atlan, it will only attempt to change the information you send in your request â€” leaving any information not in your request as-is (unchanged) on the asset in Atlan. By using trimToRequired() you can remove all information you do not want to update, and then chain on only the details you do want to update. In this running example, you are updating the certificate to verified and setting a new owner â€” so you simply chain those updates onto the trimmed builder. In this running example, you are updating the certificate to verified and setting a new owner â€” so you simply chain those updates onto the trimmed builder. You can then add your (in-memory) modified asset to the batch. Auto-saves as it goes As long as the number of assets built-up is below the maximum batch size specified when creating the batch, this will simply continue to build up the batch. As soon as you hit the size limit for the batch, though, this same method will call the save() operation to batch-update all of those assets in a single API call. Remember to flush Since your loop could finish before you reach another full batch, you must always remember to flush() the batch. This will send any remaining assets that were queued up, when the size of the queue did not yet reach the full batch size. You can then add your (in-memory) modified asset to the batch. Auto-saves as it goes As long as the number of assets built-up is below the maximum batch size specified when creating the batch, this will simply continue to build up the batch. As soon as you hit the size limit for the batch, though, this same method will call the save() operation to batch-update all of those assets in a single API call. Remember to flush Since your loop could finish before you reach another full batch, you must always remember to flush() the batch. This will send any remaining assets that were queued up, when the size of the queue did not yet reach the full batch size. Both the .add() and .flush() operations of the AssetBatch could send a request over to Atlan. Either can therefore also run into trouble and raise an error through an AtlanException . It is up to you to handle such potential errors as you see fit. Both the .add() and .flush() operations of the AssetBatch could send a request over to Atlan. Either can therefore also run into trouble and raise an error through an AtlanException . It is up to you to handle such potential errors as you see fit. The AssetBatch 's add() method used in the previous step will automatically save as its internal queue of assets reaches a full batch size. Remember to flush However, since your loop could finish before you reach another full batch, you must always remember to flush() the batch. This will send any remaining assets that were queued up. The AssetBatch 's add() method used in the previous step will automatically save as its internal queue of assets reaches a full batch size. Remember to flush However, since your loop could finish before you reach another full batch, you must always remember to flush() the batch. This will send any remaining assets that were queued up. Requires numerous API calls To implement the same logic purely through raw API calls will require making many calls: To page through the results. To batch up a set of assets to update. To submit each batch of assets to update."}
{"url":"https:\/\/developer.atlan.com\/getting-started\/#where-to-go-from-here","title":"Introductory walkthrough - Developer","text":"An introductory walkthrough You might also like our Atlan Platform Essentials certification . Not sure where to start? Allow us to introduce Atlan development through example. 1 We strongly recommend using one of our SDKs to simplify the development process. As a first step, set one up: The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include logback as a simple binding mechanism to send any logging information out to your console (standard out). Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on PyPI . You can use pip to install it as follows: Provide two values to create an Atlan client: Provide your Atlan tenant URL to the base_url parameter. (You can also do this through environment variables .) Provide your API token to the api_key parameter. (You can also do this through environment variables .) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include slf4j-simple as a simple binding mechanism to send any logging information out to your console (standard out), along with the kotlin-logging-jvm microutil. Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on GitHub , ready to be included in your project: Provide two values to set up connectivity to Atlan: Provide your Atlan tenant URL to the assets.Context() method. If you prefer using the value from an environment variable, you can use assets.NewContext() without any parameters. Provide your API token as the second parameter to the assets.Context() method. (Or again, have it picked up automatically by the assets.NewContext() method.) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. Don't forget to give permissions If you want to be able to access existing metadata with an API token, don't forget that you need to assign one or more personas to the API token that grant it access to metadata. Now that you have an SDK installed and configured, you are ready to code! Before we jump straight to code, though, let's first introduce some key concepts in Atlan: What is an asset? In Atlan, we refer to all objects that provide context to your data as assets . Each type of asset in Atlan has a set of: Properties , such as: Certificates Announcements Properties , such as: Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table In an object-oriented programming sense, think of an asset as an instance of a class. The structure of an asset (the class itself, in this analogy) is defined by something called a type definition , but that's for another day. So as you can see: There are many different kinds of assets: tables, columns, schemas, databases, business intelligence dashboards, reports, and so on. Assets inter-relate with each other: a table has a parent schema and child columns, a schema has a parent database and child tables, and so on. Different kinds of assets have some common properties (like certificates) and other properties that are unique to that kind of asset (like a columnCount that only exists on tables, not on schemas or databases). When you know the asset When you already know which asset you want to retrieve, you can read it from Atlan using one of its identifiers . We'll discuss these in more detail as part of updates, but for now you can think of them as: is a primary key for an asset: completely unique, but meaningless by itself is a business key for an asset: unique for a given kind of asset, and interpretable You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the asset.get_by_guid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the asset.get_by_qualified_name() method on the Atlan client, providing the type of asset you expect to retrieve and its qualified_name . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the assets.GetByGuid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the assets.GetByQualifiedName() method on the Atlan client, providing the type of asset you expect to retrieve and its qualifiedName . (Each asset type is its own unique class in the SDK.) Note that the response is strongly typed: If you are retrieving a table, you will get a table back (as long as it exists). You do not need to figure out what properties or relationships exist on a table - the Table class defines them for for you already. In any modern IDE, this means you have type-ahead support for retrieving the properties and relationships from the table variable. You can also refer to the types reference in this portal for full details of every kind of asset. Retrieval by identifier can be more costly than you might expect Even though you are retrieving an asset by an identifier, this can be more costly than you might expect. Retrieving an asset in this way will: Retrieve all its properties and their values Retrieve all its relationships Imagine the asset you are retrieving has 100's or 1000's of these. If you only care about its certificate and any owners, you will be retrieving far more information than you need. When you need to find it first For example, imagine you want to find all tables named MY_TABLE : You can then run the request using Execute() . For example, if you want to know the certificate of the asset you only need to tack that onto the query: Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many include_on_results calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can include as many attributes in IncludeOnResults as you want to specify the properties and relationships you want to retrieve for matching assets. Also gives the best performance If all you want to do is check or report on metadata, you should have a starting point from the information above. Or, now that you've found an asset of interest, maybe you want to update the asset with additional metadata ? Once again, before we jump to code, let's first understand some key concepts about how Atlan handles updates: Importance of identifiers Most operations on assets are upserts , that is, they could either create (insert) or update a given asset. How do you know which is going to happen? To answer this question, you need to understand how Atlan uniquely identifies each asset. Recall earlier we discussed asset's different identifiers in Atlan . Every asset in Atlan has at least the following two unique identifiers. These are both mandatory for every asset, so no asset can exist without these: Atlan uses globally-unique identifiers (GUIDs) to uniquely identify each asset, globally . They look something like this: As the name implies, GUIDs are: Globally unique (across all systems). Generated in a way that makes it nearly impossible for anything else to ever generate that same ID. 2 Note that this means the GUID itself is not : Meaningful or capable of being interpreted in any way Atlan uses qualifiedName s to uniquely identify assets based on their characteristics. They look something like this: Qualified names are not : Globally unique (across all systems). Instead, they are: Consistently constructed in a meaningful way, making it possible for them to be reconstructed. Note that this means the qualifiedName is: Meaningful and capable of being interpreted How these impact updates Since they are truly unique, operations that include a GUID will only update an asset, not create one. Conversely, operations that take a qualifiedName can: Create an asset, if no exactly-matching qualifiedName is found in Atlan. Update an asset, if an exact-match for the qualifiedName is found in Atlan. These operations also require a typeName , so that if creation does occur the correct type of asset is created. Unintended consequences of this behavior Be careful when using operations with only the qualifiedName . You may end up creating assets when you were only expecting them to be updated or to fail if they did not already exist. This is particularly true when you do not give the exact, case-sensitive qualifiedName of an asset. a\/b\/c\/d is not the same as a\/B\/c\/d when it comes to qualifiedName s. Perhaps this leaves you wondering: why have a qualifiedName at all? The qualifiedName 's purpose is to identify what is a unique asset. Many different tools might all have information about that asset. Having a common \"identity\" means that many different systems can each independently construct its identifier the same way. If a crawler gets table details from Snowflake it can upsert based on those identity characteristics in Atlan. The crawler will not create duplicate tables every time it runs. This gives idempotency. Looker knows the same identity characteristics for the Snowflake tables and columns. So if you get details from Looker about the tables it uses for reporting, you can link them together in lineage. (Looker can construct the same identifier for the table as Snowflake itself.) These characteristics are not possible using GUIDs alone. Limit to changes only Now that you understand the nuances of identifiers, let's look at how you can update metadata in Atlan. In general, you only need to send changes to Atlan. You do not need to send an entire asset each time you want to make changes to it. For example, imagine you want to mark a table as certified but do not want to change anything else (its name, description, owner details, and so on): You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualified_name . Using the updater() class method on any asset type, you pass in (typically) the qualified_name and name of the asset. You can then add onto the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to client.asset.save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the Updater() method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns an object into which you can then place any updates. You can place into the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to .Save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. Atlan will handle idempotency By sending only the changes you want to apply, Atlan can make idempotent updates. Atlan will only attempt to update the asset with the changes you send. Atlan leaves any existing metadata on the asset as-is. If the asset already has the metadata values you are sending, Atlan does nothing. It will not even update audit details like the last update timestamp, and is thus idempotent. What if you want to make changes to many assets, as efficiently as possible? Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Where to go from here Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Real code samples our customers use to solve particular use cases. Review live samples Events Delve deep into the details of the events Atlan triggers. Learn more about events Delve deep into the details of the events Atlan triggers. Learn more about events Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†©"}
{"url":"https:\/\/developer.atlan.com\/snippets\/","title":"Common tasks - Developer","text":"As developers ourselves, we know that learning is often easiest through example. In this Common tasks section you will find examples and explanations of the most common tasks in Atlan â€” those that are available on all assets. However, if you want to find out how to do something unique to a particular kind of asset, or to do something orthogonal to assets, you may also want to explore: Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures"}
{"url":"https:\/\/developer.atlan.com\/patterns\/","title":"Asset-specific - Developer","text":"As developers ourselves, we know that learning is often easiest through example. In this Asset-specific operations section you will find details about managing specific kinds of assets. You may also want to explore: Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures"}
{"url":"https:\/\/developer.atlan.com\/governance\/","title":"Governance structures - Developer","text":"As developers ourselves, we know that learning is often easiest through example. In this Governance structures operations section you will find details about managing the structures that can be used to govern your assets, rather than managing the assets themselves. You may also want to explore: Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset"}
{"url":"https:\/\/developer.atlan.com\/search\/","title":"Introduction to searching in Atlan - Developer","text":"Then, you can get a bit more advanced by adding in: Whether you want to sort those results How to page the results, when there are many of them Whether you want to aggregate any information from those results"}
{"url":"https:\/\/developer.atlan.com\/events\/","title":"Events overview - Developer","text":"Atlan produces events when any of the following activities occur in the system: An asset is created An asset is updated An asset is deleted Custom metadata is added to an asset Custom metadata is removed from an asset An asset is tagged An asset is untagged Lineage is created Each scenario above can be identified by Atlan producing one or more event payloads:"}
{"url":"https:\/\/developer.atlan.com\/getting-started\/#fnref%3A1","title":"Introductory walkthrough - Developer","text":"An introductory walkthrough You might also like our Atlan Platform Essentials certification . Not sure where to start? Allow us to introduce Atlan development through example. 1 We strongly recommend using one of our SDKs to simplify the development process. As a first step, set one up: The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include logback as a simple binding mechanism to send any logging information out to your console (standard out). Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on PyPI . You can use pip to install it as follows: Provide two values to create an Atlan client: Provide your Atlan tenant URL to the base_url parameter. (You can also do this through environment variables .) Provide your API token to the api_key parameter. (You can also do this through environment variables .) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include slf4j-simple as a simple binding mechanism to send any logging information out to your console (standard out), along with the kotlin-logging-jvm microutil. Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on GitHub , ready to be included in your project: Provide two values to set up connectivity to Atlan: Provide your Atlan tenant URL to the assets.Context() method. If you prefer using the value from an environment variable, you can use assets.NewContext() without any parameters. Provide your API token as the second parameter to the assets.Context() method. (Or again, have it picked up automatically by the assets.NewContext() method.) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. Don't forget to give permissions If you want to be able to access existing metadata with an API token, don't forget that you need to assign one or more personas to the API token that grant it access to metadata. Now that you have an SDK installed and configured, you are ready to code! Before we jump straight to code, though, let's first introduce some key concepts in Atlan: What is an asset? In Atlan, we refer to all objects that provide context to your data as assets . Each type of asset in Atlan has a set of: Properties , such as: Certificates Announcements Properties , such as: Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table In an object-oriented programming sense, think of an asset as an instance of a class. The structure of an asset (the class itself, in this analogy) is defined by something called a type definition , but that's for another day. So as you can see: There are many different kinds of assets: tables, columns, schemas, databases, business intelligence dashboards, reports, and so on. Assets inter-relate with each other: a table has a parent schema and child columns, a schema has a parent database and child tables, and so on. Different kinds of assets have some common properties (like certificates) and other properties that are unique to that kind of asset (like a columnCount that only exists on tables, not on schemas or databases). When you know the asset When you already know which asset you want to retrieve, you can read it from Atlan using one of its identifiers . We'll discuss these in more detail as part of updates, but for now you can think of them as: is a primary key for an asset: completely unique, but meaningless by itself is a business key for an asset: unique for a given kind of asset, and interpretable You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the asset.get_by_guid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the asset.get_by_qualified_name() method on the Atlan client, providing the type of asset you expect to retrieve and its qualified_name . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the assets.GetByGuid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the assets.GetByQualifiedName() method on the Atlan client, providing the type of asset you expect to retrieve and its qualifiedName . (Each asset type is its own unique class in the SDK.) Note that the response is strongly typed: If you are retrieving a table, you will get a table back (as long as it exists). You do not need to figure out what properties or relationships exist on a table - the Table class defines them for for you already. In any modern IDE, this means you have type-ahead support for retrieving the properties and relationships from the table variable. You can also refer to the types reference in this portal for full details of every kind of asset. Retrieval by identifier can be more costly than you might expect Even though you are retrieving an asset by an identifier, this can be more costly than you might expect. Retrieving an asset in this way will: Retrieve all its properties and their values Retrieve all its relationships Imagine the asset you are retrieving has 100's or 1000's of these. If you only care about its certificate and any owners, you will be retrieving far more information than you need. When you need to find it first For example, imagine you want to find all tables named MY_TABLE : You can then run the request using Execute() . For example, if you want to know the certificate of the asset you only need to tack that onto the query: Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many include_on_results calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can include as many attributes in IncludeOnResults as you want to specify the properties and relationships you want to retrieve for matching assets. Also gives the best performance If all you want to do is check or report on metadata, you should have a starting point from the information above. Or, now that you've found an asset of interest, maybe you want to update the asset with additional metadata ? Once again, before we jump to code, let's first understand some key concepts about how Atlan handles updates: Importance of identifiers Most operations on assets are upserts , that is, they could either create (insert) or update a given asset. How do you know which is going to happen? To answer this question, you need to understand how Atlan uniquely identifies each asset. Recall earlier we discussed asset's different identifiers in Atlan . Every asset in Atlan has at least the following two unique identifiers. These are both mandatory for every asset, so no asset can exist without these: Atlan uses globally-unique identifiers (GUIDs) to uniquely identify each asset, globally . They look something like this: As the name implies, GUIDs are: Globally unique (across all systems). Generated in a way that makes it nearly impossible for anything else to ever generate that same ID. 2 Note that this means the GUID itself is not : Meaningful or capable of being interpreted in any way Atlan uses qualifiedName s to uniquely identify assets based on their characteristics. They look something like this: Qualified names are not : Globally unique (across all systems). Instead, they are: Consistently constructed in a meaningful way, making it possible for them to be reconstructed. Note that this means the qualifiedName is: Meaningful and capable of being interpreted How these impact updates Since they are truly unique, operations that include a GUID will only update an asset, not create one. Conversely, operations that take a qualifiedName can: Create an asset, if no exactly-matching qualifiedName is found in Atlan. Update an asset, if an exact-match for the qualifiedName is found in Atlan. These operations also require a typeName , so that if creation does occur the correct type of asset is created. Unintended consequences of this behavior Be careful when using operations with only the qualifiedName . You may end up creating assets when you were only expecting them to be updated or to fail if they did not already exist. This is particularly true when you do not give the exact, case-sensitive qualifiedName of an asset. a\/b\/c\/d is not the same as a\/B\/c\/d when it comes to qualifiedName s. Perhaps this leaves you wondering: why have a qualifiedName at all? The qualifiedName 's purpose is to identify what is a unique asset. Many different tools might all have information about that asset. Having a common \"identity\" means that many different systems can each independently construct its identifier the same way. If a crawler gets table details from Snowflake it can upsert based on those identity characteristics in Atlan. The crawler will not create duplicate tables every time it runs. This gives idempotency. Looker knows the same identity characteristics for the Snowflake tables and columns. So if you get details from Looker about the tables it uses for reporting, you can link them together in lineage. (Looker can construct the same identifier for the table as Snowflake itself.) These characteristics are not possible using GUIDs alone. Limit to changes only Now that you understand the nuances of identifiers, let's look at how you can update metadata in Atlan. In general, you only need to send changes to Atlan. You do not need to send an entire asset each time you want to make changes to it. For example, imagine you want to mark a table as certified but do not want to change anything else (its name, description, owner details, and so on): You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualified_name . Using the updater() class method on any asset type, you pass in (typically) the qualified_name and name of the asset. You can then add onto the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to client.asset.save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the Updater() method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns an object into which you can then place any updates. You can place into the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to .Save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. Atlan will handle idempotency By sending only the changes you want to apply, Atlan can make idempotent updates. Atlan will only attempt to update the asset with the changes you send. Atlan leaves any existing metadata on the asset as-is. If the asset already has the metadata values you are sending, Atlan does nothing. It will not even update audit details like the last update timestamp, and is thus idempotent. What if you want to make changes to many assets, as efficiently as possible? Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Where to go from here Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Real code samples our customers use to solve particular use cases. Review live samples Events Delve deep into the details of the events Atlan triggers. Learn more about events Delve deep into the details of the events Atlan triggers. Learn more about events Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†©"}
{"url":"https:\/\/developer.atlan.com\/getting-started\/#fnref%3A2","title":"Introductory walkthrough - Developer","text":"An introductory walkthrough You might also like our Atlan Platform Essentials certification . Not sure where to start? Allow us to introduce Atlan development through example. 1 We strongly recommend using one of our SDKs to simplify the development process. As a first step, set one up: The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include logback as a simple binding mechanism to send any logging information out to your console (standard out). Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on PyPI . You can use pip to install it as follows: Provide two values to create an Atlan client: Provide your Atlan tenant URL to the base_url parameter. (You can also do this through environment variables .) Provide your API token to the api_key parameter. (You can also do this through environment variables .) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on Maven Central , ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include slf4j-simple as a simple binding mechanism to send any logging information out to your console (standard out), along with the kotlin-logging-jvm microutil. Provide two values to create an Atlan client: Provide your Atlan tenant URL as the first parameter. You can also read the value from an environment variable, if you leave out both parameters. Provide your API token as the second parameter. You can also read the value from another environment variable, by leaving out this parameter. You can then start writing some actual code to run within a static main method. (We'll show some examples of this further below.) Once the block is complete, any resources held by the client (i.e. for caching) will be automatically released. Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. The SDK is available on GitHub , ready to be included in your project: Provide two values to set up connectivity to Atlan: Provide your Atlan tenant URL to the assets.Context() method. If you prefer using the value from an environment variable, you can use assets.NewContext() without any parameters. Provide your API token as the second parameter to the assets.Context() method. (Or again, have it picked up automatically by the assets.NewContext() method.) Set up logging for SDK You can also checkout to the advanced configuration section of the SDK to learn about how to set up logging. Don't forget to give permissions If you want to be able to access existing metadata with an API token, don't forget that you need to assign one or more personas to the API token that grant it access to metadata. Now that you have an SDK installed and configured, you are ready to code! Before we jump straight to code, though, let's first introduce some key concepts in Atlan: What is an asset? In Atlan, we refer to all objects that provide context to your data as assets . Each type of asset in Atlan has a set of: Properties , such as: Certificates Announcements Properties , such as: Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table Relationships to other assets, such as: Schema child tables Table parent schema Table child columns Column parent table In an object-oriented programming sense, think of an asset as an instance of a class. The structure of an asset (the class itself, in this analogy) is defined by something called a type definition , but that's for another day. So as you can see: There are many different kinds of assets: tables, columns, schemas, databases, business intelligence dashboards, reports, and so on. Assets inter-relate with each other: a table has a parent schema and child columns, a schema has a parent database and child tables, and so on. Different kinds of assets have some common properties (like certificates) and other properties that are unique to that kind of asset (like a columnCount that only exists on tables, not on schemas or databases). When you know the asset When you already know which asset you want to retrieve, you can read it from Atlan using one of its identifiers . We'll discuss these in more detail as part of updates, but for now you can think of them as: is a primary key for an asset: completely unique, but meaningless by itself is a business key for an asset: unique for a given kind of asset, and interpretable You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the asset.get_by_guid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the asset.get_by_qualified_name() method on the Atlan client, providing the type of asset you expect to retrieve and its qualified_name . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the static get() method on any asset type, providing the client and either the asset's GUID or qualifiedName . (Each asset type is its own unique class in the SDK.) You can retrieve an asset using the assets.GetByGuid() method on the Atlan client, providing both the type of asset you expect to retrieve and its GUID. (Each asset type is its own unique class in the SDK.) You can also retrieve an asset using the assets.GetByQualifiedName() method on the Atlan client, providing the type of asset you expect to retrieve and its qualifiedName . (Each asset type is its own unique class in the SDK.) Note that the response is strongly typed: If you are retrieving a table, you will get a table back (as long as it exists). You do not need to figure out what properties or relationships exist on a table - the Table class defines them for for you already. In any modern IDE, this means you have type-ahead support for retrieving the properties and relationships from the table variable. You can also refer to the types reference in this portal for full details of every kind of asset. Retrieval by identifier can be more costly than you might expect Even though you are retrieving an asset by an identifier, this can be more costly than you might expect. Retrieving an asset in this way will: Retrieve all its properties and their values Retrieve all its relationships Imagine the asset you are retrieving has 100's or 1000's of these. If you only care about its certificate and any owners, you will be retrieving far more information than you need. When you need to find it first For example, imagine you want to find all tables named MY_TABLE : You can then run the request using Execute() . For example, if you want to know the certificate of the asset you only need to tack that onto the query: Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many include_on_results calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can chain as many includeOnResults calls as you want to specify the properties and relationships you want to retrieve for matching assets. Only this line differs from the original query. You can include as many attributes in IncludeOnResults as you want to specify the properties and relationships you want to retrieve for matching assets. Also gives the best performance If all you want to do is check or report on metadata, you should have a starting point from the information above. Or, now that you've found an asset of interest, maybe you want to update the asset with additional metadata ? Once again, before we jump to code, let's first understand some key concepts about how Atlan handles updates: Importance of identifiers Most operations on assets are upserts , that is, they could either create (insert) or update a given asset. How do you know which is going to happen? To answer this question, you need to understand how Atlan uniquely identifies each asset. Recall earlier we discussed asset's different identifiers in Atlan . Every asset in Atlan has at least the following two unique identifiers. These are both mandatory for every asset, so no asset can exist without these: Atlan uses globally-unique identifiers (GUIDs) to uniquely identify each asset, globally . They look something like this: As the name implies, GUIDs are: Globally unique (across all systems). Generated in a way that makes it nearly impossible for anything else to ever generate that same ID. 2 Note that this means the GUID itself is not : Meaningful or capable of being interpreted in any way Atlan uses qualifiedName s to uniquely identify assets based on their characteristics. They look something like this: Qualified names are not : Globally unique (across all systems). Instead, they are: Consistently constructed in a meaningful way, making it possible for them to be reconstructed. Note that this means the qualifiedName is: Meaningful and capable of being interpreted How these impact updates Since they are truly unique, operations that include a GUID will only update an asset, not create one. Conversely, operations that take a qualifiedName can: Create an asset, if no exactly-matching qualifiedName is found in Atlan. Update an asset, if an exact-match for the qualifiedName is found in Atlan. These operations also require a typeName , so that if creation does occur the correct type of asset is created. Unintended consequences of this behavior Be careful when using operations with only the qualifiedName . You may end up creating assets when you were only expecting them to be updated or to fail if they did not already exist. This is particularly true when you do not give the exact, case-sensitive qualifiedName of an asset. a\/b\/c\/d is not the same as a\/B\/c\/d when it comes to qualifiedName s. Perhaps this leaves you wondering: why have a qualifiedName at all? The qualifiedName 's purpose is to identify what is a unique asset. Many different tools might all have information about that asset. Having a common \"identity\" means that many different systems can each independently construct its identifier the same way. If a crawler gets table details from Snowflake it can upsert based on those identity characteristics in Atlan. The crawler will not create duplicate tables every time it runs. This gives idempotency. Looker knows the same identity characteristics for the Snowflake tables and columns. So if you get details from Looker about the tables it uses for reporting, you can link them together in lineage. (Looker can construct the same identifier for the table as Snowflake itself.) These characteristics are not possible using GUIDs alone. Limit to changes only Now that you understand the nuances of identifiers, let's look at how you can update metadata in Atlan. In general, you only need to send changes to Atlan. You do not need to send an entire asset each time you want to make changes to it. For example, imagine you want to mark a table as certified but do not want to change anything else (its name, description, owner details, and so on): You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualified_name . Using the updater() class method on any asset type, you pass in (typically) the qualified_name and name of the asset. You can then add onto the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to client.asset.save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the updater() static method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns a builder onto which you can then chain any updates. You can then chain onto the returned builder as many updates as you want. In this example, we change the certificate status to VERIFIED . At the end of your chain of updates, you need to build the builder (into an object, in-memory). And then, finally, you need to .save() that object to persist those changes in Atlan (passing the client for the tenant you want to save it in). The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. You can update an asset without first looking the asset up, if you know (can construct) its identifying qualifiedName . Using the Updater() method on any asset type, you pass in (typically) the qualifiedName and name of the asset. This returns an object into which you can then place any updates. You can place into the returned object as many updates as you want. In this example, we change the certificate status to VERIFIED . And then, finally, you need to .Save() that object to persist those changes in Atlan. The response will contain details of the change: whether the asset was created, updated, or nothing happened because the asset already had those changes. Atlan will handle idempotency By sending only the changes you want to apply, Atlan can make idempotent updates. Atlan will only attempt to update the asset with the changes you send. Atlan leaves any existing metadata on the asset as-is. If the asset already has the metadata values you are sending, Atlan does nothing. It will not even update audit details like the last update timestamp, and is thus idempotent. What if you want to make changes to many assets, as efficiently as possible? Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Start by initializing a batch. Through this batch, we can automatically queue up and bulk-upsert assets â€” in this example, 20 at a time. Be sure to include any details you might need to make a decision about whether to update the asset or not (and what to update it with). You can then add() any updated objects directly into the batch you created earlier. The batch itself will handle saving these to Atlan when a sufficient number have been queued up (20, in this example). You must flush() the batch outside of any loop where you've added assets into it. This ensures any final remaining elements in the batch are still sent to Atlan, even if the batch is not \"full\". Finally, from the batch you can retrieve the minimal details about any assets it created or updated. Where to go from here Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Real code samples our customers use to solve particular use cases. Review live samples Events Delve deep into the details of the events Atlan triggers. Learn more about events Delve deep into the details of the events Atlan triggers. Learn more about events Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© Note that this is intentionally kept as simple as possible. The walkthrough is not intended to be exhaustive. Where possible, we have cross-referenced other detailed examples elsewhere in the site. â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†© There are orders of magnitude lower chances of GUIDs conflicting with each other than there are grains of sand on the planet. (And generating them does not rely on a central ID-assigning registry.) â†©"}
{"url":"https:\/\/developer.atlan.com\/sdks\/#integration-options","title":"Integration options - Developer","text":"Throughout the portal you can focus on your preferred integration approach (and switch between them as you like): CLI Use the Atlan CLI to manage data contracts for assets in Atlan. Get started with CLI Use the Atlan CLI to manage data contracts for assets in Atlan. Get started with CLI dbt Use dbt's meta field to enrich metadata resources straight from dbt into Atlan. Get started with dbt Use dbt's meta field to enrich metadata resources straight from dbt into Atlan. Get started with dbt Java Pull our Java SDK from Maven Central, just like any other dependency. Get started with Java Pull our Java SDK from Maven Central, just like any other dependency. Get started with Java Python Pull our Python SDK from PyPI, just like any other dependency. Get started with Python Pull our Python SDK from PyPI, just like any other dependency. Get started with Python Kotlin Pull our Java SDK from Maven Central, just like any other dependency. Get started with Kotlin Pull our Java SDK from Maven Central, just like any other dependency. Get started with Kotlin Scala Pull our Java SDK from Maven Central, just like any other dependency. Get started with Scala Pull our Java SDK from Maven Central, just like any other dependency. Get started with Scala Clojure Pull our Java SDK from Maven Central, just like any other dependency. Get started with Clojure Pull our Java SDK from Maven Central, just like any other dependency. Get started with Clojure Go Pull our Go SDK from GitHub, just like any other dependency. Get started with Go Pull our Go SDK from GitHub, just like any other dependency. Get started with Go Events Tap into events Atlan produces to take immediate action, as metadata changes. Get started with events Tap into events Atlan produces to take immediate action, as metadata changes. Get started with events Raw REST API You can call directly into our REST API, though we would recommend the SDKs. Get started with raw REST APIs Raw REST API You can call directly into our REST API, though we would recommend the SDKs. Get started with raw REST APIs"}
{"url":"https:\/\/developer.atlan.com\/sdks\/cli\/","title":"Atlan CLI - Developer","text":"Limited functionality (so far) You can use Atlan's command-line interface (CLI) to manage some metadata in Atlan. Currently data contracts and metadata for a limited set of asset types can be managed through the CLI. Obtain the CLI For now, the CLI must be downloaded as a pre-built binary: Disclaimer â€” closed preview When installed via Homebrew, you can easily keep things up-to-date. If you do not use it already, see Homebrew's own installation documents for setting up Homebrew itself . Configure the CLI You can configure the CLI using a config file or in some cases environment variables, with the following minimum settings 1 : An API token that has access to your assets. The base URL of your tenant (including the https:\/\/ ). (Optional) Enable logging to produce more details on what the CLI is doing. When logging is enabled, specify the level of verbosity. An API token that has access to your assets. Define data sources You should also define data sources in the config file: Each data source definition must start with data_source , followed by a space and a unique reference name for the data source ( snowflake in this example). Reference name is your choice The reference name you give in the configuration file is only used here and as a reference in any data contracts you define. It need not match the name of the connection or data source in Atlan itself. Each data source definition must start with data_source , followed by a space and a unique reference name for the data source ( snowflake in this example). Reference name is your choice The reference name you give in the configuration file is only used here and as a reference in any data contracts you define. It need not match the name of the connection or data source in Atlan itself. You must indicate the type of connector for the data source (see connector types for options). You must indicate the type of connector for the data source (see connector types for options). Details of the connection must also be provided. You must provide the name of the connection, as it appears in Atlan. You must provide the unique qualified_name of the connection in Atlan. (Optional) You can also specify the database to use for this connection's assets by default, if none is specified in the data contract. (Optional) You can also specify the schema to use for this connection's assets by default, if none is specified in the data contract. These ensure the CLI can map the details you specify in your data contract to the appropriate corresponding asset in Atlan. With the CLI, you can: Manage data contracts Upload and download files from Atlan's backing object store Sync metadata to a limited set of asset types Integrate data contracts with CI\/CD processing When both are specified, environment variables will take precedence. â†© When both are specified, environment variables will take precedence. â†©"}
{"url":"https:\/\/developer.atlan.com\/sdks\/dbt\/","title":"dbt - Developer","text":"See it in action in our automated enrichment course (45 mins). You can use dbt's meta field to enrich metadata resources from dbt into Atlan. Atlan will ingest the information from this field and update the assets in Atlan accordingly. With this, you have a powerful way to keep the dbt assets documented directly as part of your dbt work. The following is an example: The description at the top level of an asset defined in dbt will already be mapped to the description field for that asset in Atlan. More detailed metadata, however, needs to be specified within the meta field. . and within the meta field, further within the atlan sub-field. For attributes, such as certificates, announcements, or owners these need to be specified within the attributes sub-field. Classifications need to be specified within a classifications sub-field. Note that the meta field and its sub-structure (including all the detailed attributes) can also be applied to columns within a model. This rich metadata will then be loaded to the corresponding attributes on the asset in Atlan. For more details on specific examples, see the dbt tabs in the Common asset actions snippets."}
{"url":"https:\/\/developer.atlan.com\/sdks\/java\/","title":"Java SDK - Developer","text":"Walk through step-by-step in our intro to custom integration course (30 mins). Obtain the SDK The SDK is available on Maven Central, ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include logback as a simple binding mechanism to send any logging information out to your console (standard out), at INFO -level or above. Configure the SDK There are two ways to configure the SDK: Using environment variables ATLAN_API_KEY should be given your Atlan API token , for authentication ( don't forget to assign one or more personas to the API token to give access to existing assets! ) ATLAN_BASE_URL should be given your Atlan URL (for example, https:\/\/tenant.atlan.com ) Here's an example of setting those environment variables: On client creation If you prefer to not use environment variables, you can do the following: Careful not to expose your API token! We generally discourage including your API token directly in your code, in case you accidentally commit it into a (public) version control system. But it's your choice exactly how you manage the API token and including it for use within the client. (Note that you can also explicity provide only the tenant URL, and the constructor will look for only the API key through an environment variable.) That's it â€” once these are set you can start using your SDK to make live calls against your Atlan instance! ðŸŽ‰ Delve into more detailed examples: Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Real code samples our customers use to solve particular use cases. Review live samples Events Delve deep into the details of the events Atlan triggers. Learn more about events Delve deep into the details of the events Atlan triggers. Learn more about events The SDK defines checked exceptions for the following categories of error: A given API call could fail due to all of the errors above. So these all extend a generic AtlanException checked exception, and all API operations throw AtlanException . For example, when creating a connection there is an asynchronous process that grants permissions to the admins of that connection. So there can be a slight delay between creating the connection and being permitted to do any operations with the connection. During that delay, any attempt to interact with the connection will result in a PermissionException , even if your API token was used to create connection in the first place. Another example you may occasionally hit is some network issue that causes your connection to Atlan to be interrupted. In these cases, an ApiConnectionException will be raised. Don't worry, the SDK retries automatically While these are useful to know for detecting issues, the SDK automatically retries on such problems. Atlan is a distributed, cloud-native application, where network problems can arise. These advanced configuration options allow you to optimize how the SDK handles such ephemeral problems. The SDK uses slf4j to be logging framework-agnostic. You can therefore configure your own preferred logging framework: Replace the ch.qos.logback:logback-classic:1.2.11 logback binding with log4j2 bindings. The SDK handles automatically retrying your requests when it detects certain problems: When an ApiConnectionException occurs that is caused by an underlying ConnectException or SocketTimeoutException . When there is a 403 response indicating that permission for an operation is not (yet) available. When there is a 500 response indicating that something went wrong on the server side. If any request encounters one of these problems, it will be retried. Before each retry, the SDK will apply a delay using: An exponential backoff (starting from 500ms) A jitter (in the range of 75-100% of the backoff delay) Each retry will be at least 500ms, and at most 5s. (Currently these values are not configurable.) For each request that encounters any of these problems, only up to a maximum number of retries will be attempted. (This is set to 3 by default.) You can configure the maximum number of retries globally using setMaxNetworkRetries() on a client. Set this to an integer: The SDK will only wait so long for a response before assuming a network problem has occurred and the request should be timed out. By default, this is set to 80 seconds. You can configure the maximum time the SDK will wait before timing out a request using setReadTimeout() on a client. Set this to an integer giving the number of milliseconds before timing out: Remember this must be given in milliseconds. This example sets the timeout to 2 minutes (120 seconds * 1000 milliseconds). Since version 0.9.0, the Java SDK supports connecting to multiple tenants. From version 4.0.0 onwards you can create any number of clients against any number of different tenants, since every operation that interacts with a tenant now explicitly requires a client to be provided to it: Constructing a new client with a different tenant's URL is sufficient to create connectivity to that other tenant. You can also (optionally) provide a second argument to directly give the API token for the tenant. Create an object as usual. You can access the operations for assets directly on the client, under client.assets . These will generally give you the most flexibility â€” they can handle multiple objects at a time and allow overrides. Every operation on the client itself has a variant with an (optional) final argument through which you can override settings like retry limits or timeouts for this single request. You can use the from(client) factory method to initialize the request options with all the settings of your client, and then you only need to chain on those you want to override for this particular request. Alternatively, you can pass the client to the operation on the object itself. Limit the number of clients to those you must have Each client you create maintains its own independent copy of various caches. So the more clients you have, the more resources your code will consume. For this reason, we recommended limiting the number of clients you create to the bare minimum you require â€” ideally just a single client per tenant. Using a proxy To use the Java SDK with a proxy, you need to send in some additional parameters when running any java . command. These are described in detail in the Java documentation , but are summarized here for simplicity: socksProxyHost should be set to the hostname for your SOCKS proxy socksProxyPort should be set to the port for your SOCKS proxy (default being 1080) Providing credentials to the proxy In either case, if you need to authenticate to your proxy, you will need to wrap whatever code you want to run to set up these credentials using something like the following: You need to create a built-in Java PasswordAuthentication object. Provide your username as the first argument. . and your password as the second argument, as a char[] . (Of course, you should not hard-code your password in your code itself, but rather pull it from elsewhere.) Then use setProxyCredential() to pass this PasswordAuthentication object to the Atlan client, before any of the rest of the code will execute."}
{"url":"https:\/\/developer.atlan.com\/sdks\/python\/","title":"Python SDK - Developer","text":"Walk through step-by-step in our intro to custom integration course (30 mins). Obtain the SDK The SDK is currently available on pypi . You can use pip to install it as follows: Configure the SDK There are two ways to configure the SDK: Using environment variables ATLAN_API_KEY should be given your Atlan API token , for authentication ( don't forget to assign one or more personas to the API token to give access to existing assets! ) ATLAN_BASE_URL should be given your Atlan URL (for example, https:\/\/tenant.atlan.com ) Here's an example of setting those environment variables: On client creation If you prefer to not use environment variables, you can do the following: Careful not to expose your API token! We generally discourage including your API token directly in your code, in case you accidentally commit it into a (public) version control system. But it's your choice exactly how you manage the API token and including it for use within the client. In some scenarios, you may not want to expose the entire API token or manage environment variables. Instead, you can provide the GUID of the API token, and the SDK will internally fetch the actual access token. When to use this approach: Building apps that use the SDK where token security is a concern When you want to avoid exposing full API tokens in your configuration For containerized applications that need secure token management Before using this approach, ensure your Argo template is configured with CLIENT_ID and CLIENT_SECRET : Create client from token GUID : Use AtlanClient.from_token_guid() to create a client using the GUID of an API token. The SDK will automatically fetch the actual access token using the configured CLIENT_ID and CLIENT_SECRET . That's it â€” once these are set you can start using your SDK to make live calls against your Atlan instance! ðŸŽ‰ Delve into more detailed examples: Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Real code samples our customers use to solve particular use cases. Review live samples Events Delve deep into the details of the events Atlan triggers. Learn more about events Delve deep into the details of the events Atlan triggers. Learn more about events The SDK defines exceptions for the following categories of error: A given API call could fail due to all of the errors above. So these all extend a generic AtlanError exception, and all API operations can potentially raise AtlanError . For example, when creating a connection there is an asynchronous process that grants permissions to the admins of that connection. So there can be a slight delay between creating the connection and being permitted to do any operations with the connection. During that delay, any attempt to interact with the connection will result in a PermissionError , even if your API token was used to create connection in the first place. Another example you may occasionally hit is some network issue that causes your connection to Atlan to be interrupted. In these cases, an ApiConnectionError will be raised. Don't worry, the SDK retries automatically While these are useful to know for detecting issues, the SDK automatically retries on such problems. Atlan is a distributed, cloud-native application, where network problems can arise. The SDK therefore automatically attempts to handle ephemeral problems. The SDK uses logging module of the standard library that can provide a flexible framework for emitting log messages. You can enable logging for your SDK script by adding the following lines above your snippets: You can enable logging by using basicConfig with various logging levels: logging.DEBUG : used to give detailed information, typically of interest only when diagnosing problems (mostly used level in SDK). logging.INFO : used to confirm that things are working as expected. logging.WARN : used as an indication that something unexpected happened, or as a warning of some problem in the near future. logging.ERROR : indicates that due to a more serious problem, the SDK has not been able to perform some operation. logging.CRITICAL : indicates a serious error, suggesting that the program itself may be unable to continue running (not used in SDK as of now). You can enable logging by using basicConfig with various logging levels: logging.DEBUG : used to give detailed information, typically of interest only when diagnosing problems (mostly used level in SDK). logging.INFO : used to confirm that things are working as expected. logging.WARN : used as an indication that something unexpected happened, or as a warning of some problem in the near future. logging.ERROR : indicates that due to a more serious problem, the SDK has not been able to perform some operation. logging.CRITICAL : indicates a serious error, suggesting that the program itself may be unable to continue running (not used in SDK as of now). By default, logs will appear in your console. If you want to use file logging, you can add the following line: logging.config.fileConfig('pyatlan\/logging.conf') : this will generate logs according to the configuration defined in pyatlan\/logging.conf and will generate two log files: \/tmp\/pyatlan.log : default log file. \/tmp\/pyatlan.json : log file in JSON format. By default, logs will appear in your console. If you want to use file logging, you can add the following line: logging.config.fileConfig('pyatlan\/logging.conf') : this will generate logs according to the configuration defined in pyatlan\/logging.conf and will generate two log files: \/tmp\/pyatlan.log : default log file. \/tmp\/pyatlan.json : log file in JSON format. logging.config.fileConfig('pyatlan\/logging.conf') : this will generate logs according to the configuration defined in pyatlan\/logging.conf and will generate two log files: \/tmp\/pyatlan.log : default log file. \/tmp\/pyatlan.json : log file in JSON format. The SDK handles automatically retrying your requests when it detects certain problems: When there is a 403 response indicating that permission for an operation is not (yet) available. When there is a 429 response indicating that the request rate limit has been exceeded, and you need to retry after some time. When there is a 50x response indicating that something went wrong on the server side. If any request encounters one of these problems, it will be retried. Before each retry, the SDK will apply a delay using an exponential backoff. (Currently the values for the exponential backoff are not configurable.) For each request that encounters any of these problems, only up to a maximum number of retries will be attempted. (This is set to 5 by default.) By default, the SDK AtlanClient() has the following timeout settings: read_timeout : 900.0 seconds ( 15 minutes) connect_timeout : 30.0 seconds If you need to override these defaults, you can do so as shown in the example below: Since version 1.0.0, the Python SDK supports connecting to multiple tenants.[^1] When you use the AtlanClient() method you are actually setting a default client. This default client will be used behind-the-scenes for any operations that need information specific to an Atlan tenant. When you want to override that default client you can create a new one and use the set_default_client() method to change it: The AtlanClient() method will return a client for the given base URL, creating a new client and setting this new client as the default client. If you want to switch between clients that you have already created, you can use Atlan.set_default_client() to change between them. Limit the number of clients to those you must have Each client you create maintains its own independent copy of various caches. So the more clients you have, the more resources your code will consume. For this reason, we recommended limiting the number of clients you create to the bare minimum you require â€” ideally just a single client per tenant. (And since in the majority of use cases you only need access to a single tenant, this means you can most likely just rely on the default client and the fallback behavior.) Pyatlan uses the Requests library which supports proxy configuration via environment variables. Requests relies on the proxy configuration defined by standard environment variables http_proxy, https_proxy, no_proxy, and all_proxy. Uppercase variants of these variables are also supported. You can therefore set them to configure Pyatlan (only set the ones relevant to your needs): To use HTTP Basic Auth with your proxy, use the http:\/\/user:password@host\/ syntax in any of the above configuration entries: Currently, the way this is implemented limits you to either avoiding multiple threads in your Python code (if you need to use multiple clients), or if you want to use multiple threads you should only use a single client. Asynchronous SDK operations To get started, you need to initialize an AsyncAtlanClient : Create an async client using the same configuration pattern as the synchronous client. Concurrent operations for improved performance The real power of async comes from running multiple operations concurrently . Instead of waiting for each operation to complete sequentially, you can execute them in parallel and reduce total execution time: Synchronous : Total time = operationâ‚ + operationâ‚‚ + . + operationâ‚™ Asynchronous : Total time = max(operationâ‚, operationâ‚‚, . , operationâ‚™) When to use async Async is most beneficial when you have: Multiple independent operations that can run concurrently I\/O-heavy workloads like API calls, database queries, or file operations Long-running operations where parallelization provides significant time savings For simple, single operations, the synchronous client may be more straightforward to use."}
{"url":"https:\/\/developer.atlan.com\/sdks\/kotlin\/","title":"Kotlin SDK - Developer","text":"Obtain the SDK For Kotlin, you can reuse the existing Java SDK as-is. It is available on Maven Central, ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include slf4j-simple as a simple binding mechanism to send any logging information out to your console (standard out), along with the kotlin-logging-jvm microutil. Configure the SDK There are two ways to configure the SDK: Using environment variables ATLAN_API_KEY should be given your Atlan API token , for authentication ( don't forget to assign one or more personas to the API token to give access to existing assets! ) ATLAN_BASE_URL should be given your Atlan URL (for example, https:\/\/tenant.atlan.com ) Here's an example of setting those environment variables: On client creation If you prefer to not use environment variables, you can do the following: Delve into more detailed examples: Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Real code samples our customers use to solve particular use cases. Review live samples Events Delve deep into the details of the events Atlan triggers. Learn more about events Delve deep into the details of the events Atlan triggers. Learn more about events The SDK defines checked exceptions for the following categories of error: A given API call could fail due to all of the errors above. So these all extend a generic AtlanException checked exception, and all API operations throw AtlanException . For example, when creating a connection there is an asynchronous process that grants permissions to the admins of that connection. So there can be a slight delay between creating the connection and being permitted to do any operations with the connection. During that delay, any attempt to interact with the connection will result in a PermissionException , even if your API token was used to create connection in the first place. Another example you may occasionally hit is some network issue that causes your connection to Atlan to be interrupted. In these cases, an ApiConnectionException will be raised. Don't worry, the SDK retries automatically While these are useful to know for detecting issues, the SDK automatically retries on such problems. Atlan is a distributed, cloud-native application, where network problems can arise. These advanced configuration options allow you to optimize how the SDK handles such ephemeral problems. The SDK uses slf4j to be logging framework-agnostic. You can therefore configure your own preferred logging framework: Replace the org.slf4j:slf4j-simple:2.0.7 binding with log4j2 bindings. The SDK handles automatically retrying your requests when it detects certain problems: When an ApiConnectionException occurs that is caused by an underlying ConnectException or SocketTimeoutException . When there is a 403 response indicating that permission for an operation is not (yet) available. When there is a 500 response indicating that something went wrong on the server side. If any request encounters one of these problems, it will be retried. Before each retry, the SDK will apply a delay using: An exponential backoff (starting from 500ms) A jitter (in the range of 75-100% of the backoff delay) Each retry will be at least 500ms, and at most 5s. (Currently these values are not configurable.) For each request that encounters any of these problems, only up to a maximum number of retries will be attempted. (This is set to 3 by default.) You can configure the maximum number of retries globally using setMaxNetworkRetries() on a client. Set this to an integer: The SDK will only wait so long for a response before assuming a network problem has occurred and the request should be timed out. By default, this is set to 80 seconds. You can configure the maximum time the SDK will wait before timing out a request using setReadTimeout() on a client. Set this to an integer giving the number of milliseconds before timing out: Remember this must be given in milliseconds. This example sets the timeout to 2 minutes (120 seconds * 1000 milliseconds). Since version 0.9.0, the Java SDK supports connecting to multiple tenants. From version 4.0.0 onwards you can create any number of clients against any number of different tenants, since every operation that interacts with a tenant now explicitly requires a client to be provided to it: Constructing a new client with a different tenant's URL is sufficient to create connectivity to that other tenant. You can also (optionally) provide a second argument to directly give the API token for the tenant. Create an object as usual. You can access the operations for assets directly on the client, under client.assets . These will generally give you the most flexibility â€” they can handle multiple objects at a time and allow overrides. Every operation on the client itself has a variant with an (optional) final argument through which you can override settings like retry limits or timeouts for this single request. You can use the from(client) factory method to initialize the request options with all the settings of your client, and then you only need to chain on those you want to override for this particular request. Alternatively, you can pass the client to the operation on the object itself. Limit the number of clients to those you must have Each client you create maintains its own independent copy of various caches. So the more clients you have, the more resources your code will consume. For this reason, we recommended limiting the number of clients you create to the bare minimum you require â€” ideally just a single client per tenant. Using a proxy To use the Java SDK with a proxy, you need to send in some additional parameters when running any java . command. These are described in detail in the Java documentation , but are summarized here for simplicity: socksProxyHost should be set to the hostname for your SOCKS proxy socksProxyPort should be set to the port for your SOCKS proxy (default being 1080) Providing credentials to the proxy In either case, if you need to authenticate to your proxy, you will need to wrap whatever code you want to run to set up these credentials using something like the following: You need to create a built-in Java PasswordAuthentication object. Provide your username as the first argument. . and your password as the second argument, as a char[] . (Of course, you should not hard-code your password in your code itself, but rather pull it from elsewhere.) Then use setProxyCredential() to pass this PasswordAuthentication object to the Atlan client, before any of the rest of the code will execute."}
{"url":"https:\/\/developer.atlan.com\/sdks\/scala\/","title":"Scala SDK - Developer","text":"Obtain the SDK For Scala, you can reuse the existing Java SDK as-is. It is available on Maven Central, ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency (replace the version number with the latest version indicated in the badges above). The Java SDK uses slf4j for logging purposes. You can include the scala-logging utility as a simple binding mechanism to send any logging information out to your console (standard out). Configure the SDK There are two ways to configure the SDK: Using environment variables ATLAN_API_KEY should be given your Atlan API token , for authentication ( don't forget to assign one or more personas to the API token to give access to existing assets! ) ATLAN_BASE_URL should be given your Atlan URL (for example, https:\/\/tenant.atlan.com ) Here's an example of setting those environment variables: On client creation If you prefer to not use environment variables, you can do the following: Delve into more detailed examples: Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Real code samples our customers use to solve particular use cases. Review live samples Events Delve deep into the details of the events Atlan triggers. Learn more about events Delve deep into the details of the events Atlan triggers. Learn more about events The SDK defines checked exceptions for the following categories of error: A given API call could fail due to all of the errors above. So these all extend a generic AtlanException checked exception, and all API operations throw AtlanException . For example, when creating a connection there is an asynchronous process that grants permissions to the admins of that connection. So there can be a slight delay between creating the connection and being permitted to do any operations with the connection. During that delay, any attempt to interact with the connection will result in a PermissionException , even if your API token was used to create connection in the first place. Another example you may occasionally hit is some network issue that causes your connection to Atlan to be interrupted. In these cases, an ApiConnectionException will be raised. Don't worry, the SDK retries automatically While these are useful to know for detecting issues, the SDK automatically retries on such problems. Atlan is a distributed, cloud-native application, where network problems can arise. These advanced configuration options allow you to optimize how the SDK handles such ephemeral problems. The SDK uses slf4j to be logging framework-agnostic. You can therefore configure your own preferred logging framework: Add log4j2 bindings. The SDK handles automatically retrying your requests when it detects certain problems: When an ApiConnectionException occurs that is caused by an underlying ConnectException or SocketTimeoutException . When there is a 403 response indicating that permission for an operation is not (yet) available. When there is a 500 response indicating that something went wrong on the server side. If any request encounters one of these problems, it will be retried. Before each retry, the SDK will apply a delay using: An exponential backoff (starting from 500ms) A jitter (in the range of 75-100% of the backoff delay) Each retry will be at least 500ms, and at most 5s. (Currently these values are not configurable.) For each request that encounters any of these problems, only up to a maximum number of retries will be attempted. (This is set to 3 by default.) You can configure the maximum number of retries globally using setMaxNetworkRetries() on a client. Set this to an integer: The SDK will only wait so long for a response before assuming a network problem has occurred and the request should be timed out. By default, this is set to 80 seconds. You can configure the maximum time the SDK will wait before timing out a request using setReadTimeout() on a client. Set this to an integer giving the number of milliseconds before timing out: Remember this must be given in milliseconds. This example sets the timeout to 2 minutes (120 seconds * 1000 milliseconds). Since version 0.9.0, the Java SDK supports connecting to multiple tenants. From version 4.0.0 onwards you can create any number of clients against any number of different tenants, since every operation that interacts with a tenant now explicitly requires a client to be provided to it: Constructing a new client with a different tenant's URL is sufficient to create connectivity to that other tenant. You can also (optionally) provide a second argument to directly give the API token for the tenant. Create an object as usual. You can access the operations for assets directly on the client, under client.assets . These will generally give you the most flexibility â€” they can handle multiple objects at a time and allow overrides. Every operation on the client itself has a variant with an (optional) final argument through which you can override settings like retry limits or timeouts for this single request. You can use the from(client) factory method to initialize the request options with all the settings of your client, and then you only need to chain on those you want to override for this particular request. Alternatively, you can pass the client to the operation on the object itself. Limit the number of clients to those you must have Each client you create maintains its own independent copy of various caches. So the more clients you have, the more resources your code will consume. For this reason, we recommended limiting the number of clients you create to the bare minimum you require â€” ideally just a single client per tenant. Using a proxy To use the Java SDK with a proxy, you need to send in some additional parameters when running any java . command. These are described in detail in the Java documentation , but are summarized here for simplicity: socksProxyHost should be set to the hostname for your SOCKS proxy socksProxyPort should be set to the port for your SOCKS proxy (default being 1080) Providing credentials to the proxy In either case, if you need to authenticate to your proxy, you will need to wrap whatever code you want to run to set up these credentials using something like the following: You need to create a built-in Java PasswordAuthentication object. Provide your username as the first argument. . and your password as the second argument, as a char[] . (Of course, you should not hard-code your password in your code itself, but rather pull it from elsewhere.) Then use setProxyCredential() to pass this PasswordAuthentication object to the Atlan client, before any of the rest of the code will execute."}
{"url":"https:\/\/developer.atlan.com\/sdks\/clojure\/","title":"Clojure SDK - Developer","text":"Obtain the SDK For Clojure, you can reuse the existing Java SDK as-is. It is available on Maven Central, ready to be included in your project: Include the latest version of the Java SDK in your project as a dependency (replace the version number with the latest version indicated in the badges above). The Java SDK uses slf4j for logging purposes. You can include the org.clojure\/tools.logging utility as a simple binding mechanism to send any logging information out to your console (standard out). Configure the SDK There are two ways to configure the SDK: Using environment variables ATLAN_API_KEY should be given your Atlan API token , for authentication ( don't forget to assign one or more personas to the API token to give access to existing assets! ) ATLAN_BASE_URL should be given your Atlan URL (for example, https:\/\/tenant.atlan.com ) Here's an example of setting those environment variables: On client creation If you prefer to not use environment variables, you can do the following: Delve into more detailed examples: Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Real code samples our customers use to solve particular use cases. Review live samples Events Delve deep into the details of the events Atlan triggers. Learn more about events Delve deep into the details of the events Atlan triggers. Learn more about events The SDK defines checked exceptions for the following categories of error: A given API call could fail due to all of the errors above. So these all extend a generic AtlanException checked exception, and all API operations throw AtlanException . For example, when creating a connection there is an asynchronous process that grants permissions to the admins of that connection. So there can be a slight delay between creating the connection and being permitted to do any operations with the connection. During that delay, any attempt to interact with the connection will result in a PermissionException , even if your API token was used to create connection in the first place. Another example you may occasionally hit is some network issue that causes your connection to Atlan to be interrupted. In these cases, an ApiConnectionException will be raised. Don't worry, the SDK retries automatically While these are useful to know for detecting issues, the SDK automatically retries on such problems. Atlan is a distributed, cloud-native application, where network problems can arise. These advanced configuration options allow you to optimize how the SDK handles such ephemeral problems. The SDK uses slf4j to be logging framework-agnostic. You can therefore configure your own preferred logging framework: Add log4j2 bindings. The SDK handles automatically retrying your requests when it detects certain problems: When an ApiConnectionException occurs that is caused by an underlying ConnectException or SocketTimeoutException . When there is a 403 response indicating that permission for an operation is not (yet) available. When there is a 500 response indicating that something went wrong on the server side. If any request encounters one of these problems, it will be retried. Before each retry, the SDK will apply a delay using: An exponential backoff (starting from 500ms) A jitter (in the range of 75-100% of the backoff delay) Each retry will be at least 500ms, and at most 5s. (Currently these values are not configurable.) For each request that encounters any of these problems, only up to a maximum number of retries will be attempted. (This is set to 3 by default.) You can configure the maximum number of retries globally using setMaxNetworkRetries() on a client. Set this to an integer: The SDK will only wait so long for a response before assuming a network problem has occurred and the request should be timed out. By default, this is set to 80 seconds. You can configure the maximum time the SDK will wait before timing out a request using setReadTimeout() on a client. Set this to an integer giving the number of milliseconds before timing out: Remember this must be given in milliseconds. This example sets the timeout to 2 minutes (120 seconds * 1000 milliseconds). Since version 0.9.0, the Java SDK supports connecting to multiple tenants. From version 4.0.0 onwards you can create any number of clients against any number of different tenants, since every operation that interacts with a tenant now explicitly requires a client to be provided to it: Constructing a new client with a different tenant's URL is sufficient to create connectivity to that other tenant. You can also (optionally) provide a second argument to directly give the API token for the tenant. Create an object as usual. You can access the operations for assets directly on the client, under client.assets . These will generally give you the most flexibility â€” they can handle multiple objects at a time and allow overrides. Every operation on the client itself has a variant with an (optional) final argument through which you can override settings like retry limits or timeouts for this single request. You can use the from(client) factory method to initialize the request options with all the settings of your client, and then you only need to chain on those you want to override for this particular request. Alternatively, you can pass the client to the operation on the object itself. Limit the number of clients to those you must have Each client you create maintains its own independent copy of various caches. So the more clients you have, the more resources your code will consume. For this reason, we recommended limiting the number of clients you create to the bare minimum you require â€” ideally just a single client per tenant. Using a proxy To use the Java SDK with a proxy, you need to send in some additional parameters when running any java . command. These are described in detail in the Java documentation , but are summarized here for simplicity: socksProxyHost should be set to the hostname for your SOCKS proxy socksProxyPort should be set to the port for your SOCKS proxy (default being 1080) Providing credentials to the proxy In either case, if you need to authenticate to your proxy, you will need to wrap whatever code you want to run to set up these credentials using something like the following: You need to create a built-in Java PasswordAuthentication object. Provide your username as the first argument. . and your password as the second argument, as a char[] . (Of course, you should not hard-code your password in your code itself, but rather pull it from elsewhere.) You need to create a built-in Java PasswordAuthentication object. Provide your username as the first argument. . and your password as the second argument, as a char[] . (Of course, you should not hard-code your password in your code itself, but rather pull it from elsewhere.) Then use setProxyCredential() to pass this PasswordAuthentication object to the Atlan client, before any of the rest of the code will execute. Then use setProxyCredential() to pass this PasswordAuthentication object to the Atlan client, before any of the rest of the code will execute."}
{"url":"https:\/\/developer.atlan.com\/sdks\/go\/","title":"Go SDK - Developer","text":"Obtain the SDK The Go SDK is currently in a pre-release, experimental state. While in this state, we reserve the right to make any changes to it (including breaking changes) without worrying about backwards compatibility, semantic versioning, and so on. If you are eager to experiment with it, it is available on GitHub . You can use Go dependencies to install it directly from there. We welcome your feedback during the pre-release, but cannot commit to any specific revisions or timelines at this point in time. Configure the SDK There are two ways to configure the SDK: Using environment variables ATLAN_API_KEY should be given your Atlan API token , for authentication ( don't forget to assign one or more personas to the API token to give access to existing assets! ) ATLAN_BASE_URL should be given your Atlan URL (for example, https:\/\/tenant.atlan.com ) Here's an example of setting those environment variables: On client creation If you prefer to not use environment variables, you can do the following: Careful not to expose your API token! We generally discourage including your API token directly in your code, in case you accidentally commit it into a (public) version control system. But it's your choice exactly how you manage the API token and including it for use within the client. That's it â€” once these are set you can start using your SDK to make live calls against your Atlan instance! ðŸŽ‰ Delve into more detailed examples: Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Real code samples our customers use to solve particular use cases. Review live samples Events Delve deep into the details of the events Atlan triggers. Learn more about events Delve deep into the details of the events Atlan triggers. Learn more about events The SDK defines exceptions for the following categories of error: A given API call could fail due to all of the errors above. So these all extend a generic AtlanError exception, and all API operations can potentially raise AtlanError . For example, when creating a connection there is an asynchronous process that grants permissions to the admins of that connection. So there can be a slight delay between creating the connection and being permitted to do any operations with the connection. During that delay, any attempt to interact with the connection will result in a PermissionError , even if your API token was used to create connection in the first place. Another example you may occasionally hit is some network issue that causes your connection to Atlan to be interrupted. In these cases, an ApiConnectionError will be raised. Atlan is a distributed, cloud-native application, where network problems can arise. The SDK therefore automatically attempts to handle ephemeral problems. The SDK uses the slog library internally to provide a flexible framework for emitting log messages. You can enable logging for your SDK script by adding the following lines above your snippets: You can enable logging by using .SetLogger() on the context object with various logging levels: \"debug\" : used to give detailed information, typically of interest only when diagnosing problems (mostly used level in SDK). \"info\" : used to confirm that things are working as expected. \"warn\" : used as an indication that something unexpected happened, or as a warning of some problem in the near future. \"error\" : indicates that due to a more serious problem, the SDK has not been able to perform some operation. You can enable logging by using .SetLogger() on the context object with various logging levels: \"debug\" : used to give detailed information, typically of interest only when diagnosing problems (mostly used level in SDK). \"info\" : used to confirm that things are working as expected. \"warn\" : used as an indication that something unexpected happened, or as a warning of some problem in the near future. \"error\" : indicates that due to a more serious problem, the SDK has not been able to perform some operation."}
{"url":"https:\/\/developer.atlan.com\/sdks\/events\/","title":"Events - Developer","text":"Atlan produces events when certain activities occur in the system. You can tap into these in a push-based integration model to take action the moment they occur. To tap into the events, you need to first set up a webhook in Atlan. Have a look at the event handling pattern for more details on implementing event-handling from webhooks using either of the SDKs."}
{"url":"https:\/\/developer.atlan.com\/sdks\/raw\/","title":"Raw REST API - Developer","text":"Raw REST API Walk through step-by-step in our intro to custom integration course (30 mins). There are a number of details and nuances to understand about the underlying REST APIs to use them most effectively: the underlying REST API communications (HTTP protocols, methods, headers, endpoint URLs, response codes, etc) translating the complex JSON structures for each request and response payload knowing exactly which values are required (and which aren't) depending on the object you're interacting with, what operation you're carrying out, etc (including which exact string (and capitalization) to use for values that are really enumerations behind-the-scenes) Ultimately, all pull-based integration mechanisms (including the SDKs) use the REST API; however, the SDKs also encode best practices to avoid the need to understand all these details and low-level nuances. If you want to adopt these best practices from the start, we would strongly recommend directly using any of our SDKs rather than the raw REST APIs directly: That being said, we have documented the raw REST API interactions in most cases. So if you really want to interact with the APIs directly, there should still be some guidance â€” anywhere you see Raw REST API gives details on endpoint URLs, methods, and payloads. If all you really want to do is some initial experimentation directly against the API, you can use Postman . We do not maintain a Postman collection of requests, but you can use the Raw REST API tab to find the endpoint and an example payload to use for most operations here on developer.atlan.com. Why not just publish an OpenAPI spec? We did try this in the past, as we liked the idea of generating client libraries using tools like the OpenAPI Generator . Unfortunately, in our own testing of these tools, we found that: We could generate code that is free from syntax errors, but the generated code was not fully functional. Problems we found: The generated code drops significant portions of data from payloads. (Our APIs are payload-rich and endpoint-light, while the generators seem to favor endpoint-rich and payload-light APIs.) The various objects the generator creates often make developer consumption cumbersome â€” long, difficult-to-read object names; many variations of similar objects; and so on. The generated code naturally does not include any validation that can't be encoded in the OpenAPI spec. To add this we'd need to wrap the generated code with another layer anyway. After several attempts at mangling the OpenAPI spec to meet the constraints of the code generator, we eventually decided to go the way we've seen other API-first companies adopt. We found very few API-first companies appear to rely on these code generators, but rather directly maintain their own SDKs for which they may have their own code generators. (Which is in fact exactly what we're doing as well.) Request for feedback If you use the raw REST APIs rather than one of the provided SDKs, we would love to understand more. If you can spare a few minutes to fill out our survey , we would be very grateful!"}
{"url":"https:\/\/docs.atlan.com\/support\/references\/customer-support#__docusaurus_skipToContent_fallback","title":"Customer support | Atlan Documentation","text":"One of Atlan's core values is to help you and your team do your life's best work. Ã° That's why Atlan wants to make it as easy as possible for you to keep driving your work forward with data. Atlan's customer support is a combination of several teams in Atlan: Product support personnel Cloud support personnel DevOps\/engineering support personnel Vast repository of self-service resources Service-level commitment Atlan's Technical Support team provides support globally with high response commitment levels. This includes 24\/7 SRE support for critical (P0) issues. Customers get a service-level commitment, including the following: 99.5% uptime for Atlan Dedicated support center, available from within the Atlan product Commitments for aggressive response times for business critical issues Designated Customer Success Manager to assist with escalations Ways to contact support Ã¯Â¸ Email support at a dedicated customer support email account ( [email protected] ) Ã° Â¨ Ã° Â» In-product support widget to log tickets and a help desk portal to log and track tickets. You can sign up to track support tickets on the help desk portal. You must use your organizational email address as the username and create a password. Ã° Submit a support request via the online form. To track your support tickets: Navigate to https:\/\/atlan.zendesk.com and log into the help desk portal with your credentials or via SSO. From the top right, click your avatar, and then from the dropdown, click My activities . On the My activities page, you can do the following: My requests and Requests I'm CC'd on - view and edit the support tickets you either created or were copied on, respectively. Organization requests - to access all other support tickets for your organization, please reach out to your customer success manager. Atlan provides you with read access to all the support tickets for your organization. To be able to comment on or close them, you must be CC'd on all tickets. My requests and Requests I'm CC'd on - view and edit the support tickets you either created or were copied on, respectively. Organization requests - to access all other support tickets for your organization, please reach out to your customer success manager. Atlan provides you with read access to all the support tickets for your organization. To be able to comment on or close them, you must be CC'd on all tickets. Hours of operation 24x7 availability for all requests and issues Severity levels The Atlan Technical Support team determines the severity of an issue. The customer's position is considered, and these guidelines are followed to determine priority. Below are the response time SLAs: Escalation procedure If the business impact of a support request changes or a ticket isn't being handled according to your expectations, you may escalate the ticket. Please first speak with the Technical Support representative assigned to the ticket to confirm that the business impact and urgency are understood. You may further escalate by contacting: 1st level of escalation : Technical Support Engineer 2nd level of escalation : Director, Support 3rd level of escalation : Head of Customer Experience Ways to contact support Hours of operation"}
{"url":"https:\/\/docs.atlan.com\/support\/references\/customer-support#service-level-commitment","title":"Customer support | Atlan Documentation","text":"One of Atlan's core values is to help you and your team do your life's best work. Ã° That's why Atlan wants to make it as easy as possible for you to keep driving your work forward with data. Atlan's customer support is a combination of several teams in Atlan: Product support personnel Cloud support personnel DevOps\/engineering support personnel Vast repository of self-service resources Service-level commitment Atlan's Technical Support team provides support globally with high response commitment levels. This includes 24\/7 SRE support for critical (P0) issues. Customers get a service-level commitment, including the following: 99.5% uptime for Atlan Dedicated support center, available from within the Atlan product Commitments for aggressive response times for business critical issues Designated Customer Success Manager to assist with escalations Ways to contact support Ã¯Â¸ Email support at a dedicated customer support email account ( [email protected] ) Ã° Â¨ Ã° Â» In-product support widget to log tickets and a help desk portal to log and track tickets. You can sign up to track support tickets on the help desk portal. You must use your organizational email address as the username and create a password. Ã° Submit a support request via the online form. To track your support tickets: Navigate to https:\/\/atlan.zendesk.com and log into the help desk portal with your credentials or via SSO. From the top right, click your avatar, and then from the dropdown, click My activities . On the My activities page, you can do the following: My requests and Requests I'm CC'd on - view and edit the support tickets you either created or were copied on, respectively. Organization requests - to access all other support tickets for your organization, please reach out to your customer success manager. Atlan provides you with read access to all the support tickets for your organization. To be able to comment on or close them, you must be CC'd on all tickets. My requests and Requests I'm CC'd on - view and edit the support tickets you either created or were copied on, respectively. Organization requests - to access all other support tickets for your organization, please reach out to your customer success manager. Atlan provides you with read access to all the support tickets for your organization. To be able to comment on or close them, you must be CC'd on all tickets. Hours of operation 24x7 availability for all requests and issues Severity levels The Atlan Technical Support team determines the severity of an issue. The customer's position is considered, and these guidelines are followed to determine priority. Below are the response time SLAs: Escalation procedure If the business impact of a support request changes or a ticket isn't being handled according to your expectations, you may escalate the ticket. Please first speak with the Technical Support representative assigned to the ticket to confirm that the business impact and urgency are understood. You may further escalate by contacting: 1st level of escalation : Technical Support Engineer 2nd level of escalation : Director, Support 3rd level of escalation : Head of Customer Experience Ways to contact support Hours of operation"}
{"url":"https:\/\/docs.atlan.com\/support\/references\/customer-support#ways-to-contact-support","title":"Customer support | Atlan Documentation","text":"One of Atlan's core values is to help you and your team do your life's best work. Ã° That's why Atlan wants to make it as easy as possible for you to keep driving your work forward with data. Atlan's customer support is a combination of several teams in Atlan: Product support personnel Cloud support personnel DevOps\/engineering support personnel Vast repository of self-service resources Service-level commitment Atlan's Technical Support team provides support globally with high response commitment levels. This includes 24\/7 SRE support for critical (P0) issues. Customers get a service-level commitment, including the following: 99.5% uptime for Atlan Dedicated support center, available from within the Atlan product Commitments for aggressive response times for business critical issues Designated Customer Success Manager to assist with escalations Ways to contact support Ã¯Â¸ Email support at a dedicated customer support email account ( [email protected] ) Ã° Â¨ Ã° Â» In-product support widget to log tickets and a help desk portal to log and track tickets. You can sign up to track support tickets on the help desk portal. You must use your organizational email address as the username and create a password. Ã° Submit a support request via the online form. To track your support tickets: Navigate to https:\/\/atlan.zendesk.com and log into the help desk portal with your credentials or via SSO. From the top right, click your avatar, and then from the dropdown, click My activities . On the My activities page, you can do the following: My requests and Requests I'm CC'd on - view and edit the support tickets you either created or were copied on, respectively. Organization requests - to access all other support tickets for your organization, please reach out to your customer success manager. Atlan provides you with read access to all the support tickets for your organization. To be able to comment on or close them, you must be CC'd on all tickets. My requests and Requests I'm CC'd on - view and edit the support tickets you either created or were copied on, respectively. Organization requests - to access all other support tickets for your organization, please reach out to your customer success manager. Atlan provides you with read access to all the support tickets for your organization. To be able to comment on or close them, you must be CC'd on all tickets. Hours of operation 24x7 availability for all requests and issues Severity levels The Atlan Technical Support team determines the severity of an issue. The customer's position is considered, and these guidelines are followed to determine priority. Below are the response time SLAs: Escalation procedure If the business impact of a support request changes or a ticket isn't being handled according to your expectations, you may escalate the ticket. Please first speak with the Technical Support representative assigned to the ticket to confirm that the business impact and urgency are understood. You may further escalate by contacting: 1st level of escalation : Technical Support Engineer 2nd level of escalation : Director, Support 3rd level of escalation : Head of Customer Experience Ways to contact support Hours of operation"}
{"url":"https:\/\/docs.atlan.com\/cdn-cgi\/l\/email-protection#721301193213061e131c5c111d1f","title":"Email Protection | Cloudflare","text":"You are unable to access this email address atlan.com The website from which you got to this page is protected by Cloudflare. Email addresses on that page have been hidden in order to keep them from being accessed by malicious bots. You must enable Javascript in your browser in order to decode the e-mail address . If you have a website and are interested in protecting it in a similar way, you can sign up for Cloudflare . How does Cloudflare protect email addresses on website from spammers? Can I sign up for Cloudflare? Cloudflare Ray ID: 97e29b39d988990a â€¢ Your IP: Click to reveal 130.91.58.216 â€¢ Performance & security by Cloudflare"}
{"url":"https:\/\/docs.atlan.com\/support\/references\/customer-support#hours-of-operation","title":"Customer support | Atlan Documentation","text":"One of Atlan's core values is to help you and your team do your life's best work. Ã° That's why Atlan wants to make it as easy as possible for you to keep driving your work forward with data. Atlan's customer support is a combination of several teams in Atlan: Product support personnel Cloud support personnel DevOps\/engineering support personnel Vast repository of self-service resources Service-level commitment Atlan's Technical Support team provides support globally with high response commitment levels. This includes 24\/7 SRE support for critical (P0) issues. Customers get a service-level commitment, including the following: 99.5% uptime for Atlan Dedicated support center, available from within the Atlan product Commitments for aggressive response times for business critical issues Designated Customer Success Manager to assist with escalations Ways to contact support Ã¯Â¸ Email support at a dedicated customer support email account ( [email protected] ) Ã° Â¨ Ã° Â» In-product support widget to log tickets and a help desk portal to log and track tickets. You can sign up to track support tickets on the help desk portal. You must use your organizational email address as the username and create a password. Ã° Submit a support request via the online form. To track your support tickets: Navigate to https:\/\/atlan.zendesk.com and log into the help desk portal with your credentials or via SSO. From the top right, click your avatar, and then from the dropdown, click My activities . On the My activities page, you can do the following: My requests and Requests I'm CC'd on - view and edit the support tickets you either created or were copied on, respectively. Organization requests - to access all other support tickets for your organization, please reach out to your customer success manager. Atlan provides you with read access to all the support tickets for your organization. To be able to comment on or close them, you must be CC'd on all tickets. My requests and Requests I'm CC'd on - view and edit the support tickets you either created or were copied on, respectively. Organization requests - to access all other support tickets for your organization, please reach out to your customer success manager. Atlan provides you with read access to all the support tickets for your organization. To be able to comment on or close them, you must be CC'd on all tickets. Hours of operation 24x7 availability for all requests and issues Severity levels The Atlan Technical Support team determines the severity of an issue. The customer's position is considered, and these guidelines are followed to determine priority. Below are the response time SLAs: Escalation procedure If the business impact of a support request changes or a ticket isn't being handled according to your expectations, you may escalate the ticket. Please first speak with the Technical Support representative assigned to the ticket to confirm that the business impact and urgency are understood. You may further escalate by contacting: 1st level of escalation : Technical Support Engineer 2nd level of escalation : Director, Support 3rd level of escalation : Head of Customer Experience Ways to contact support Hours of operation"}
{"url":"https:\/\/docs.atlan.com\/support\/references\/customer-support#severity-levels","title":"Customer support | Atlan Documentation","text":"One of Atlan's core values is to help you and your team do your life's best work. Ã° That's why Atlan wants to make it as easy as possible for you to keep driving your work forward with data. Atlan's customer support is a combination of several teams in Atlan: Product support personnel Cloud support personnel DevOps\/engineering support personnel Vast repository of self-service resources Service-level commitment Atlan's Technical Support team provides support globally with high response commitment levels. This includes 24\/7 SRE support for critical (P0) issues. Customers get a service-level commitment, including the following: 99.5% uptime for Atlan Dedicated support center, available from within the Atlan product Commitments for aggressive response times for business critical issues Designated Customer Success Manager to assist with escalations Ways to contact support Ã¯Â¸ Email support at a dedicated customer support email account ( [email protected] ) Ã° Â¨ Ã° Â» In-product support widget to log tickets and a help desk portal to log and track tickets. You can sign up to track support tickets on the help desk portal. You must use your organizational email address as the username and create a password. Ã° Submit a support request via the online form. To track your support tickets: Navigate to https:\/\/atlan.zendesk.com and log into the help desk portal with your credentials or via SSO. From the top right, click your avatar, and then from the dropdown, click My activities . On the My activities page, you can do the following: My requests and Requests I'm CC'd on - view and edit the support tickets you either created or were copied on, respectively. Organization requests - to access all other support tickets for your organization, please reach out to your customer success manager. Atlan provides you with read access to all the support tickets for your organization. To be able to comment on or close them, you must be CC'd on all tickets. My requests and Requests I'm CC'd on - view and edit the support tickets you either created or were copied on, respectively. Organization requests - to access all other support tickets for your organization, please reach out to your customer success manager. Atlan provides you with read access to all the support tickets for your organization. To be able to comment on or close them, you must be CC'd on all tickets. Hours of operation 24x7 availability for all requests and issues Severity levels The Atlan Technical Support team determines the severity of an issue. The customer's position is considered, and these guidelines are followed to determine priority. Below are the response time SLAs: Escalation procedure If the business impact of a support request changes or a ticket isn't being handled according to your expectations, you may escalate the ticket. Please first speak with the Technical Support representative assigned to the ticket to confirm that the business impact and urgency are understood. You may further escalate by contacting: 1st level of escalation : Technical Support Engineer 2nd level of escalation : Director, Support 3rd level of escalation : Head of Customer Experience Ways to contact support Hours of operation"}
{"url":"https:\/\/docs.atlan.com\/support\/references\/customer-support#escalation-procedure","title":"Customer support | Atlan Documentation","text":"One of Atlan's core values is to help you and your team do your life's best work. Ã° That's why Atlan wants to make it as easy as possible for you to keep driving your work forward with data. Atlan's customer support is a combination of several teams in Atlan: Product support personnel Cloud support personnel DevOps\/engineering support personnel Vast repository of self-service resources Service-level commitment Atlan's Technical Support team provides support globally with high response commitment levels. This includes 24\/7 SRE support for critical (P0) issues. Customers get a service-level commitment, including the following: 99.5% uptime for Atlan Dedicated support center, available from within the Atlan product Commitments for aggressive response times for business critical issues Designated Customer Success Manager to assist with escalations Ways to contact support Ã¯Â¸ Email support at a dedicated customer support email account ( [email protected] ) Ã° Â¨ Ã° Â» In-product support widget to log tickets and a help desk portal to log and track tickets. You can sign up to track support tickets on the help desk portal. You must use your organizational email address as the username and create a password. Ã° Submit a support request via the online form. To track your support tickets: Navigate to https:\/\/atlan.zendesk.com and log into the help desk portal with your credentials or via SSO. From the top right, click your avatar, and then from the dropdown, click My activities . On the My activities page, you can do the following: My requests and Requests I'm CC'd on - view and edit the support tickets you either created or were copied on, respectively. Organization requests - to access all other support tickets for your organization, please reach out to your customer success manager. Atlan provides you with read access to all the support tickets for your organization. To be able to comment on or close them, you must be CC'd on all tickets. My requests and Requests I'm CC'd on - view and edit the support tickets you either created or were copied on, respectively. Organization requests - to access all other support tickets for your organization, please reach out to your customer success manager. Atlan provides you with read access to all the support tickets for your organization. To be able to comment on or close them, you must be CC'd on all tickets. Hours of operation 24x7 availability for all requests and issues Severity levels The Atlan Technical Support team determines the severity of an issue. The customer's position is considered, and these guidelines are followed to determine priority. Below are the response time SLAs: Escalation procedure If the business impact of a support request changes or a ticket isn't being handled according to your expectations, you may escalate the ticket. Please first speak with the Technical Support representative assigned to the ticket to confirm that the business impact and urgency are understood. You may further escalate by contacting: 1st level of escalation : Technical Support Engineer 2nd level of escalation : Director, Support 3rd level of escalation : Head of Customer Experience Ways to contact support Hours of operation"}
{"url":"https:\/\/developer.atlan.com\/concepts\/#overall-site-map","title":"Developing with Atlan - Developer","text":"Overall site map If you are new to Atlan, or to developing with Atlan, start with one of the following two options. These will set you up to develop with Atlan, step-by-step. Atlan University Self-paced, video-based walkthrough of the essentials of Atlan as a platform. Atlan Platform Essentials certification Self-paced, video-based walkthrough of the essentials of Atlan as a platform. Atlan Platform Essentials certification Introductory walkthrough Documentation-based walkthrough, including step-by-step examples. Start the walkthrough Documentation-based walkthrough, including step-by-step examples. Start the walkthrough Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Real code samples our customers use to solve particular use cases. Review live samples Events Delve deep into the details of the events Atlan triggers. Learn more about events Delve deep into the details of the events Atlan triggers. Learn more about events"}
{"url":"https:\/\/developer.atlan.com\/concepts\/#introduction","title":"Developing with Atlan - Developer","text":"Overall site map If you are new to Atlan, or to developing with Atlan, start with one of the following two options. These will set you up to develop with Atlan, step-by-step. Atlan University Self-paced, video-based walkthrough of the essentials of Atlan as a platform. Atlan Platform Essentials certification Self-paced, video-based walkthrough of the essentials of Atlan as a platform. Atlan Platform Essentials certification Introductory walkthrough Documentation-based walkthrough, including step-by-step examples. Start the walkthrough Documentation-based walkthrough, including step-by-step examples. Start the walkthrough Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Real code samples our customers use to solve particular use cases. Review live samples Events Delve deep into the details of the events Atlan triggers. Learn more about events Delve deep into the details of the events Atlan triggers. Learn more about events"}
{"url":"https:\/\/developer.atlan.com\/concepts\/#jumping-in","title":"Developing with Atlan - Developer","text":"Overall site map If you are new to Atlan, or to developing with Atlan, start with one of the following two options. These will set you up to develop with Atlan, step-by-step. Atlan University Self-paced, video-based walkthrough of the essentials of Atlan as a platform. Atlan Platform Essentials certification Self-paced, video-based walkthrough of the essentials of Atlan as a platform. Atlan Platform Essentials certification Introductory walkthrough Documentation-based walkthrough, including step-by-step examples. Start the walkthrough Documentation-based walkthrough, including step-by-step examples. Start the walkthrough Common tasks Common operations on assets, that are available across all assets. Discover actions Common operations on assets, that are available across all assets. Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Real code samples our customers use to solve particular use cases. Review live samples Events Delve deep into the details of the events Atlan triggers. Learn more about events Delve deep into the details of the events Atlan triggers. Learn more about events"}
